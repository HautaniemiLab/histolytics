{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Histolytics","text":"A Python library for scalable panoptic spatial analysis of histological WSIs Welcome to Histolytics documentation"},{"location":"#introduction","title":"Introduction","text":"<p>histolytics is a spatial analysis library for histological whole slide images (WSI). Built upon <code>torch</code>, <code>geopandas</code> and <code>libpysal</code>, the library provides a comprehensive and scalable framework for panoptic segmentation and interpretable panoptic spatial analysis of routine histopathology slides.</p>"},{"location":"#panoptic-segmentation-features","title":"Panoptic Segmentation Features \ud83c\udf1f","text":"<ul> <li>Fast WSI-level panoptic segmentation. See example.</li> <li>Low memory-footprint segmentation results with <code>__geo_interface__</code>-specification.</li> <li>Multiple vectorized segmentation output formats (geojson/feather/parquet).</li> <li>Several panoptic segmentation model architectures for histological WSIs with flexible backbone support: See example</li> <li>Pre-trained models in model-hub. See: histolytics-hub</li> </ul>"},{"location":"#spatial-analysis-features","title":"Spatial Analysis Features \ud83d\udcca","text":"<ul> <li>Fast Spatial Querying of WSI-scale panoptic segmentation maps. See example</li> <li>Spatial indexing/partitioning for localized spatial statistics and analysis. See example</li> <li>Graph-based neighborhood analysis for local cell neighborhoods. See example</li> <li>Plotting utilities for spatial data visualization. See example</li> <li>Spatial clustering and cluster centrography metrics. See example</li> <li>Large set of morphological, intensity, chromatin distribution, and textural features at nuclear level. See example</li> <li>Large set of collagen fiber and intensity based features to characterize stroma and ECM. See example</li> </ul>"},{"location":"#example-workflows","title":"Example Workflows \ud83e\uddea","text":""},{"location":"#immuno-oncology-profiling","title":"Immuno-oncology Profiling:","text":"<ul> <li>Spatial Statistics of TILs.</li> <li>Profiling TLS and Lymphoid Aggregates.</li> </ul>"},{"location":"#nuclear-pleomorphism","title":"Nuclear Pleomorphism:","text":"<ul> <li>Nuclear Morphology Analysis.</li> <li>Nuclear Chromatin Distribution Analysis.</li> </ul>"},{"location":"#tme-characterization","title":"TME Characterization:","text":"<ul> <li>Collagen Fiber Disorder Analysis.</li> <li>Characterization of Desmoplastic Stroma.</li> </ul>"},{"location":"#nuclei-neighborhoods","title":"Nuclei Neighborhoods:","text":"<ul> <li>Tumor Cell Accessibility.</li> </ul>"},{"location":"#installation","title":"Installation \ud83d\udee0\ufe0f","text":"<pre><code>pip install histolytics\n</code></pre>"},{"location":"#models","title":"Models \ud83e\udd16","text":"<ul> <li>Panoptic HoVer-Net</li> <li>Panoptic Cellpose</li> <li>Panoptic Stardist</li> <li>Panoptic CellVit-SAM</li> <li>Panoptic CPP-Net</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! To get started:</p> <ol> <li>Fork the repository and create your branch from <code>main</code>.</li> <li>Make your changes with clear commit messages.</li> <li>Ensure all tests pass and add new tests as needed.</li> <li>Submit a pull request describing your changes.</li> </ol> <p>See contributing guide for detailed guidelines.</p>"},{"location":"#citation","title":"Citation","text":"<pre><code>@article{2025histolytics,\n  title={Histolytics: A Panoptic Spatial Analysis Framework for Interpretable Histopathology},\n  author={Oskari Lehtonen, Niko Nordlund, Shams Salloum, Ilkka Kalliala, Anni Virtanen, Sampsa Hautaniemi},\n  journal={XX},\n  volume={XX},\n  number={XX},\n  pages={XX},\n  year={2025},\n  publisher={XX}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<pre><code>pip install histolytics\n</code></pre>"},{"location":"installation/#install-from-source-with-uv","title":"Install from Source with <code>uv</code>","text":"<p>To install the latest development version using uv:</p> <pre><code>uv pip install git+https://github.com/HautaniemiLab/histolytics.git\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the Histolytics API Reference. Here you'll find an overview of all public objects, functions and methods implemented in Histolytics.</p>"},{"location":"api/#modules","title":"Modules","text":""},{"location":"api/#data","title":"Data","text":"<p>Sample datasets</p> <ul> <li>cervix_nuclei: A GeoDataframe of segmented nuclei of a cervical biopsy.</li> <li>cervix_tissue: A GeoDataframe of segmented tissue regions of a cervical biopsy.</li> <li>cervix_nuclei_crop: A GeoDataframe of segmented nuclei of a cervical biopsy (cropped).</li> <li>cervix_tissue_crop: A GeoDataframe of segmented tissue regions of a cervical biopsy (cropped).</li> <li>hgsc_nuclei_wsi: A GeoDataframe of segmented nuclei of a HGSC whole slide image.</li> <li>hgsc_tissue_wsi: A GeoDataframe of segmented tissue regions of a HGSC whole slide image.</li> <li>hgsc_cancer_nuclei: A GeoDataframe of segmented nuclei of a HGSC tumor nest.</li> <li>hgsc_cancer_he: A 1500x1500 H&amp;E image of HGSC containing a tumor nest.</li> <li>hgsc_stroma_nuclei: A GeoDataframe of segmented nuclei of a HGSC stroma.</li> <li>hgsc_stroma_he: A 1500x1500 H&amp;E image of HGSC containing stroma.</li> </ul>"},{"location":"api/#losses","title":"Losses","text":"<p>Loss functions for panoptic segmentation</p> <ul> <li>BCELoss: Binary Cross Entropy Loss.</li> <li>CELoss: Cross Entropy Loss.</li> <li>DiceLoss: Dice Loss.</li> <li>FocalLoss: Focal Loss.</li> <li>JointLoss: Joint Loss. Combines arbitrary number of losses into one.</li> <li>MSE: Mean Squared Error Loss.</li> <li>MAE: Mean Absolute Error Loss.</li> <li>MultiTaskLoss: Multi-task loss for panoptic segmentation.   Combines multiple losses for multi prediction tasks like panoptic segmentation.</li> <li>SSIM: Structural Similarity Index Loss.</li> <li>TverskyLoss: Tversky Loss.</li> </ul>"},{"location":"api/#metrics","title":"Metrics","text":"<p>Metrics for panoptic segmentation</p> <ul> <li>accuracy_multiclass: Accuracy metric for multiclass segmentation.</li> <li>aggregated_jaccard_index: Aggregated Jaccard Index for multiclass segmentation.</li> <li>average_precision: Average Precision metric for multiclass segmentation.</li> <li>dice_multiclass: Dice metric for multiclass segmentation.</li> <li>dice2: Alternative Dice metric for multiclass segmentation.</li> <li>f1score_multiclass: F1 score metric for multiclass segmentation.</li> <li>iou_multiclass: Intersection over Union (IoU) metric for multiclass segmentation.</li> <li>pairwise_object_stats: Pairwise object statistics (TP, FP, TN, FN) for instance segmentation.</li> <li>pairwise_pixel_stats: Pairwise pixel-level statistics (TP, FP, TN, FN) for instance segmentation.</li> <li>panoptic_quality: Panoptic Quality metric for panoptic segmentation.</li> <li>sensitivity_multiclass: Sensitivity metric for multiclass segmentation.</li> <li>specificity_multiclass: Specificity metric for multiclass segmentation.</li> </ul>"},{"location":"api/#models","title":"Models","text":"<p>Panoptic segmentation models</p> <ul> <li>CellposePanoptic: Panoptic segmentation model based on Cellpose.</li> <li>CellVitPanoptic: Panoptic segmentation model based on CellVit.</li> <li>CPPNetPanoptic: Panoptic segmentation model based on CPPNet.</li> <li>HoverNetPanoptic: Panoptic segmentation model based on HoverNet.</li> <li>StarDistPanoptic: Panoptic segmentation model based on StarDist.</li> </ul>"},{"location":"api/#nuclei-features","title":"Nuclei Features","text":"<p>Extracting features from nuclei</p> <ul> <li>extract_chromatin_clumps: Extract chromatin clumps from a nuclei segmentation.</li> <li>chromatin_feats: Extract chromatin clumps and measurements from a nuclei segmentation.</li> <li>grayscale_intensity: Extract grayscale intensity features from a nuclei segmentation.</li> <li>rgb_intensity: Extract RGB intensity features from a nuclei segmentation.</li> <li>textural_feats: Extract textural features from a nuclei segmentation.</li> </ul>"},{"location":"api/#spatial-aggregation","title":"Spatial Aggregation","text":"<p>Neighborhood statistics and grid aggregation</p> <ul> <li>local_character: Get summary metrics of neighboring nuclei features.</li> <li>local_diversity: Get diversity indices of neighboring nuclei features.</li> <li>local_distances: Get distances to neighboring nuclei.</li> <li>local_vals: Get local values of neighboring nuclei.</li> <li>local_type_counts: Get counts of neighboring nuclei types.</li> <li>grid_agg: Aggregate spatial data within grid cells.</li> </ul>"},{"location":"api/#spatial-clustering","title":"Spatial Clustering","text":"<p>Clustering and cluster metrics</p> <ul> <li>density_clustering: Perform density-based clustering on spatial data.</li> <li>lisa_clustering: Perform Local Indicators of Spatial Association (LISA) clustering.</li> <li>cluster_feats: Extract features from spatial clusters.</li> <li>cluster_tendency: Calculate cluster tendency (centroid).</li> <li>local_autocorr: Calculate local Moran's I for each object in a GeoDataFrame.</li> <li>global_autocorr: Calculate global Moran's I for a GeoDataFrame.</li> <li>ripley_test: Perform Ripley's alphabet analysis for GeoDataFrames.</li> </ul>"},{"location":"api/#spatial-geometry","title":"Spatial Geometry","text":"<p>Morphometrics and shapes</p> <ul> <li>shape_metric: Calculate shape moprhometrics for polygon geometries.</li> <li>line_metric: Calculate shape moprhometrics for line geometries.</li> <li>medial_lines: Create medial lines of input polygons.</li> <li>hull: Create various hull types around point sets.</li> </ul>"},{"location":"api/#spatial-graph","title":"Spatial Graph","text":"<p>Graph fitting</p> <ul> <li>fit_graph: Fit a graph to a GeoDataFrame of segmented objects.</li> <li>get_connected_components: Get connected components of a spatial graph.</li> <li>weights2gdf: Convert spatial weights to a GeoDataFrame.</li> </ul>"},{"location":"api/#spatial-operations","title":"Spatial Operations","text":"<p>Spatial querying and partitioning</p> <ul> <li>get_objs: Query segmented objects from specified regions.</li> <li>get_interfaces: Get interfaces of two segmented tissues.</li> <li>rect_grid: Partition a GeoDataFrame into a rectangular grid.</li> <li>h3_grid: Partition a GeoDataFrame into an H3 hexagonal spatial index (grid).</li> <li>quadbin_grid: Partition a GeoDataFrame into a Quadbin spatial index (grid).</li> </ul>"},{"location":"api/#stroma-features","title":"Stroma Features","text":"<p>Extracting features from stroma</p> <ul> <li>extract_collagen_fibers: Extract collagen fibers from a H&amp;E images.</li> <li>stromal_intensity_features: Compute intensity features from a H&amp;E image representing stroma.</li> <li>fiber_feats: Extract fiber features from a H&amp;E image representing stroma.</li> </ul>"},{"location":"api/#transforms","title":"Transforms","text":"<p>Image and instance label transforms for model training</p> <ul> <li>AlbuStrongAugment: Apply StrongAugment augmentation algorithm.</li> <li>ApplyEach: Apply a functions to label masks and return each output separately.</li> <li>BinarizeTransform: Binarize label masks.</li> <li>CellposeTransform: Transform label masks to Cellpose flow maps.</li> <li>ContourTransform: Transform label masks to contour maps.</li> <li>DistTransform: Transform label masks to distance maps.</li> <li>EdgeWeightTransform: Transform label masks to edge weight maps.</li> <li>HoverNetTransform: Transform label masks to HoverNet horizontal and vertical gradient maps.</li> <li>MinMaxNormalization: Apply Min-Max normalization to input image.</li> <li>Normalization: Normalize/Standardize input image.</li> <li>PercentileNormalization: Normalize input image using percentiles.</li> <li>SmoothDistTransform: Transform label masks to smooth distance maps.</li> <li>StarDistTransform: Transform label masks to StarDist star-distance maps.</li> </ul>"},{"location":"api/#utils","title":"Utils","text":"<p>Utility functions and classes</p>"},{"location":"api/#gdf","title":"gdf","text":"<ul> <li>gdf_apply: Apply a function to a GeoDataFrame in parallel.</li> <li>gdf_to_polars: Convert a GeoDataFrame to a Polars DataFrame.</li> <li>get_centroid_numpy: Get the centroids of a GeoDataFrame as a NumPy array.</li> <li>set_uid: Set a unique identifier (UID) for each object in a GeoDataFrame.</li> <li>set_geom_precision: Set the precision of geometries in a GeoDataFrame.</li> </ul>"},{"location":"api/#raster","title":"raster","text":"<ul> <li>inst2gdf: Convert an instance segmentation mask to a GeoDataFrame.</li> <li>sem2gdf: Convert a semantic tissue segmentation mask to a GeoDataFrame.</li> <li>gdf2inst: Convert a GeoDataFrame to an instance segmentation mask.</li> <li>gdf2sem: Convert a GeoDataFrame to a semantic tissue segmentation mask.</li> </ul>"},{"location":"api/#plot","title":"plot","text":"<ul> <li>draw_thing_contours: Draw contours of segmented nuclei and overlay them on an image.</li> <li>legendgram: Create a histogram legend for a specified column in a GeoDataFrame.</li> </ul>"},{"location":"api/#im","title":"im","text":"<ul> <li>get_eosin_mask: Get eosin mask from a H&amp;E image.</li> <li>get_hematoxylin_mask: Get hematoxylin mask from a H&amp;E image.</li> <li>hed_decompose: Transform an image to HED space.</li> <li>kmeans_img: Perform KMeans clustering on an image.</li> <li>tissue_components: Extract background, foreground, and nuclear components from a H&amp;E image.</li> </ul>"},{"location":"api/#wsi-whole-slide-images","title":"WSI (Whole Slide Images)","text":"<p>WSI handling and WSI-level segmentation</p> <ul> <li>SlideReader: Functions for reading whole slide images</li> <li>WsiPanopticSegmenter: Class handling the panoptic segmentation of whole slide images</li> <li>WSIGridProcessor: Class for processing WSI grid cells.</li> <li>get_sub_grids: Get sub-grids from a whole slide image.</li> </ul>"},{"location":"api/data/cervix_nuclei/","title":"cervix_nuclei","text":"<p>A GeoDataframe of segmented nuclei of a cervical biopsy.</p> Note <p>Pairs with: <code>cervix_tissue()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; ax = cervix_nuclei().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def cervix_nuclei():\n    \"\"\"A GeoDataframe of segmented nuclei of a cervical biopsy.\n\n    Note:\n        Pairs with: `cervix_tissue()`.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; ax = cervix_nuclei().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/cervix_biopsy_nuc.png)\n    \"\"\"\n    return _load(BASE_PATH / \"cervix_biopsy_nuclei.parquet\")\n</code></pre>"},{"location":"api/data/cervix_nuclei_crop/","title":"cervix_nuclei_crop","text":"<p>A GeoDataframe of segmented nuclei of cervical tissue crop.</p> Note <p>Pairs with: <code>cervix_tissue_crop()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import cervix_nuclei_crop\n&gt;&gt;&gt; ax = cervix_nuclei_crop().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def cervix_nuclei_crop():\n    \"\"\"A GeoDataframe of segmented nuclei of cervical tissue crop.\n\n    Note:\n        Pairs with: `cervix_tissue_crop()`.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei_crop\n        &gt;&gt;&gt; ax = cervix_nuclei_crop().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/cervix_biopsy_nuc_crop.png)\n    \"\"\"\n    return _load(BASE_PATH / \"cervix_nuclei_crop.parquet\")\n</code></pre>"},{"location":"api/data/cervix_tissue/","title":"cervix_tissue","text":"<p>A GeoDataframe of segmented tissue regions of a cervical biopsy.</p> Note <p>Pairs with: <code>cervix_nuclei()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import cervix_tissue\n&gt;&gt;&gt; ax = cervix_tissue().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def cervix_tissue():\n    \"\"\"A GeoDataframe of segmented tissue regions of a cervical biopsy.\n\n    Note:\n        Pairs with: `cervix_nuclei()`.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue\n        &gt;&gt;&gt; ax = cervix_tissue().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/cervix_biopsy_tis.png)\n    \"\"\"\n    return _load(BASE_PATH / \"cervix_biopsy_tissue.parquet\")\n</code></pre>"},{"location":"api/data/cervix_tissue_crop/","title":"cervix_tissue_crop","text":"<p>A GeoDataframe of segmented tissue regions of cervical tissue crop.</p> Note <p>Pairs with: <code>cervix_nuclei_crop()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import cervix_tissue_crop\n&gt;&gt;&gt; ax = cervix_tissue_crop().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def cervix_tissue_crop():\n    \"\"\"A GeoDataframe of segmented tissue regions of cervical tissue crop.\n\n    Note:\n        Pairs with: `cervix_nuclei_crop()`.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue_crop\n        &gt;&gt;&gt; ax = cervix_tissue_crop().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/cervix_biopsy_tis_crop.png)\n    \"\"\"\n    return _load(BASE_PATH / \"cervix_tissue_crop.parquet\")\n</code></pre>"},{"location":"api/data/hgsc_cancer_he/","title":"hgsc_cancer_he","text":"<p>A 1500x1500 H&amp;E image of HGSC containing a tumor nest.</p> Note <p>Pairs with:</p> <ul> <li><code>hgsc_cancer_nuclei()</code></li> <li><code>hgsc_cancer_inst_mask()</code></li> <li><code>hgsc_cancer_type_mask()</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_he\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n&gt;&gt;&gt; im = hgsc_cancer_he()\n&gt;&gt;&gt; ax.imshow(im)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_cancer_he():\n    \"\"\"A 1500x1500 H&amp;E image of HGSC containing a tumor nest.\n\n    Note:\n        Pairs with:\n\n        - `hgsc_cancer_nuclei()`\n        - `hgsc_cancer_inst_mask()`\n        - `hgsc_cancer_type_mask()`\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_he\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n        &gt;&gt;&gt; im = hgsc_cancer_he()\n        &gt;&gt;&gt; ax.imshow(im)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_cancer_he.png)\n    \"\"\"\n    return FileHandler.read_img(BASE_PATH / \"hgsc_nest.jpg\")\n</code></pre>"},{"location":"api/data/hgsc_cancer_inst_mask/","title":"hgsc_cancer_inst_mask","text":"<p>An instance raster mask (1500x1500px) of segmented nuclei of a HGSC tumor nest.</p> Note <p>Pairs with:</p> <ul> <li><code>hgsc_cancer_nuclei()</code></li> <li><code>hgsc_cancer_he()</code></li> <li><code>hgsc_cancer_type_mask()</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_type_mask\n&gt;&gt;&gt; from skimage.measure import label\n&gt;&gt;&gt; from skimage.color import label2rgb\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n&gt;&gt;&gt; im = hgsc_cancer_type_mask()\n&gt;&gt;&gt; ax.imshow(label2rgb(label(im), bg_label=0))\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_cancer_inst_mask():\n    \"\"\"An instance raster mask (1500x1500px) of segmented nuclei of a HGSC tumor nest.\n\n    Note:\n        Pairs with:\n\n        - `hgsc_cancer_nuclei()`\n        - `hgsc_cancer_he()`\n        - `hgsc_cancer_type_mask()`\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_type_mask\n        &gt;&gt;&gt; from skimage.measure import label\n        &gt;&gt;&gt; from skimage.color import label2rgb\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n        &gt;&gt;&gt; im = hgsc_cancer_type_mask()\n        &gt;&gt;&gt; ax.imshow(label2rgb(label(im), bg_label=0))\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_cancer_inst.png)\n    \"\"\"\n    data = np.load(BASE_PATH / \"hgsc_nest_inst_mask.npz\")\n    return data[\"nuc_raster\"]\n</code></pre>"},{"location":"api/data/hgsc_cancer_nuclei/","title":"hgsc_cancer_nuclei","text":"<p>A GeoDataframe of segmented nuclei of a HGSC tumor nest.</p> Note <p>Pairs with:</p> <ul> <li><code>hgsc_cancer_he()</code></li> <li><code>hgsc_cancer_inst_mask()</code></li> <li><code>hgsc_cancer_type_mask()</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; ax = hgsc_cancer_nuclei().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_cancer_nuclei():\n    \"\"\"A GeoDataframe of segmented nuclei of a HGSC tumor nest.\n\n    Note:\n        Pairs with:\n\n        - `hgsc_cancer_he()`\n        - `hgsc_cancer_inst_mask()`\n        - `hgsc_cancer_type_mask()`\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; ax = hgsc_cancer_nuclei().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_crop_nuc.png)\n    \"\"\"\n    return _load(BASE_PATH / \"hgsc_nest.parquet\")\n</code></pre>"},{"location":"api/data/hgsc_cancer_type_mask/","title":"hgsc_cancer_type_mask","text":"<p>An semantic raster mask (1500x1500px) of segmented nuclei of a HGSC tumor nest.</p> Note <p>Pairs with:</p> <ul> <li><code>hgsc_cancer_nuclei()</code></li> <li><code>hgsc_cancer_he()</code></li> <li><code>hgsc_cancer_inst_mask()</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_type_mask\n&gt;&gt;&gt; from skimage.color import label2rgb\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n&gt;&gt;&gt; im = hgsc_cancer_type_mask()\n&gt;&gt;&gt; ax.imshow(label2rgb(im, bg_label=0))\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_cancer_type_mask():\n    \"\"\"An semantic raster mask (1500x1500px) of segmented nuclei of a HGSC tumor nest.\n\n    Note:\n        Pairs with:\n\n        - `hgsc_cancer_nuclei()`\n        - `hgsc_cancer_he()`\n        - `hgsc_cancer_inst_mask()`\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_type_mask\n        &gt;&gt;&gt; from skimage.color import label2rgb\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n        &gt;&gt;&gt; im = hgsc_cancer_type_mask()\n        &gt;&gt;&gt; ax.imshow(label2rgb(im, bg_label=0))\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_cancer_type.png)\n    \"\"\"\n    data = np.load(BASE_PATH / \"hgsc_nest_type_mask.npz\")\n    return data[\"nuc_raster\"]\n</code></pre>"},{"location":"api/data/hgsc_nuclei_wsi/","title":"hgsc_nuclei_wsi","text":"<p>A GeoDataframe of segmented nuclei of a HGSC WSI.</p> Note <p>Pairs with: <code>hgsc_tissue_wsi()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_nuclei_wsi\n&gt;&gt;&gt; ax = hgsc_nuclei_wsi().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_nuclei_wsi():\n    \"\"\"A GeoDataframe of segmented nuclei of a HGSC WSI.\n\n    Note:\n        Pairs with: `hgsc_tissue_wsi()`.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_nuclei_wsi\n        &gt;&gt;&gt; ax = hgsc_nuclei_wsi().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_wsi_nuc.png)\n    \"\"\"\n    return _load(BASE_PATH / \"hgsc_nuclei_wsi.parquet\")\n</code></pre>"},{"location":"api/data/hgsc_stroma_he/","title":"hgsc_stroma_he","text":"<p>A 1500x1500 H&amp;E image of HGSC containing stroma.</p> Note <p>Pairs with <code>hgsc_stroma_nuclei()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from histolytics.data import hgsc_stroma_he\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n&gt;&gt;&gt; im = hgsc_stroma_he()\n&gt;&gt;&gt; ax.imshow(im)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_stroma_he():\n    \"\"\"A 1500x1500 H&amp;E image of HGSC containing stroma.\n\n    Note:\n        Pairs with `hgsc_stroma_nuclei()`.\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from histolytics.data import hgsc_stroma_he\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(4, 4))\n        &gt;&gt;&gt; im = hgsc_stroma_he()\n        &gt;&gt;&gt; ax.imshow(im)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_stroma_he.png)\n    \"\"\"\n    return FileHandler.read_img(BASE_PATH / \"hgsc_stromal_he.jpg\")\n</code></pre>"},{"location":"api/data/hgsc_stroma_nuclei/","title":"hgsc_stroma_nuclei","text":"<p>A GeoDataframe of segmented nuclei of a HGSC stroma.</p> Note <p>Pairs with <code>hgsc_stroma_he()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_stroma_nuclei\n&gt;&gt;&gt; ax = hgsc_stroma_nuclei().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_stroma_nuclei():\n    \"\"\"A GeoDataframe of segmented nuclei of a HGSC stroma.\n\n    Note:\n        Pairs with `hgsc_stroma_he()`.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_stroma_nuclei\n        &gt;&gt;&gt; ax = hgsc_stroma_nuclei().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_stroma_crop_nuc.png)\n    \"\"\"\n    return _load(BASE_PATH / \"hgsc_stromal_cells.parquet\")\n</code></pre>"},{"location":"api/data/hgsc_tissue_wsi/","title":"hgsc_tissue_wsi","text":"<p>A GeoDataframe of segmented tissue regions of a HGSC WSI.</p> Note <p>Pairs with: <code>hgsc_nuclei_wsi()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_tissue_wsi\n&gt;&gt;&gt; ax = hgsc_tissue_wsi().plot(column=\"class_name\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/data/fetch.py</code> <pre><code>def hgsc_tissue_wsi():\n    \"\"\"A GeoDataframe of segmented tissue regions of a HGSC WSI.\n\n    Note:\n        Pairs with: `hgsc_nuclei_wsi()`.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_tissue_wsi\n        &gt;&gt;&gt; ax = hgsc_tissue_wsi().plot(column=\"class_name\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/hgsc_wsi_tis.png)\n    \"\"\"\n    return _load(BASE_PATH / \"hgsc_tissue_wsi.parquet\")\n</code></pre>"},{"location":"api/losses/bce/","title":"BCELoss","text":"<p>               Bases: <code>WeightedBaseLoss</code></p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.apply_spectral_decouple","title":"apply_spectral_decouple","text":"<pre><code>apply_spectral_decouple(loss_matrix: Tensor, yhat: Tensor, lam: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Apply spectral decoupling L2 norm after the loss.</p> <p>https://arxiv.org/abs/2011.09468</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>yhat</code> <code>Tensor</code> <p>The pixel predictions of the model. Shape (B, C, H, W).</p> required <code>lam</code> <code>float, default=0.01</code> <p>Lambda constant.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: SD-regularized loss matrix. Same shape as input.</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.apply_ls_to_target","title":"apply_ls_to_target","text":"<pre><code>apply_ls_to_target(target: Tensor, n_classes: int, label_smoothing: float = 0.1) -&gt; torch.Tensor\n</code></pre> <p>Apply regular label smoothing to the target map.</p> <p>https://arxiv.org/abs/1512.00567</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>label_smoothing</code> <code>float, default=0.1</code> <p>The smoothing coeff alpha.</p> <code>0.1</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.apply_svls_to_target","title":"apply_svls_to_target","text":"<pre><code>apply_svls_to_target(target: Tensor, n_classes: int, kernel_size: int = 5, sigma: int = 3, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply spatially varying label smoothihng to target map.</p> <p>https://arxiv.org/abs/2104.05788</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>kernel_size</code> <code>int, default=3</code> <p>Size of a square kernel.</p> <code>5</code> <code>sigma</code> <code>int, default=3</code> <p>The std of the gaussian.</p> <code>3</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.apply_class_weights","title":"apply_class_weights","text":"<pre><code>apply_class_weights(loss_matrix: Tensor, target: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Multiply pixelwise loss matrix by the class weights.</p> Note <p>Does not apply normalization</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>target</code> <code>Tensor</code> <p>The target mask. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the weight matrix. Shape (B, H, W).</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.apply_edge_weights","title":"apply_edge_weights","text":"<pre><code>apply_edge_weights(loss_matrix: Tensor, weight_map: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply weights to the object boundaries.</p> <p>Basically just computes <code>edge_weight</code>**<code>weight_map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>weight_map</code> <code>Tensor</code> <p>Map that points to the pixels that will be weighted. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the nuclear boundary weights. Shape (B, H, W).</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.apply_mask_weight","title":"apply_mask_weight","text":"<pre><code>apply_mask_weight(loss_matrix: Tensor, mask: Tensor, norm: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Apply a mask to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>mask</code> <code>Tensor</code> <p>The mask. Shape (B, H, W).</p> required <code>norm</code> <code>bool, default=True</code> <p>If True, the loss matrix will be normalized by the mean of the mask.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the mask. Shape (B, H, W).</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.__init__","title":"__init__","text":"<pre><code>__init__(apply_sd: bool = False, apply_ls: bool = False, apply_svls: bool = False, apply_mask: bool = False, edge_weight: float = None, class_weights: Tensor = None, **kwargs) -&gt; None\n</code></pre> <p>Binary cross entropy loss with weighting and other tricks.</p> <p>Parameters     apply_sd (bool, default=False):         If True, Spectral decoupling regularization will be applied  to the         loss matrix.     apply_ls (bool, default=False):         If True, Label smoothing will be applied to the target.     apply_svls (bool, default=False):         If True, spatially varying label smoothing will be applied to the target     apply_mask (bool, default=False):         If True, a mask will be applied to the loss matrix. Mask shape: (B, H, W)     edge_weight (float, default=None):         Weight that is added to object borders.     class_weights (torch.Tensor, default=None):         Class weights. A tensor of shape (n_classes,).</p>"},{"location":"api/losses/bce/#histolytics.losses.BCELoss.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, target_weight: Tensor = None, mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute binary cross entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>yhat</code> <code>Tensor</code> <p>The prediction map. Shape (B, C, H, W).</p> required <code>target</code> <code>Tensor</code> <p>the ground truth annotations. Shape (B, H, W).</p> required <code>target_weight</code> <code>torch.Tensor, default=None</code> <p>The edge weight map. Shape (B, H, W).</p> <code>None</code> <code>mask</code> <code>torch.Tensor, default=None</code> <p>The mask map. Shape (B, H, W).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed BCE loss (scalar).</p>"},{"location":"api/losses/ce/","title":"CELoss","text":"<p>               Bases: <code>WeightedBaseLoss</code></p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.apply_spectral_decouple","title":"apply_spectral_decouple","text":"<pre><code>apply_spectral_decouple(loss_matrix: Tensor, yhat: Tensor, lam: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Apply spectral decoupling L2 norm after the loss.</p> <p>https://arxiv.org/abs/2011.09468</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>yhat</code> <code>Tensor</code> <p>The pixel predictions of the model. Shape (B, C, H, W).</p> required <code>lam</code> <code>float, default=0.01</code> <p>Lambda constant.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: SD-regularized loss matrix. Same shape as input.</p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.apply_ls_to_target","title":"apply_ls_to_target","text":"<pre><code>apply_ls_to_target(target: Tensor, n_classes: int, label_smoothing: float = 0.1) -&gt; torch.Tensor\n</code></pre> <p>Apply regular label smoothing to the target map.</p> <p>https://arxiv.org/abs/1512.00567</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>label_smoothing</code> <code>float, default=0.1</code> <p>The smoothing coeff alpha.</p> <code>0.1</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.apply_svls_to_target","title":"apply_svls_to_target","text":"<pre><code>apply_svls_to_target(target: Tensor, n_classes: int, kernel_size: int = 5, sigma: int = 3, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply spatially varying label smoothihng to target map.</p> <p>https://arxiv.org/abs/2104.05788</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>kernel_size</code> <code>int, default=3</code> <p>Size of a square kernel.</p> <code>5</code> <code>sigma</code> <code>int, default=3</code> <p>The std of the gaussian.</p> <code>3</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.apply_class_weights","title":"apply_class_weights","text":"<pre><code>apply_class_weights(loss_matrix: Tensor, target: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Multiply pixelwise loss matrix by the class weights.</p> Note <p>Does not apply normalization</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>target</code> <code>Tensor</code> <p>The target mask. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the weight matrix. Shape (B, H, W).</p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.apply_edge_weights","title":"apply_edge_weights","text":"<pre><code>apply_edge_weights(loss_matrix: Tensor, weight_map: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply weights to the object boundaries.</p> <p>Basically just computes <code>edge_weight</code>**<code>weight_map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>weight_map</code> <code>Tensor</code> <p>Map that points to the pixels that will be weighted. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the nuclear boundary weights. Shape (B, H, W).</p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.apply_mask_weight","title":"apply_mask_weight","text":"<pre><code>apply_mask_weight(loss_matrix: Tensor, mask: Tensor, norm: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Apply a mask to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>mask</code> <code>Tensor</code> <p>The mask. Shape (B, H, W).</p> required <code>norm</code> <code>bool, default=True</code> <p>If True, the loss matrix will be normalized by the mean of the mask.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the mask. Shape (B, H, W).</p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.__init__","title":"__init__","text":"<pre><code>__init__(apply_sd: bool = False, apply_ls: bool = False, apply_svls: bool = False, apply_mask: bool = False, edge_weight: float = None, class_weights: Tensor = None, **kwargs) -&gt; None\n</code></pre> <p>Cross-Entropy loss with weighting.</p> <p>Parameters:</p> Name Type Description Default <code>apply_sd</code> <code>bool, default=False</code> <p>If True, Spectral decoupling regularization will be applied  to the loss matrix.</p> <code>False</code> <code>apply_ls</code> <code>bool, default=False</code> <p>If True, Label smoothing will be applied to the target.</p> <code>False</code> <code>apply_svls</code> <code>bool, default=False</code> <p>If True, spatially varying label smoothing will be applied to the target</p> <code>False</code> <code>apply_mask</code> <code>bool, default=False</code> <p>If True, a mask will be applied to the loss matrix. Mask shape: (B, H, W)</p> <code>False</code> <code>edge_weight</code> <code>float, default=None</code> <p>Weight that is added to object borders.</p> <code>None</code> <code>class_weights</code> <code>torch.Tensor, default=None</code> <p>Class weights. A tensor of shape (n_classes,).</p> <code>None</code>"},{"location":"api/losses/ce/#histolytics.losses.CELoss.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, target_weight: Tensor = None, mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the cross entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>yhat</code> <code>Tensor</code> <p>The prediction map. Shape (B, C, H, W).</p> required <code>target</code> <code>Tensor</code> <p>the ground truth annotations. Shape (B, H, W).</p> required <code>target_weight</code> <code>torch.Tensor, default=None</code> <p>The edge weight map. Shape (B, H, W).</p> <code>None</code> <code>mask</code> <code>torch.Tensor, default=None</code> <p>The mask map. Shape (B, H, W).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed CE loss (scalar).</p>"},{"location":"api/losses/dice/","title":"DiceLoss","text":"<p>               Bases: <code>WeightedBaseLoss</code></p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.apply_spectral_decouple","title":"apply_spectral_decouple","text":"<pre><code>apply_spectral_decouple(loss_matrix: Tensor, yhat: Tensor, lam: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Apply spectral decoupling L2 norm after the loss.</p> <p>https://arxiv.org/abs/2011.09468</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>yhat</code> <code>Tensor</code> <p>The pixel predictions of the model. Shape (B, C, H, W).</p> required <code>lam</code> <code>float, default=0.01</code> <p>Lambda constant.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: SD-regularized loss matrix. Same shape as input.</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.apply_ls_to_target","title":"apply_ls_to_target","text":"<pre><code>apply_ls_to_target(target: Tensor, n_classes: int, label_smoothing: float = 0.1) -&gt; torch.Tensor\n</code></pre> <p>Apply regular label smoothing to the target map.</p> <p>https://arxiv.org/abs/1512.00567</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>label_smoothing</code> <code>float, default=0.1</code> <p>The smoothing coeff alpha.</p> <code>0.1</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.apply_svls_to_target","title":"apply_svls_to_target","text":"<pre><code>apply_svls_to_target(target: Tensor, n_classes: int, kernel_size: int = 5, sigma: int = 3, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply spatially varying label smoothihng to target map.</p> <p>https://arxiv.org/abs/2104.05788</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>kernel_size</code> <code>int, default=3</code> <p>Size of a square kernel.</p> <code>5</code> <code>sigma</code> <code>int, default=3</code> <p>The std of the gaussian.</p> <code>3</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.apply_class_weights","title":"apply_class_weights","text":"<pre><code>apply_class_weights(loss_matrix: Tensor, target: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Multiply pixelwise loss matrix by the class weights.</p> Note <p>Does not apply normalization</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>target</code> <code>Tensor</code> <p>The target mask. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the weight matrix. Shape (B, H, W).</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.apply_edge_weights","title":"apply_edge_weights","text":"<pre><code>apply_edge_weights(loss_matrix: Tensor, weight_map: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply weights to the object boundaries.</p> <p>Basically just computes <code>edge_weight</code>**<code>weight_map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>weight_map</code> <code>Tensor</code> <p>Map that points to the pixels that will be weighted. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the nuclear boundary weights. Shape (B, H, W).</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.apply_mask_weight","title":"apply_mask_weight","text":"<pre><code>apply_mask_weight(loss_matrix: Tensor, mask: Tensor, norm: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Apply a mask to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>mask</code> <code>Tensor</code> <p>The mask. Shape (B, H, W).</p> required <code>norm</code> <code>bool, default=True</code> <p>If True, the loss matrix will be normalized by the mean of the mask.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the mask. Shape (B, H, W).</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.__init__","title":"__init__","text":"<pre><code>__init__(apply_sd: bool = False, apply_ls: bool = False, apply_svls: bool = False, apply_mask: bool = False, edge_weight: float = None, class_weights: Tensor = None, **kwargs) -&gt; None\n</code></pre> <p>S\u00f8rensen-Dice Coefficient Loss.</p> <p>Optionally applies weights at the object edges and classes.</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.__init__--parameters","title":"Parameters","text":"<p>apply_sd : bool, default=False     If True, Spectral decoupling regularization will be applied  to the     loss matrix. apply_ls : bool, default=False     If True, Label smoothing will be applied to the target. apply_svls : bool, default=False     If True, spatially varying label smoothing will be applied to the target apply_mask : bool, default=False     If True, a mask will be applied to the loss matrix. Mask shape: (B, H, W) edge_weight : float, default=none     Weight that is added to object borders. class_weights : torch.Tensor, default=None     Class weights. A tensor of shape (n_classes,).</p>"},{"location":"api/losses/dice/#histolytics.losses.DiceLoss.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, target_weight: Tensor = None, mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the DICE coefficient.</p> <p>Parameters     yhat (torch.Tensor):         The prediction map. Shape (B, C, H, W).     target (torch.Tensor):         the ground truth annotations. Shape (B, H, W).     target_weight (torch.Tensor, default=None):         The edge weight map. Shape (B, H, W).     mask (torch.Tensor, default=None):         The mask map. Shape (B, H, W).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed DICE loss (scalar).</p>"},{"location":"api/losses/focal/","title":"FocalLoss","text":"<p>               Bases: <code>WeightedBaseLoss</code></p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.apply_spectral_decouple","title":"apply_spectral_decouple","text":"<pre><code>apply_spectral_decouple(loss_matrix: Tensor, yhat: Tensor, lam: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Apply spectral decoupling L2 norm after the loss.</p> <p>https://arxiv.org/abs/2011.09468</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>yhat</code> <code>Tensor</code> <p>The pixel predictions of the model. Shape (B, C, H, W).</p> required <code>lam</code> <code>float, default=0.01</code> <p>Lambda constant.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: SD-regularized loss matrix. Same shape as input.</p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.apply_ls_to_target","title":"apply_ls_to_target","text":"<pre><code>apply_ls_to_target(target: Tensor, n_classes: int, label_smoothing: float = 0.1) -&gt; torch.Tensor\n</code></pre> <p>Apply regular label smoothing to the target map.</p> <p>https://arxiv.org/abs/1512.00567</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>label_smoothing</code> <code>float, default=0.1</code> <p>The smoothing coeff alpha.</p> <code>0.1</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.apply_svls_to_target","title":"apply_svls_to_target","text":"<pre><code>apply_svls_to_target(target: Tensor, n_classes: int, kernel_size: int = 5, sigma: int = 3, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply spatially varying label smoothihng to target map.</p> <p>https://arxiv.org/abs/2104.05788</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>kernel_size</code> <code>int, default=3</code> <p>Size of a square kernel.</p> <code>5</code> <code>sigma</code> <code>int, default=3</code> <p>The std of the gaussian.</p> <code>3</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.apply_class_weights","title":"apply_class_weights","text":"<pre><code>apply_class_weights(loss_matrix: Tensor, target: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Multiply pixelwise loss matrix by the class weights.</p> Note <p>Does not apply normalization</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>target</code> <code>Tensor</code> <p>The target mask. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the weight matrix. Shape (B, H, W).</p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.apply_edge_weights","title":"apply_edge_weights","text":"<pre><code>apply_edge_weights(loss_matrix: Tensor, weight_map: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply weights to the object boundaries.</p> <p>Basically just computes <code>edge_weight</code>**<code>weight_map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>weight_map</code> <code>Tensor</code> <p>Map that points to the pixels that will be weighted. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the nuclear boundary weights. Shape (B, H, W).</p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.apply_mask_weight","title":"apply_mask_weight","text":"<pre><code>apply_mask_weight(loss_matrix: Tensor, mask: Tensor, norm: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Apply a mask to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>mask</code> <code>Tensor</code> <p>The mask. Shape (B, H, W).</p> required <code>norm</code> <code>bool, default=True</code> <p>If True, the loss matrix will be normalized by the mean of the mask.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the mask. Shape (B, H, W).</p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.__init__","title":"__init__","text":"<pre><code>__init__(alpha: float = 0.5, gamma: float = 2.0, apply_sd: bool = False, apply_ls: bool = False, apply_svls: bool = False, apply_mask: bool = False, edge_weight: float = None, class_weights: Tensor = None, **kwargs) -&gt; None\n</code></pre> <p>Focal loss.</p> <p>https://arxiv.org/abs/1708.02002</p> <p>Optionally applies, label smoothing, spatially varying label smoothing or weights at the object edges or class weights to the loss.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float, default=0.5</code> <p>Weight factor b/w [0,1].</p> <code>0.5</code> <code>gamma</code> <code>float, default=2.0</code> <p>Focusing factor.</p> <code>2.0</code> <code>apply_sd</code> <code>bool, default=False</code> <p>If True, Spectral decoupling regularization will be applied  to the loss matrix.</p> <code>False</code> <code>apply_ls</code> <code>bool, default=False</code> <p>If True, Label smoothing will be applied to the target.</p> <code>False</code> <code>apply_svls</code> <code>bool, default=False</code> <p>If True, spatially varying label smoothing will be applied to the target</p> <code>False</code> <code>apply_mask</code> <code>bool, default=False</code> <p>If True, a mask will be applied to the loss matrix. Mask shape: (B, H, W)</p> <code>False</code> <code>edge_weight</code> <code>float, default=none</code> <p>Weight that is added to object borders.</p> <code>None</code> <code>class_weights</code> <code>torch.Tensor, default=None</code> <p>Class weights. A tensor of shape (n_classes,).</p> <code>None</code>"},{"location":"api/losses/focal/#histolytics.losses.FocalLoss.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, target_weight: Tensor = None, mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the focal loss.</p> <p>Parameters:</p> Name Type Description Default <code>yhat</code> <code>Tensor</code> <p>The prediction map. Shape (B, C, H, W).</p> required <code>target</code> <code>Tensor</code> <p>the ground truth annotations. Shape (B, H, W).</p> required <code>target_weight</code> <code>torch.Tensor, default=None</code> <p>The edge weight map. Shape (B, H, W).</p> <code>None</code> <code>mask</code> <code>torch.Tensor, default=None</code> <p>The mask map. Shape (B, H, W).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed Focal loss (scalar).</p>"},{"location":"api/losses/joint_loss/","title":"JointLoss","text":"<p>               Bases: <code>ModuleDict</code></p>"},{"location":"api/losses/joint_loss/#histolytics.losses.JointLoss.__init__","title":"__init__","text":"<pre><code>__init__(losses: List[Module], weights: List[float] = None) -&gt; None\n</code></pre> <p>Joint loss function.</p> <p>Takes in a list of nn.Module losses and computes the loss for each loss in the list and at the end sums the outputs together as one joint loss.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>List[Module]</code> <p>List of initialized nn.Module losses.</p> required <code>weights</code> <code>List[float], default=None</code> <p>List of weights for each loss.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If more than 4 losses are given as input. If given weights are not between [0, 1].</p>"},{"location":"api/losses/joint_loss/#histolytics.losses.JointLoss.forward","title":"forward","text":"<pre><code>forward(**kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the joint-loss.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed joint loss.</p>"},{"location":"api/losses/joint_loss/#histolytics.losses.JointLoss.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/mae/","title":"MAE","text":"<p>               Bases: <code>WeightedBaseLoss</code></p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.apply_spectral_decouple","title":"apply_spectral_decouple","text":"<pre><code>apply_spectral_decouple(loss_matrix: Tensor, yhat: Tensor, lam: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Apply spectral decoupling L2 norm after the loss.</p> <p>https://arxiv.org/abs/2011.09468</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>yhat</code> <code>Tensor</code> <p>The pixel predictions of the model. Shape (B, C, H, W).</p> required <code>lam</code> <code>float, default=0.01</code> <p>Lambda constant.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: SD-regularized loss matrix. Same shape as input.</p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.apply_ls_to_target","title":"apply_ls_to_target","text":"<pre><code>apply_ls_to_target(target: Tensor, n_classes: int, label_smoothing: float = 0.1) -&gt; torch.Tensor\n</code></pre> <p>Apply regular label smoothing to the target map.</p> <p>https://arxiv.org/abs/1512.00567</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>label_smoothing</code> <code>float, default=0.1</code> <p>The smoothing coeff alpha.</p> <code>0.1</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.apply_svls_to_target","title":"apply_svls_to_target","text":"<pre><code>apply_svls_to_target(target: Tensor, n_classes: int, kernel_size: int = 5, sigma: int = 3, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply spatially varying label smoothihng to target map.</p> <p>https://arxiv.org/abs/2104.05788</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>kernel_size</code> <code>int, default=3</code> <p>Size of a square kernel.</p> <code>5</code> <code>sigma</code> <code>int, default=3</code> <p>The std of the gaussian.</p> <code>3</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.apply_class_weights","title":"apply_class_weights","text":"<pre><code>apply_class_weights(loss_matrix: Tensor, target: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Multiply pixelwise loss matrix by the class weights.</p> Note <p>Does not apply normalization</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>target</code> <code>Tensor</code> <p>The target mask. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the weight matrix. Shape (B, H, W).</p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.apply_edge_weights","title":"apply_edge_weights","text":"<pre><code>apply_edge_weights(loss_matrix: Tensor, weight_map: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply weights to the object boundaries.</p> <p>Basically just computes <code>edge_weight</code>**<code>weight_map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>weight_map</code> <code>Tensor</code> <p>Map that points to the pixels that will be weighted. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the nuclear boundary weights. Shape (B, H, W).</p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.apply_mask_weight","title":"apply_mask_weight","text":"<pre><code>apply_mask_weight(loss_matrix: Tensor, mask: Tensor, norm: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Apply a mask to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>mask</code> <code>Tensor</code> <p>The mask. Shape (B, H, W).</p> required <code>norm</code> <code>bool, default=True</code> <p>If True, the loss matrix will be normalized by the mean of the mask.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the mask. Shape (B, H, W).</p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/mae/#histolytics.losses.MAE.__init__","title":"__init__","text":"<pre><code>__init__(alpha: float = 0.0001, apply_sd: bool = False, apply_mask: bool = False, edge_weight: float = None, **kwargs) -&gt; None\n</code></pre> <p>Compute the MAE loss. Used in the stardist method.</p> Stardist <p>https://arxiv.org/pdf/1806.03535.pdf</p> Note <p>additionally apply spectral decoupling and edge weights to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>apply_sd</code> <code>bool, default=False</code> <p>If True, applies Spectral decoupling regularization to the loss matrix.</p> <code>False</code> <code>apply_mask</code> <code>bool, default=False</code> <p>If True, a mask will be applied to the loss matrix. Mask shape: (B, H, W)</p> <code>False</code> <code>edge_weight</code> <code>float, default=none</code> <p>Weight that is added to object borders.</p> <code>None</code>"},{"location":"api/losses/mae/#histolytics.losses.MAE.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, target_weight: Tensor = None, mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the masked MAE loss.</p> <p>Parameters:</p> Name Type Description Default <code>yhat</code> <code>Tensor</code> <p>The prediction map. Shape (B, C, H, W).</p> required <code>target</code> <code>Tensor</code> <p>the ground truth annotations. Shape (B, H, W).</p> required <code>target_weight</code> <code>torch.Tensor, default=None</code> <p>The edge weight map. Shape (B, H, W).</p> <code>None</code> <code>mask</code> <code>torch.Tensor, default=None</code> <p>The mask map. Shape (B, H, W).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed MAE loss (scalar).</p>"},{"location":"api/losses/mse/","title":"MSE","text":"<p>               Bases: <code>WeightedBaseLoss</code></p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.apply_spectral_decouple","title":"apply_spectral_decouple","text":"<pre><code>apply_spectral_decouple(loss_matrix: Tensor, yhat: Tensor, lam: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Apply spectral decoupling L2 norm after the loss.</p> <p>https://arxiv.org/abs/2011.09468</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>yhat</code> <code>Tensor</code> <p>The pixel predictions of the model. Shape (B, C, H, W).</p> required <code>lam</code> <code>float, default=0.01</code> <p>Lambda constant.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: SD-regularized loss matrix. Same shape as input.</p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.apply_ls_to_target","title":"apply_ls_to_target","text":"<pre><code>apply_ls_to_target(target: Tensor, n_classes: int, label_smoothing: float = 0.1) -&gt; torch.Tensor\n</code></pre> <p>Apply regular label smoothing to the target map.</p> <p>https://arxiv.org/abs/1512.00567</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>label_smoothing</code> <code>float, default=0.1</code> <p>The smoothing coeff alpha.</p> <code>0.1</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.apply_svls_to_target","title":"apply_svls_to_target","text":"<pre><code>apply_svls_to_target(target: Tensor, n_classes: int, kernel_size: int = 5, sigma: int = 3, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply spatially varying label smoothihng to target map.</p> <p>https://arxiv.org/abs/2104.05788</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>kernel_size</code> <code>int, default=3</code> <p>Size of a square kernel.</p> <code>5</code> <code>sigma</code> <code>int, default=3</code> <p>The std of the gaussian.</p> <code>3</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.apply_class_weights","title":"apply_class_weights","text":"<pre><code>apply_class_weights(loss_matrix: Tensor, target: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Multiply pixelwise loss matrix by the class weights.</p> Note <p>Does not apply normalization</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>target</code> <code>Tensor</code> <p>The target mask. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the weight matrix. Shape (B, H, W).</p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.apply_edge_weights","title":"apply_edge_weights","text":"<pre><code>apply_edge_weights(loss_matrix: Tensor, weight_map: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply weights to the object boundaries.</p> <p>Basically just computes <code>edge_weight</code>**<code>weight_map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>weight_map</code> <code>Tensor</code> <p>Map that points to the pixels that will be weighted. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the nuclear boundary weights. Shape (B, H, W).</p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.apply_mask_weight","title":"apply_mask_weight","text":"<pre><code>apply_mask_weight(loss_matrix: Tensor, mask: Tensor, norm: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Apply a mask to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>mask</code> <code>Tensor</code> <p>The mask. Shape (B, H, W).</p> required <code>norm</code> <code>bool, default=True</code> <p>If True, the loss matrix will be normalized by the mean of the mask.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the mask. Shape (B, H, W).</p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/mse/#histolytics.losses.MSE.__init__","title":"__init__","text":"<pre><code>__init__(apply_sd: bool = False, apply_ls: bool = False, apply_svls: bool = False, apply_mask: bool = False, edge_weight: float = None, class_weights: Tensor = None, **kwargs) -&gt; None\n</code></pre> <p>MSE-loss.</p> <p>Parameters:</p> Name Type Description Default <code>apply_sd</code> <code>bool, default=False</code> <p>If True, Spectral decoupling regularization will be applied  to the loss matrix.</p> <code>False</code> <code>apply_ls</code> <code>bool, default=False</code> <p>If True, Label smoothing will be applied to the target.</p> <code>False</code> <code>apply_svls</code> <code>bool, default=False</code> <p>If True, spatially varying label smoothing will be applied to the target</p> <code>False</code> <code>apply_mask</code> <code>bool, default=False</code> <p>If True, a mask will be applied to the loss matrix. Mask shape: (B, H, W)</p> <code>False</code> <code>edge_weight</code> <code>float, default=none</code> <p>Weight that is added to object borders.</p> <code>None</code> <code>class_weights</code> <code>torch.Tensor, default=None</code> <p>Class weights. A tensor of shape (n_classes,).</p> <code>None</code>"},{"location":"api/losses/mse/#histolytics.losses.MSE.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, target_weight: Tensor = None, mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the MSE-loss.</p> <p>Parameters:</p> Name Type Description Default <code>yhat</code> <code>Tensor</code> <p>The prediction map. Shape (B, C, H, W).</p> required <code>target</code> <code>Tensor</code> <p>the ground truth annotations. Shape (B, H, W).</p> required <code>target_weight</code> <code>torch.Tensor, default=None</code> <p>The edge weight map. Shape (B, H, W).</p> <code>None</code> <code>mask</code> <code>torch.Tensor, default=None</code> <p>The mask map. Shape (B, H, W).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed MSE loss (scalar).</p>"},{"location":"api/losses/multi_task_loss/","title":"MultiTaskLoss","text":"<p>               Bases: <code>ModuleDict</code></p>"},{"location":"api/losses/multi_task_loss/#histolytics.losses.MultiTaskLoss.__init__","title":"__init__","text":"<pre><code>__init__(head_losses: Dict[str, JointLoss], loss_weights: Dict[str, float] = None, **kwargs) -&gt; None\n</code></pre> <p>Multi-task loss wrapper.</p> <p>Combines losses from different heades to one loss function.</p> <p>Parameters:</p> Name Type Description Default <code>head_losses</code> <code>Dict[str, Module]</code> <p>Dictionary of head names mapped to a loss module. e.g. {\"inst\": JointLoss(MSE(), Dice()), \"type\": Dice()}.</p> required <code>loss_weights</code> <code>Dict[str, float], default=None</code> <p>Dictionary of head names mapped to the weight used for that head loss.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arguments have different lengths. If the input arguments have mismatching keys.</p>"},{"location":"api/losses/multi_task_loss/#histolytics.losses.MultiTaskLoss.forward","title":"forward","text":"<pre><code>forward(yhats: Dict[str, Tensor], targets: Dict[str, Tensor], mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the joint loss of the multi-task network.</p> <p>Parameters:</p> Name Type Description Default <code>yhats</code> <code>Dict[str, Tensor]</code> <p>Dictionary of head names mapped to the predicted masks. e.g. {\"inst\": (B, C, H, W), \"type\": (B, C, H, W)}.</p> required <code>targets</code> <code>Dict[str, Tensor]</code> <p>Dictionary of head names mapped to the GT masks. e.g. {\"inst\": (B, C, H, W), \"type\": (B, C, H, W)}.</p> required <code>mask</code> <code>torch.Tensor, default=None</code> <p>The mask for masked losses. Shape (B, H, W).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed multi-task loss (Scalar).</p>"},{"location":"api/losses/multi_task_loss/#histolytics.losses.MultiTaskLoss.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/ssim/","title":"SSIM","text":"<p>               Bases: <code>Module</code></p>"},{"location":"api/losses/ssim/#histolytics.losses.SSIM.__init__","title":"__init__","text":"<pre><code>__init__(window_size: int = 11, return_cs: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Structural similarity index loss.</p> <p>I.e. the dissimilarity: (1 - SSIM(x, y)) / 2</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int, default=11</code> <p>Size of the gaussian kernel.</p> <code>11</code> <code>return_cs</code> <code>bool, default=False</code> <p>Return also the the contrast sensitivity coeff.</p> <code>False</code>"},{"location":"api/losses/ssim/#histolytics.losses.SSIM.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the SSIM loss.</p> <p>Parameters     yhat (torch.Tensor):         The prediction map. Shape (B, C, H, W).     target (torch.Tensor):         the ground truth annotations. Shape (B, H, W).     target_weight (torch.Tensor, default=None):         The edge weight map. Shape (B, H, W).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed SSIM loss (scalar).</p>"},{"location":"api/losses/tversky_loss/","title":"TverskyLoss","text":"<p>               Bases: <code>WeightedBaseLoss</code></p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.apply_spectral_decouple","title":"apply_spectral_decouple","text":"<pre><code>apply_spectral_decouple(loss_matrix: Tensor, yhat: Tensor, lam: float = 0.01) -&gt; torch.Tensor\n</code></pre> <p>Apply spectral decoupling L2 norm after the loss.</p> <p>https://arxiv.org/abs/2011.09468</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>yhat</code> <code>Tensor</code> <p>The pixel predictions of the model. Shape (B, C, H, W).</p> required <code>lam</code> <code>float, default=0.01</code> <p>Lambda constant.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: SD-regularized loss matrix. Same shape as input.</p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.apply_ls_to_target","title":"apply_ls_to_target","text":"<pre><code>apply_ls_to_target(target: Tensor, n_classes: int, label_smoothing: float = 0.1) -&gt; torch.Tensor\n</code></pre> <p>Apply regular label smoothing to the target map.</p> <p>https://arxiv.org/abs/1512.00567</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>label_smoothing</code> <code>float, default=0.1</code> <p>The smoothing coeff alpha.</p> <code>0.1</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.apply_svls_to_target","title":"apply_svls_to_target","text":"<pre><code>apply_svls_to_target(target: Tensor, n_classes: int, kernel_size: int = 5, sigma: int = 3, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply spatially varying label smoothihng to target map.</p> <p>https://arxiv.org/abs/2104.05788</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target one hot tensor. Shape (B, C, H, W). Dtype: Int64.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the data.</p> required <code>kernel_size</code> <code>int, default=3</code> <p>Size of a square kernel.</p> <code>5</code> <code>sigma</code> <code>int, default=3</code> <p>The std of the gaussian.</p> <code>3</code> Retrurns <p>Torch.Tensor:     Label smoothed target. Same shape as input.</p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.apply_class_weights","title":"apply_class_weights","text":"<pre><code>apply_class_weights(loss_matrix: Tensor, target: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Multiply pixelwise loss matrix by the class weights.</p> Note <p>Does not apply normalization</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>target</code> <code>Tensor</code> <p>The target mask. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the weight matrix. Shape (B, H, W).</p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.apply_edge_weights","title":"apply_edge_weights","text":"<pre><code>apply_edge_weights(loss_matrix: Tensor, weight_map: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply weights to the object boundaries.</p> <p>Basically just computes <code>edge_weight</code>**<code>weight_map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>weight_map</code> <code>Tensor</code> <p>Map that points to the pixels that will be weighted. Shape (B, H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the nuclear boundary weights. Shape (B, H, W).</p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.apply_mask_weight","title":"apply_mask_weight","text":"<pre><code>apply_mask_weight(loss_matrix: Tensor, mask: Tensor, norm: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Apply a mask to the loss matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loss_matrix</code> <code>Tensor</code> <p>Pixelwise losses. A tensor of shape (B, H, W).</p> required <code>mask</code> <code>Tensor</code> <p>The mask. Shape (B, H, W).</p> required <code>norm</code> <code>bool, default=True</code> <p>If True, the loss matrix will be normalized by the mean of the mask.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The loss matrix scaled with the mask. Shape (B, H, W).</p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre> <p>Add info to print.</p>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.__init__","title":"__init__","text":"<pre><code>__init__(alpha: float = 0.7, beta: float = 0.3, apply_sd: bool = False, apply_ls: bool = False, apply_svls: bool = False, apply_mask: bool = False, edge_weight: float = None, class_weights: Tensor = None, **kwargs) -&gt; None\n</code></pre> <p>Tversky loss.</p> <p>https://arxiv.org/abs/1706.05721</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float, default=0.7</code> <p>False positive dice coefficient.</p> <code>0.7</code> <code>beta</code> <code>float, default=0.3</code> <p>False negative tanimoto coefficient.</p> <code>0.3</code> <code>apply_sd</code> <code>bool, default=False</code> <p>If True, Spectral decoupling regularization will be applied  to the loss matrix.</p> <code>False</code> <code>apply_ls</code> <code>bool, default=False</code> <p>If True, Label smoothing will be applied to the target.</p> <code>False</code> <code>apply_svls</code> <code>bool, default=False</code> <p>If True, spatially varying label smoothing will be applied to the target</p> <code>False</code> <code>apply_mask</code> <code>bool, default=False</code> <p>If True, a mask will be applied to the loss matrix. Mask shape: (B, H, W)</p> <code>False</code> <code>edge_weight</code> <code>float, default=none</code> <p>Weight that is added to object borders.</p> <code>None</code> <code>class_weights</code> <code>torch.Tensor, default=None</code> <p>Class weights. A tensor of shape (n_classes,).</p> <code>None</code>"},{"location":"api/losses/tversky_loss/#histolytics.losses.TverskyLoss.forward","title":"forward","text":"<pre><code>forward(yhat: Tensor, target: Tensor, target_weight: Tensor = None, mask: Tensor = None, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Compute the Tversky loss.</p> <p>Parameters:</p> Name Type Description Default <code>yhat</code> <code>Tensor</code> <p>The prediction map. Shape (B, C, H, W).</p> required <code>target</code> <code>Tensor</code> <p>the ground truth annotations. Shape (B, H, W).</p> required <code>target_weight</code> <code>torch.Tensor, default=None</code> <p>The edge weight map. Shape (B, H, W).</p> <code>None</code> <code>mask</code> <code>torch.Tensor, default=None</code> <p>The mask map. Shape (B, H, W).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed Tversky loss (scalar).</p>"},{"location":"api/metrics/accuracy_multiclass/","title":"accuracy_multiclass","text":"<p>Compute multi-class accuracy for semantic segmentation masks.</p>"},{"location":"api/metrics/accuracy_multiclass/#histolytics.metrics.accuracy_multiclass--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth semantic mask. Shape (H, W).\npred : np.ndarray\n    Predicted semantic mask. Shape (H, W).\nnum_classes : int\n    Number of classes in the training dataset.\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\nclamp_absent: bool = True\n    If a class is not present in either true or pred, the value of that ix\n    in the result array will be clamped to -1.0.\n</code></pre>"},{"location":"api/metrics/accuracy_multiclass/#histolytics.metrics.accuracy_multiclass--returns","title":"Returns","text":"<pre><code>np.ndarray:\n    Per class accuracy-metrics. Shape: (num_classes,).\n</code></pre>"},{"location":"api/metrics/aggregated_jaccard_index/","title":"aggregated_jaccard_index","text":"<p>Compute the aggregated jaccard index (AJI) for a labelled mask.</p>"},{"location":"api/metrics/aggregated_jaccard_index/#histolytics.metrics.aggregated_jaccard_index--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth (labelled mask). Shape (H, W).\npred : np.ndarray\n    Predicted (labelled mask). Shape (H, W).\nthresh : float, default=0.5\n    Threshold for the iou to include the prediction as TP\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\n</code></pre>"},{"location":"api/metrics/aggregated_jaccard_index/#histolytics.metrics.aggregated_jaccard_index--returns","title":"Returns","text":"<pre><code>float:\n    The computed aji.\n</code></pre>"},{"location":"api/metrics/average_precision/","title":"average_precision","text":"<p>Compute the average precision of a labelled mask.</p>"},{"location":"api/metrics/average_precision/#histolytics.metrics.average_precision--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth (labelled mask). Shape (H, W).\npred : np.ndarray\n    Predicted (labelled mask). Shape (H, W).\nthresh : float, default=0.5\n    Threshold for the iou to include the prediction as TP\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\n</code></pre>"},{"location":"api/metrics/average_precision/#histolytics.metrics.average_precision--returns","title":"Returns","text":"<pre><code>float:\n    The computed precision.\n</code></pre>"},{"location":"api/metrics/dice2/","title":"dice2","text":"<p>Compute the DICE2 metric for a labelled mask.</p>"},{"location":"api/metrics/dice2/#histolytics.metrics.dice2--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth (labelled mask). Shape (H, W).\npred : np.ndarray\n    Predicted (labelled mask). Shape (H, W).\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\n</code></pre>"},{"location":"api/metrics/dice2/#histolytics.metrics.dice2--returns","title":"Returns","text":"<pre><code>float:\n    The computed dice2 metric.\n</code></pre>"},{"location":"api/metrics/dice_multiclass/","title":"dice_multiclass","text":"<p>Compute multi-class dice for semantic segmentation masks.</p>"},{"location":"api/metrics/dice_multiclass/#histolytics.metrics.dice_multiclass--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth semantic mask. Shape (H, W).\npred : np.ndarray\n    Predicted semantic mask. Shape (H, W).\nnum_classes : int\n    Number of classes in the training dataset.\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\nclamp_absent: bool = True\n    If a class is not present in either true or pred, the value of that ix\n    in the result array will be clamped to -1.0.\n</code></pre>"},{"location":"api/metrics/dice_multiclass/#histolytics.metrics.dice_multiclass--returns","title":"Returns","text":"<pre><code>np.ndarray:\n    Per class dice-metrics. Shape: (num_classes,).\n</code></pre>"},{"location":"api/metrics/f1score_multiclass/","title":"f1score_multiclass","text":"<p>Compute multi-class f1-score for semantic segmentation masks.</p>"},{"location":"api/metrics/f1score_multiclass/#histolytics.metrics.f1score_multiclass--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth semantic mask. Shape (H, W).\npred : np.ndarray\n    Predicted semantic mask. Shape (H, W).\nnum_classes : int\n    Number of classes in the training dataset.\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\nclamp_absent: bool = True\n    If a class is not present in either true or pred, the value of that ix\n    in the result array will be clamped to -1.0.\n</code></pre>"},{"location":"api/metrics/f1score_multiclass/#histolytics.metrics.f1score_multiclass--returns","title":"Returns","text":"<pre><code>np.ndarray:\n    Per class f1score-metrics. Shape: (num_classes,).\n</code></pre>"},{"location":"api/metrics/iou_multiclass/","title":"iou_multiclass","text":"<p>Compute multi-class intersection over union for semantic segmentation masks.</p>"},{"location":"api/metrics/iou_multiclass/#histolytics.metrics.iou_multiclass--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth semantic mask. Shape (H, W).\npred : np.ndarray\n    Predicted semantic mask. Shape (H, W).\nnum_classes : int\n    Number of classes in the training dataset.\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\nclamp_absent : bool, default=True\n    If a class is not present in either true or pred, the value of that ix\n    in the result array will be clamped to -1.0.\n</code></pre>"},{"location":"api/metrics/iou_multiclass/#histolytics.metrics.iou_multiclass--returns","title":"Returns","text":"<pre><code>np.ndarray:\n    Per class IoU-metrics. Shape: (num_classes,).\n</code></pre>"},{"location":"api/metrics/pairwise_object_stats/","title":"pairwise_object_stats","text":"<p>Compute the TP, FP, FN objects from a boolean contigency table.</p>"},{"location":"api/metrics/pairwise_object_stats/#histolytics.metrics.pairwise_object_stats--parameters","title":"Parameters","text":"<pre><code>matches : np.ndarray\n    A pairwise boolean matrix where True values at pos (i, j)\n    indicate correctly detected objects for the corresponding\n    labels i and j. Shape: (n_labels_gt, n_labels_pred).\nsum_reduce : bool, default=True\n    Reduce the boolean indice arrays by summing to get the correct\n    number of TP, FP, and FN objects.\n</code></pre>"},{"location":"api/metrics/pairwise_object_stats/#histolytics.metrics.pairwise_object_stats--returns","title":"Returns","text":"<pre><code>Tuple[int, int, int]:\n    The number of TP objects, FP objects, and FN objects in\n    a labelled mask.\n</code></pre>"},{"location":"api/metrics/pairwise_pixel_stats/","title":"pairwise_pixel_stats","text":"<p>Compute the # of TP, FP, FN pixels for each object in a labelled/semantic mask.</p> <p>Optionally a binary metric can be computed instead of the satistics. Atleast 2x faster than computing with <code>np.histogram2d</code>.</p>"},{"location":"api/metrics/pairwise_pixel_stats/#histolytics.metrics.pairwise_pixel_stats--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth (semantic or labelled mask). Shape (H, W).\npred : np.ndarray\n    Predicted (semantic or labelled mask). Shape (H, W).\nnum_classes : int, optional\n    Number of classes in the dataset. If None, stats are computed for instances.\n    If not None stats are computed for classes i.e. semantic segmentation masks.\nmetric_func : Callable, optional\n    A binary metric function. e.g. `iou_score` or `dice`.\n</code></pre>"},{"location":"api/metrics/pairwise_pixel_stats/#histolytics.metrics.pairwise_pixel_stats--returns","title":"Returns","text":"<pre><code>List[np.ndarray, ...] or None:\n    A List of 2D arrays (i, j) where i corresponds to a ground\n    truth label and j corresponds to a predicted label. Each value\n    of the matrix is the computed statistic or metric at pos (i, j).\n    By default. returns the tp, fp, and fn matrices.\n\n    If stats computed for instances:\n        Shape: (n_labels_gt, n_labels_pred). Dtype. float64.\n    If stats computed for classes:\n        Shape: (num_classes, num_classes). Dtype. float64.\n</code></pre>"},{"location":"api/metrics/panoptic_quality/","title":"panoptic_quality","text":"<p>Compute the panoptic quality of a lebelled mask.</p>"},{"location":"api/metrics/panoptic_quality/#histolytics.metrics.panoptic_quality--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth (labelled mask). Shape (H, W).\npred : np.ndarray\n    Predicted (labelled mask). Shape (H, W).\nthresh : float, default=0.5\n    Threshold for the iou to include the prediction as TP\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\n</code></pre>"},{"location":"api/metrics/panoptic_quality/#histolytics.metrics.panoptic_quality--returns","title":"Returns","text":"<pre><code>Dict[str, float]:\n    Dictionary containing the detection quality (dq), segmentation\n    quality (sq) and panoptic quality (pq) values.\n</code></pre>"},{"location":"api/metrics/sensitivity_multiclass/","title":"sensitivity_multiclass","text":"<p>Compute multi-class sensitivity for semantic segmentation masks.</p>"},{"location":"api/metrics/sensitivity_multiclass/#histolytics.metrics.sensitivity_multiclass--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth semantic mask. Shape (H, W).\npred : np.ndarray\n    Predicted semantic mask. Shape (H, W).\nnum_classes : int\n    Number of classes in the training dataset.\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\nclamp_absent: bool = True\n    If a class is not present in either true or pred, the value of that ix\n    in the result array will be clamped to -1.0.\n</code></pre>"},{"location":"api/metrics/sensitivity_multiclass/#histolytics.metrics.sensitivity_multiclass--returns","title":"Returns","text":"<pre><code>np.ndarray:\n    Per class sensitivity-metrics. Shape: (num_classes,).\n</code></pre>"},{"location":"api/metrics/specificity_multiclass/","title":"specificity_multiclass","text":"<p>Compute multi-class specificity for semantic segmentation masks.</p>"},{"location":"api/metrics/specificity_multiclass/#histolytics.metrics.specificity_multiclass--parameters","title":"Parameters","text":"<pre><code>true : np.ndarray\n    Ground truth semantic mask. Shape (H, W).\npred : np.ndarray\n    Predicted semantic mask. Shape (H, W).\nnum_classes : int\n    Number of classes in the training dataset.\neps : float, default=1e-8:\n    Epsilon to avoid zero div errors.\nclamp_absent: bool = True\n    If a class is not present in either true or pred, the value of that ix\n    in the result array will be clamped to -1.0.\n</code></pre>"},{"location":"api/metrics/specificity_multiclass/#histolytics.metrics.specificity_multiclass--returns","title":"Returns","text":"<pre><code>np.ndarray:\n    Per class specificity-metrics. Shape: (num_classes,).\n</code></pre>"},{"location":"api/models/cellpose_panoptic/","title":"CellposePanoptic","text":"<p>               Bases: <code>BaseModelPanoptic</code></p> Source code in <code>src/histolytics/models/cellpose_panoptic.py</code> <pre><code>class CellposePanoptic(BaseModelPanoptic):\n    model_name = \"cellpose_panoptic\"\n\n    def __init__(\n        self,\n        n_nuc_classes: int,\n        n_tissue_classes: int,\n        enc_name: str = \"efficientnet_b5\",\n        enc_pretrain: bool = True,\n        enc_freeze: bool = False,\n        device: torch.device = torch.device(\"cuda\"),\n        model_kwargs: Dict[str, Any] = {},\n    ) -&gt; None:\n        \"\"\"CellposePanoptic model for panoptic segmentation of nuclei and tissues.\n\n        Note:\n            - [Cellpose article](https://www.nature.com/articles/s41592-020-01018-x)\n\n        Parameters:\n            n_nuc_classes (int):\n                Number of nuclei type classes.\n            n_tissue_classes (int):\n                Number of tissue type classes.\n            enc_name (str):\n                Name of the pytorch-image-models encoder.\n            enc_pretrain (bool):\n                Whether to use pretrained weights in the encoder.\n            enc_freeze (bool):\n                Freeze encoder weights for training.\n            device (torch.device):\n                Device to run the model on.\n            model_kwargs (Dict[str, Any]):\n                Additional keyword arguments for the model.\n        \"\"\"\n        super().__init__()\n        self.model = cellpose_panoptic(\n            n_nuc_classes,\n            n_tissue_classes,\n            enc_name=enc_name,\n            enc_pretrain=enc_pretrain,\n            enc_freeze=enc_freeze,\n            **model_kwargs,\n        )\n\n        self.device = device\n        self.model.to(device)\n\n    def set_inference_mode(\n        self,\n        mixed_precision: bool = True,\n        postproc_kwargs: Dict[str, Any] = {\"use_gpu\": True},\n    ) -&gt; None:\n        \"\"\"Set to model inference mode.\"\"\"\n        self.model.eval()\n        self.predictor = Predictor(\n            model=self.model,\n            mixed_precision=mixed_precision,\n        )\n        self.post_processor = PostProcessor(\n            postproc_method=\"cellpose\",\n            postproc_kwargs=postproc_kwargs,\n        )\n        self.inference_mode = True\n</code></pre>"},{"location":"api/models/cellpose_panoptic/#histolytics.models.cellpose_panoptic.CellposePanoptic.__init__","title":"__init__","text":"<pre><code>__init__(n_nuc_classes: int, n_tissue_classes: int, enc_name: str = 'efficientnet_b5', enc_pretrain: bool = True, enc_freeze: bool = False, device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {}) -&gt; None\n</code></pre> <p>CellposePanoptic model for panoptic segmentation of nuclei and tissues.</p> Note <ul> <li>Cellpose article</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n_nuc_classes</code> <code>int</code> <p>Number of nuclei type classes.</p> required <code>n_tissue_classes</code> <code>int</code> <p>Number of tissue type classes.</p> required <code>enc_name</code> <code>str</code> <p>Name of the pytorch-image-models encoder.</p> <code>'efficientnet_b5'</code> <code>enc_pretrain</code> <code>bool</code> <p>Whether to use pretrained weights in the encoder.</p> <code>True</code> <code>enc_freeze</code> <code>bool</code> <p>Freeze encoder weights for training.</p> <code>False</code> <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>src/histolytics/models/cellpose_panoptic.py</code> <pre><code>def __init__(\n    self,\n    n_nuc_classes: int,\n    n_tissue_classes: int,\n    enc_name: str = \"efficientnet_b5\",\n    enc_pretrain: bool = True,\n    enc_freeze: bool = False,\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n) -&gt; None:\n    \"\"\"CellposePanoptic model for panoptic segmentation of nuclei and tissues.\n\n    Note:\n        - [Cellpose article](https://www.nature.com/articles/s41592-020-01018-x)\n\n    Parameters:\n        n_nuc_classes (int):\n            Number of nuclei type classes.\n        n_tissue_classes (int):\n            Number of tissue type classes.\n        enc_name (str):\n            Name of the pytorch-image-models encoder.\n        enc_pretrain (bool):\n            Whether to use pretrained weights in the encoder.\n        enc_freeze (bool):\n            Freeze encoder weights for training.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (Dict[str, Any]):\n            Additional keyword arguments for the model.\n    \"\"\"\n    super().__init__()\n    self.model = cellpose_panoptic(\n        n_nuc_classes,\n        n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=enc_pretrain,\n        enc_freeze=enc_freeze,\n        **model_kwargs,\n    )\n\n    self.device = device\n    self.model.to(device)\n</code></pre>"},{"location":"api/models/cellpose_panoptic/#histolytics.models.cellpose_panoptic.CellposePanoptic.set_inference_mode","title":"set_inference_mode","text":"<pre><code>set_inference_mode(mixed_precision: bool = True, postproc_kwargs: Dict[str, Any] = {'use_gpu': True}) -&gt; None\n</code></pre> <p>Set to model inference mode.</p> Source code in <code>src/histolytics/models/cellpose_panoptic.py</code> <pre><code>def set_inference_mode(\n    self,\n    mixed_precision: bool = True,\n    postproc_kwargs: Dict[str, Any] = {\"use_gpu\": True},\n) -&gt; None:\n    \"\"\"Set to model inference mode.\"\"\"\n    self.model.eval()\n    self.predictor = Predictor(\n        model=self.model,\n        mixed_precision=mixed_precision,\n    )\n    self.post_processor = PostProcessor(\n        postproc_method=\"cellpose\",\n        postproc_kwargs=postproc_kwargs,\n    )\n    self.inference_mode = True\n</code></pre>"},{"location":"api/models/cellpose_panoptic/#histolytics.models.cellpose_panoptic.CellposePanoptic.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(weights: Union[str, Path], device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {})\n</code></pre> <p>Load the model from pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the pretrained model.</p> required <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments for the model.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    weights: Union[str, Path],\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n):\n    \"\"\"Load the model from pretrained weights.\n\n    Parameters:\n        model_name (str):\n            Name of the pretrained model.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (Dict[str, Any]):\n            Additional arguments for the model.\n\n    Examples:\n        &gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n    \"\"\"\n    weights_path = Path(weights)\n    if not weights_path.is_file():\n        if weights_path.as_posix() in PRETRAINED_MODELS[cls.model_name].keys():\n            weights_path = Path(\n                hf_hub_download(\n                    repo_id=PRETRAINED_MODELS[cls.model_name][weights][\"repo_id\"],\n                    filename=PRETRAINED_MODELS[cls.model_name][weights][\"filename\"],\n                )\n            )\n\n        else:\n            raise ValueError(\n                \"Please provide a valid path. or a pre-trained model downloaded from the\"\n                f\" histolytics-hub. One of {list(PRETRAINED_MODELS[cls.model_name].keys())}.\"\n            )\n\n    enc_name, n_nuc_classes, n_tissue_classes, state_dict = cls._get_state_dict(\n        weights_path, device=device\n    )\n\n    model_inst = cls(\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=False,\n        enc_freeze=False,\n        device=device,\n        model_kwargs=model_kwargs,\n    )\n\n    if weights_path.suffix == \".safetensors\":\n        try:\n            from safetensors.torch import load_model\n        except ImportError:\n            raise ImportError(\n                \"Please install `safetensors` package to load .safetensors files.\"\n            )\n        load_model(model_inst.model, weights_path, device.type)\n    else:\n        model_inst.model.load_state_dict(state_dict, strict=True)\n\n    try:\n        cls.nuc_classes = MODEL_CLASS_DICTS[weights][\"nuc\"]\n        cls.tissue_classes = MODEL_CLASS_DICTS[weights][\"tissue\"]\n    except KeyError:\n        # if the model is not in the class dict, set to None\n        cls.nuc_classes = None\n        cls.tissue_classes = None\n\n    return model_inst\n</code></pre>"},{"location":"api/models/cellpose_panoptic/#histolytics.models.cellpose_panoptic.CellposePanoptic.predict","title":"predict","text":"<pre><code>predict(x: Union[Tensor, ndarray, Image], *, use_sliding_win: bool = False, window_size: Tuple[int, int] = None, stride: int = None) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]\n</code></pre> <p>Predict the input image or image batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, ndarray, Image]</code> <p>Input image (H, W, C) or input image batch (B, C, H, W).</p> required <code>use_sliding_win</code> <code>bool</code> <p>Whether to use sliding window for prediction.</p> <code>False</code> <code>window_size</code> <code>Tuple[int, int]</code> <p>The height and width of the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride for the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]: Dictionary of soft outputs:</p> <pre><code>- \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n- \"tissue\": SoftSemanticOutput (type_map).\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; # with sliding window if image is large\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n&gt;&gt;&gt; # without sliding window if image is small enough\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def predict(\n    self,\n    x: Union[torch.Tensor, np.ndarray, Image],\n    *,\n    use_sliding_win: bool = False,\n    window_size: Tuple[int, int] = None,\n    stride: int = None,\n) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n    \"\"\"Predict the input image or image batch.\n\n    Parameters:\n        x (Union[torch.Tensor, np.ndarray, Image]):\n            Input image (H, W, C) or input image batch (B, C, H, W).\n        use_sliding_win (bool):\n            Whether to use sliding window for prediction.\n        window_size (Tuple[int, int]):\n            The height and width of the sliding window. If `use_sliding_win` is False\n            this argument is ignored.\n        stride (int):\n            The stride for the sliding window. If `use_sliding_win` is False this\n            argument is ignored.\n\n    Returns:\n        Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n            Dictionary of soft outputs:\n\n                - \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n                - \"tissue\": SoftSemanticOutput (type_map).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; # with sliding window if image is large\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n        &gt;&gt;&gt; # without sliding window if image is small enough\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\"Run `.set_inference_mode()` before running `predict`\")\n\n    if not use_sliding_win:\n        x = self.predictor.predict(x=x, apply_boundary_weight=False)\n    else:\n        if window_size is None:\n            raise ValueError(\n                \"`window_size` must be provided when using sliding window.\"\n            )\n        if stride is None:\n            raise ValueError(\"`stride` must be provided when using sliding window.\")\n\n        x = self.predictor.predict_sliding_win(\n            x=x, window_size=window_size, stride=stride, apply_boundary_weight=True\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/cellpose_panoptic/#histolytics.models.cellpose_panoptic.CellposePanoptic.post_process","title":"post_process","text":"<pre><code>post_process(x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]], *, use_async_postproc: bool = True, start_method: str = 'threading', n_jobs: int = 4, save_paths_nuc: List[Union[Path, str]] = None, save_paths_cyto: List[Union[Path, str]] = None, save_paths_tissue: List[Union[Path, str]] = None, coords: List[Tuple[int, int, int, int]] = None, class_dict_nuc: Dict[int, str] = None, class_dict_cyto: Dict[int, str] = None, class_dict_tissue: Dict[int, str] = None, nuc_smooth_func: Callable = gaussian_smooth, cyto_smooth_func: Callable = gaussian_smooth, tissue_smooth_func: Callable = None) -&gt; Dict[str, List[np.ndarray]]\n</code></pre> <p>Post-process the output of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>The output of the .predict() method.</p> required <code>use_async_postproc</code> <code>bool</code> <p>Whether to use async post-processing. Can give some run-time benefits.</p> <code>True</code> <code>start_method</code> <code>str</code> <p>The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.</p> <code>'threading'</code> <code>n_jobs</code> <code>int</code> <p>The number of workers for the post-processing.</p> <code>4</code> <code>save_paths_nuc</code> <code>List[Union[Path, str]]</code> <p>The paths to save the panlei masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_cyto</code> <code>List[Union[Path, str]]</code> <p>The paths to save the cytoplasm masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_tissue</code> <code>List[Union[Path, str]]</code> <p>The paths to save the tissue masks. If None, the masks are not saved.</p> <code>None</code> <code>coords</code> <code>List[Tuple[int, int, int, int]]</code> <p>The XYWH coordinates of the image patch. If not None, the coordinates are saved in the filenames of outputs.</p> <code>None</code> <code>class_dict_nuc</code> <code>Dict[int, str]</code> <p>The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}</p> <code>None</code> <code>class_dict_cyto</code> <code>Dict[int, str]</code> <p>The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}</p> <code>None</code> <code>class_dict_tissue</code> <code>Dict[int, str]</code> <p>The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}</p> <code>None</code> <code>nuc_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the nuclei instance maps before post-processing. If None, no smoothing is applied. This is only used when nuclei segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_nuc is None.</p> <code>gaussian_smooth</code> <code>cyto_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the cytoplasm instance maps before post-processing. If None, no smoothing is applied. This is only used when cytoplasm segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_cyto is None.</p> <code>gaussian_smooth</code> <code>tissue_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the tissue type maps before post-processing. If None, no smoothing is applied. This is only used when tissue segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_tissue is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[ndarray]]</code> <p>Dict[str, List[np.ndarray]]: Dictionary of post-processed outputs:</p> <ul> <li>\"nuclei\": List of output nuclei masks (H, W).</li> <li>\"cyto\": List of output cytoplasm masks (H, W).</li> <li>\"tissue\": List of output tissue masks (H, W).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n&gt;&gt;&gt; x = my_model.post_process(\n...     x,\n...     use_async_postproc=True,\n...     start_method=\"threading\",\n...     n_jobs=4,\n... )\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def post_process(\n    self,\n    x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]],\n    *,\n    use_async_postproc: bool = True,\n    start_method: str = \"threading\",\n    n_jobs: int = 4,\n    save_paths_nuc: List[Union[Path, str]] = None,\n    save_paths_cyto: List[Union[Path, str]] = None,\n    save_paths_tissue: List[Union[Path, str]] = None,\n    coords: List[Tuple[int, int, int, int]] = None,\n    class_dict_nuc: Dict[int, str] = None,\n    class_dict_cyto: Dict[int, str] = None,\n    class_dict_tissue: Dict[int, str] = None,\n    nuc_smooth_func: Callable = gaussian_smooth,\n    cyto_smooth_func: Callable = gaussian_smooth,\n    tissue_smooth_func: Callable = None,\n) -&gt; Dict[str, List[np.ndarray]]:\n    \"\"\"Post-process the output of the model.\n\n    Parameters:\n        x (Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]):\n            The output of the .predict() method.\n        use_async_postproc (bool):\n            Whether to use async post-processing. Can give some run-time benefits.\n        start_method (str):\n            The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.\n        n_jobs (int):\n            The number of workers for the post-processing.\n        save_paths_nuc (List[Union[Path, str]]):\n            The paths to save the panlei masks. If None, the masks are not saved.\n        save_paths_cyto (List[Union[Path, str]]):\n            The paths to save the cytoplasm masks. If None, the masks are not saved.\n        save_paths_tissue (List[Union[Path, str]]):\n            The paths to save the tissue masks. If None, the masks are not saved.\n        coords (List[Tuple[int, int, int, int]]):\n            The XYWH coordinates of the image patch. If not None, the coordinates are\n            saved in the filenames of outputs.\n        class_dict_nuc (Dict[int, str]):\n            The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}\n        class_dict_cyto (Dict[int, str]):\n            The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}\n        class_dict_tissue (Dict[int, str]):\n            The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}\n        nuc_smooth_func (Callable):\n            The smoothing function to apply to the nuclei instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            nuclei segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_nuc is None.\n        cyto_smooth_func (Callable):\n            The smoothing function to apply to the cytoplasm instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            cytoplasm segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_cyto is None.\n        tissue_smooth_func (Callable):\n            The smoothing function to apply to the tissue type maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            tissue segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_tissue is None.\n\n    Returns:\n        Dict[str, List[np.ndarray]]:\n            Dictionary of post-processed outputs:\n\n            - \"nuclei\": List of output nuclei masks (H, W).\n            - \"cyto\": List of output cytoplasm masks (H, W).\n            - \"tissue\": List of output tissue masks (H, W).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n        &gt;&gt;&gt; x = my_model.post_process(\n        ...     x,\n        ...     use_async_postproc=True,\n        ...     start_method=\"threading\",\n        ...     n_jobs=4,\n        ... )\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\n            \"Run `.set_inference_mode()` before running `post_process`\"\n        )\n\n    # if batch size is 1, run serially\n    if x[\"tissue\"].type_map.shape[0] == 1:\n        return self.post_processor.postproc_serial(\n            x,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n        )\n\n    if use_async_postproc:\n        x = self.post_processor.postproc_parallel_async(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n    else:\n        x = self.post_processor.postproc_parallel(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/cellvit_panoptic/","title":"CellVitPanoptic","text":"<p>               Bases: <code>BaseModelPanoptic</code></p> Source code in <code>src/histolytics/models/cellvit_panoptic.py</code> <pre><code>class CellVitPanoptic(BaseModelPanoptic):\n    model_name = \"cellvit_panoptic\"\n\n    def __init__(\n        self,\n        n_nuc_classes: int,\n        n_tissue_classes: int,\n        enc_name: str = \"samvit_base_patch16\",\n        enc_pretrain: bool = True,\n        enc_freeze: bool = False,\n        device: torch.device = torch.device(\"cuda\"),\n        model_kwargs: Dict[str, Any] = {},\n    ) -&gt; None:\n        \"\"\"CellVitPanoptic model for panoptic segmentation of nuclei and tissues.\n\n        Note:\n            [CellVit article](https://arxiv.org/abs/2306.15350)\n\n        Parameters:\n            n_nuc_classes (int):\n                Number of nuclei type classes.\n            n_tissue_classes (int):\n                Number of tissue type classes.\n            enc_name (str):\n                Name of the pytorch-image-models encoder.\n            enc_pretrain (bool):\n                Whether to use pretrained weights in the encoder.\n            enc_freeze (bool):\n                Freeze encoder weights for training.\n            device (torch.device):\n                Device to run the model on.\n            model_kwargs (dict):\n                Additional keyword arguments for the model.\n        \"\"\"\n        super().__init__()\n        self.model = cellvit_panoptic(\n            n_nuc_classes=n_nuc_classes,\n            n_tissue_classes=n_tissue_classes,\n            enc_name=enc_name,\n            enc_pretrain=enc_pretrain,\n            enc_freeze=enc_freeze,\n            **model_kwargs,\n        )\n\n        self.device = device\n        self.model.to(device)\n\n    def set_inference_mode(self, mixed_precision: bool = True) -&gt; None:\n        \"\"\"Set model to inference mode.\"\"\"\n        self.model.eval()\n        self.predictor = Predictor(\n            model=self.model,\n            mixed_precision=mixed_precision,\n        )\n        self.post_processor = PostProcessor(postproc_method=\"hovernet\")\n        self.inference_mode = True\n</code></pre>"},{"location":"api/models/cellvit_panoptic/#histolytics.models.cellvit_panoptic.CellVitPanoptic.__init__","title":"__init__","text":"<pre><code>__init__(n_nuc_classes: int, n_tissue_classes: int, enc_name: str = 'samvit_base_patch16', enc_pretrain: bool = True, enc_freeze: bool = False, device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {}) -&gt; None\n</code></pre> <p>CellVitPanoptic model for panoptic segmentation of nuclei and tissues.</p> Note <p>CellVit article</p> <p>Parameters:</p> Name Type Description Default <code>n_nuc_classes</code> <code>int</code> <p>Number of nuclei type classes.</p> required <code>n_tissue_classes</code> <code>int</code> <p>Number of tissue type classes.</p> required <code>enc_name</code> <code>str</code> <p>Name of the pytorch-image-models encoder.</p> <code>'samvit_base_patch16'</code> <code>enc_pretrain</code> <code>bool</code> <p>Whether to use pretrained weights in the encoder.</p> <code>True</code> <code>enc_freeze</code> <code>bool</code> <p>Freeze encoder weights for training.</p> <code>False</code> <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>src/histolytics/models/cellvit_panoptic.py</code> <pre><code>def __init__(\n    self,\n    n_nuc_classes: int,\n    n_tissue_classes: int,\n    enc_name: str = \"samvit_base_patch16\",\n    enc_pretrain: bool = True,\n    enc_freeze: bool = False,\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n) -&gt; None:\n    \"\"\"CellVitPanoptic model for panoptic segmentation of nuclei and tissues.\n\n    Note:\n        [CellVit article](https://arxiv.org/abs/2306.15350)\n\n    Parameters:\n        n_nuc_classes (int):\n            Number of nuclei type classes.\n        n_tissue_classes (int):\n            Number of tissue type classes.\n        enc_name (str):\n            Name of the pytorch-image-models encoder.\n        enc_pretrain (bool):\n            Whether to use pretrained weights in the encoder.\n        enc_freeze (bool):\n            Freeze encoder weights for training.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (dict):\n            Additional keyword arguments for the model.\n    \"\"\"\n    super().__init__()\n    self.model = cellvit_panoptic(\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=enc_pretrain,\n        enc_freeze=enc_freeze,\n        **model_kwargs,\n    )\n\n    self.device = device\n    self.model.to(device)\n</code></pre>"},{"location":"api/models/cellvit_panoptic/#histolytics.models.cellvit_panoptic.CellVitPanoptic.set_inference_mode","title":"set_inference_mode","text":"<pre><code>set_inference_mode(mixed_precision: bool = True) -&gt; None\n</code></pre> <p>Set model to inference mode.</p> Source code in <code>src/histolytics/models/cellvit_panoptic.py</code> <pre><code>def set_inference_mode(self, mixed_precision: bool = True) -&gt; None:\n    \"\"\"Set model to inference mode.\"\"\"\n    self.model.eval()\n    self.predictor = Predictor(\n        model=self.model,\n        mixed_precision=mixed_precision,\n    )\n    self.post_processor = PostProcessor(postproc_method=\"hovernet\")\n    self.inference_mode = True\n</code></pre>"},{"location":"api/models/cellvit_panoptic/#histolytics.models.cellvit_panoptic.CellVitPanoptic.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(weights: Union[str, Path], device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {})\n</code></pre> <p>Load the model from pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the pretrained model.</p> required <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments for the model.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    weights: Union[str, Path],\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n):\n    \"\"\"Load the model from pretrained weights.\n\n    Parameters:\n        model_name (str):\n            Name of the pretrained model.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (Dict[str, Any]):\n            Additional arguments for the model.\n\n    Examples:\n        &gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n    \"\"\"\n    weights_path = Path(weights)\n    if not weights_path.is_file():\n        if weights_path.as_posix() in PRETRAINED_MODELS[cls.model_name].keys():\n            weights_path = Path(\n                hf_hub_download(\n                    repo_id=PRETRAINED_MODELS[cls.model_name][weights][\"repo_id\"],\n                    filename=PRETRAINED_MODELS[cls.model_name][weights][\"filename\"],\n                )\n            )\n\n        else:\n            raise ValueError(\n                \"Please provide a valid path. or a pre-trained model downloaded from the\"\n                f\" histolytics-hub. One of {list(PRETRAINED_MODELS[cls.model_name].keys())}.\"\n            )\n\n    enc_name, n_nuc_classes, n_tissue_classes, state_dict = cls._get_state_dict(\n        weights_path, device=device\n    )\n\n    model_inst = cls(\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=False,\n        enc_freeze=False,\n        device=device,\n        model_kwargs=model_kwargs,\n    )\n\n    if weights_path.suffix == \".safetensors\":\n        try:\n            from safetensors.torch import load_model\n        except ImportError:\n            raise ImportError(\n                \"Please install `safetensors` package to load .safetensors files.\"\n            )\n        load_model(model_inst.model, weights_path, device.type)\n    else:\n        model_inst.model.load_state_dict(state_dict, strict=True)\n\n    try:\n        cls.nuc_classes = MODEL_CLASS_DICTS[weights][\"nuc\"]\n        cls.tissue_classes = MODEL_CLASS_DICTS[weights][\"tissue\"]\n    except KeyError:\n        # if the model is not in the class dict, set to None\n        cls.nuc_classes = None\n        cls.tissue_classes = None\n\n    return model_inst\n</code></pre>"},{"location":"api/models/cellvit_panoptic/#histolytics.models.cellvit_panoptic.CellVitPanoptic.predict","title":"predict","text":"<pre><code>predict(x: Union[Tensor, ndarray, Image], *, use_sliding_win: bool = False, window_size: Tuple[int, int] = None, stride: int = None) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]\n</code></pre> <p>Predict the input image or image batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, ndarray, Image]</code> <p>Input image (H, W, C) or input image batch (B, C, H, W).</p> required <code>use_sliding_win</code> <code>bool</code> <p>Whether to use sliding window for prediction.</p> <code>False</code> <code>window_size</code> <code>Tuple[int, int]</code> <p>The height and width of the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride for the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]: Dictionary of soft outputs:</p> <pre><code>- \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n- \"tissue\": SoftSemanticOutput (type_map).\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; # with sliding window if image is large\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n&gt;&gt;&gt; # without sliding window if image is small enough\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def predict(\n    self,\n    x: Union[torch.Tensor, np.ndarray, Image],\n    *,\n    use_sliding_win: bool = False,\n    window_size: Tuple[int, int] = None,\n    stride: int = None,\n) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n    \"\"\"Predict the input image or image batch.\n\n    Parameters:\n        x (Union[torch.Tensor, np.ndarray, Image]):\n            Input image (H, W, C) or input image batch (B, C, H, W).\n        use_sliding_win (bool):\n            Whether to use sliding window for prediction.\n        window_size (Tuple[int, int]):\n            The height and width of the sliding window. If `use_sliding_win` is False\n            this argument is ignored.\n        stride (int):\n            The stride for the sliding window. If `use_sliding_win` is False this\n            argument is ignored.\n\n    Returns:\n        Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n            Dictionary of soft outputs:\n\n                - \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n                - \"tissue\": SoftSemanticOutput (type_map).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; # with sliding window if image is large\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n        &gt;&gt;&gt; # without sliding window if image is small enough\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\"Run `.set_inference_mode()` before running `predict`\")\n\n    if not use_sliding_win:\n        x = self.predictor.predict(x=x, apply_boundary_weight=False)\n    else:\n        if window_size is None:\n            raise ValueError(\n                \"`window_size` must be provided when using sliding window.\"\n            )\n        if stride is None:\n            raise ValueError(\"`stride` must be provided when using sliding window.\")\n\n        x = self.predictor.predict_sliding_win(\n            x=x, window_size=window_size, stride=stride, apply_boundary_weight=True\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/cellvit_panoptic/#histolytics.models.cellvit_panoptic.CellVitPanoptic.post_process","title":"post_process","text":"<pre><code>post_process(x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]], *, use_async_postproc: bool = True, start_method: str = 'threading', n_jobs: int = 4, save_paths_nuc: List[Union[Path, str]] = None, save_paths_cyto: List[Union[Path, str]] = None, save_paths_tissue: List[Union[Path, str]] = None, coords: List[Tuple[int, int, int, int]] = None, class_dict_nuc: Dict[int, str] = None, class_dict_cyto: Dict[int, str] = None, class_dict_tissue: Dict[int, str] = None, nuc_smooth_func: Callable = gaussian_smooth, cyto_smooth_func: Callable = gaussian_smooth, tissue_smooth_func: Callable = None) -&gt; Dict[str, List[np.ndarray]]\n</code></pre> <p>Post-process the output of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>The output of the .predict() method.</p> required <code>use_async_postproc</code> <code>bool</code> <p>Whether to use async post-processing. Can give some run-time benefits.</p> <code>True</code> <code>start_method</code> <code>str</code> <p>The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.</p> <code>'threading'</code> <code>n_jobs</code> <code>int</code> <p>The number of workers for the post-processing.</p> <code>4</code> <code>save_paths_nuc</code> <code>List[Union[Path, str]]</code> <p>The paths to save the panlei masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_cyto</code> <code>List[Union[Path, str]]</code> <p>The paths to save the cytoplasm masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_tissue</code> <code>List[Union[Path, str]]</code> <p>The paths to save the tissue masks. If None, the masks are not saved.</p> <code>None</code> <code>coords</code> <code>List[Tuple[int, int, int, int]]</code> <p>The XYWH coordinates of the image patch. If not None, the coordinates are saved in the filenames of outputs.</p> <code>None</code> <code>class_dict_nuc</code> <code>Dict[int, str]</code> <p>The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}</p> <code>None</code> <code>class_dict_cyto</code> <code>Dict[int, str]</code> <p>The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}</p> <code>None</code> <code>class_dict_tissue</code> <code>Dict[int, str]</code> <p>The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}</p> <code>None</code> <code>nuc_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the nuclei instance maps before post-processing. If None, no smoothing is applied. This is only used when nuclei segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_nuc is None.</p> <code>gaussian_smooth</code> <code>cyto_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the cytoplasm instance maps before post-processing. If None, no smoothing is applied. This is only used when cytoplasm segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_cyto is None.</p> <code>gaussian_smooth</code> <code>tissue_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the tissue type maps before post-processing. If None, no smoothing is applied. This is only used when tissue segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_tissue is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[ndarray]]</code> <p>Dict[str, List[np.ndarray]]: Dictionary of post-processed outputs:</p> <ul> <li>\"nuclei\": List of output nuclei masks (H, W).</li> <li>\"cyto\": List of output cytoplasm masks (H, W).</li> <li>\"tissue\": List of output tissue masks (H, W).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n&gt;&gt;&gt; x = my_model.post_process(\n...     x,\n...     use_async_postproc=True,\n...     start_method=\"threading\",\n...     n_jobs=4,\n... )\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def post_process(\n    self,\n    x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]],\n    *,\n    use_async_postproc: bool = True,\n    start_method: str = \"threading\",\n    n_jobs: int = 4,\n    save_paths_nuc: List[Union[Path, str]] = None,\n    save_paths_cyto: List[Union[Path, str]] = None,\n    save_paths_tissue: List[Union[Path, str]] = None,\n    coords: List[Tuple[int, int, int, int]] = None,\n    class_dict_nuc: Dict[int, str] = None,\n    class_dict_cyto: Dict[int, str] = None,\n    class_dict_tissue: Dict[int, str] = None,\n    nuc_smooth_func: Callable = gaussian_smooth,\n    cyto_smooth_func: Callable = gaussian_smooth,\n    tissue_smooth_func: Callable = None,\n) -&gt; Dict[str, List[np.ndarray]]:\n    \"\"\"Post-process the output of the model.\n\n    Parameters:\n        x (Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]):\n            The output of the .predict() method.\n        use_async_postproc (bool):\n            Whether to use async post-processing. Can give some run-time benefits.\n        start_method (str):\n            The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.\n        n_jobs (int):\n            The number of workers for the post-processing.\n        save_paths_nuc (List[Union[Path, str]]):\n            The paths to save the panlei masks. If None, the masks are not saved.\n        save_paths_cyto (List[Union[Path, str]]):\n            The paths to save the cytoplasm masks. If None, the masks are not saved.\n        save_paths_tissue (List[Union[Path, str]]):\n            The paths to save the tissue masks. If None, the masks are not saved.\n        coords (List[Tuple[int, int, int, int]]):\n            The XYWH coordinates of the image patch. If not None, the coordinates are\n            saved in the filenames of outputs.\n        class_dict_nuc (Dict[int, str]):\n            The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}\n        class_dict_cyto (Dict[int, str]):\n            The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}\n        class_dict_tissue (Dict[int, str]):\n            The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}\n        nuc_smooth_func (Callable):\n            The smoothing function to apply to the nuclei instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            nuclei segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_nuc is None.\n        cyto_smooth_func (Callable):\n            The smoothing function to apply to the cytoplasm instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            cytoplasm segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_cyto is None.\n        tissue_smooth_func (Callable):\n            The smoothing function to apply to the tissue type maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            tissue segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_tissue is None.\n\n    Returns:\n        Dict[str, List[np.ndarray]]:\n            Dictionary of post-processed outputs:\n\n            - \"nuclei\": List of output nuclei masks (H, W).\n            - \"cyto\": List of output cytoplasm masks (H, W).\n            - \"tissue\": List of output tissue masks (H, W).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n        &gt;&gt;&gt; x = my_model.post_process(\n        ...     x,\n        ...     use_async_postproc=True,\n        ...     start_method=\"threading\",\n        ...     n_jobs=4,\n        ... )\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\n            \"Run `.set_inference_mode()` before running `post_process`\"\n        )\n\n    # if batch size is 1, run serially\n    if x[\"tissue\"].type_map.shape[0] == 1:\n        return self.post_processor.postproc_serial(\n            x,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n        )\n\n    if use_async_postproc:\n        x = self.post_processor.postproc_parallel_async(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n    else:\n        x = self.post_processor.postproc_parallel(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/cppnet_panoptic/","title":"CPPNetPanoptic","text":"<p>               Bases: <code>BaseModelPanoptic</code></p> Source code in <code>src/histolytics/models/cppnet_panoptic.py</code> <pre><code>class CPPNetPanoptic(BaseModelPanoptic):\n    model_name = \"cppnet_panoptic\"\n\n    def __init__(\n        self,\n        n_nuc_classes: int,\n        n_tissue_classes: int,\n        n_rays: int = 32,\n        enc_name: str = \"efficientnet_b5\",\n        enc_pretrain: bool = True,\n        enc_freeze: bool = False,\n        device: torch.device = torch.device(\"cuda\"),\n        model_kwargs: Dict[str, Any] = {},\n    ) -&gt; None:\n        \"\"\"CPPNetPanoptic model for panoptic segmentation of nuclei and tissues.\n\n        Note:\n            [CPP-Net article](https://arxiv.org/abs/2102.06867)\n\n        Parameters:\n            n_nuc_classes (int):\n                Number of nuclei type classes.\n            n_tissue_classes (int):\n                Number of tissue type classes.\n            n_rays (int):\n                Number of rays for the Stardist model.\n            enc_name (str):\n                Name of the pytorch-image-models encoder.\n            enc_pretrain (bool):\n                Whether to use pretrained weights in the encoder.\n            enc_freeze (bool):\n                Freeze encoder weights for training.\n            device (torch.device):\n                Device to run the model on.\n            model_kwargs (dict):\n                Additional keyword arguments for the model.\n        \"\"\"\n        super().__init__()\n        self.model = cppnet_panoptic(\n            n_rays=n_rays,\n            n_nuc_classes=n_nuc_classes,\n            n_tissue_classes=n_tissue_classes,\n            enc_name=enc_name,\n            enc_pretrain=enc_pretrain,\n            enc_freeze=enc_freeze,\n            **model_kwargs,\n        )\n\n        self.device = device\n        self.model.to(device)\n\n    def set_inference_mode(\n        self,\n        mixed_precision: bool = True,\n        postproc_kwargs: Dict[str, Any] = {\"trim_bboxes\": True},\n    ) -&gt; None:\n        \"\"\"Set model to inference mode.\"\"\"\n        self.model.eval()\n        self.predictor = Predictor(\n            model=self.model,\n            mixed_precision=mixed_precision,\n        )\n        self.post_processor = PostProcessor(\n            postproc_method=\"stardist\",\n            postproc_kwargs=postproc_kwargs,\n        )\n        self.inference_mode = True\n</code></pre>"},{"location":"api/models/cppnet_panoptic/#histolytics.models.cppnet_panoptic.CPPNetPanoptic.__init__","title":"__init__","text":"<pre><code>__init__(n_nuc_classes: int, n_tissue_classes: int, n_rays: int = 32, enc_name: str = 'efficientnet_b5', enc_pretrain: bool = True, enc_freeze: bool = False, device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {}) -&gt; None\n</code></pre> <p>CPPNetPanoptic model for panoptic segmentation of nuclei and tissues.</p> Note <p>CPP-Net article</p> <p>Parameters:</p> Name Type Description Default <code>n_nuc_classes</code> <code>int</code> <p>Number of nuclei type classes.</p> required <code>n_tissue_classes</code> <code>int</code> <p>Number of tissue type classes.</p> required <code>n_rays</code> <code>int</code> <p>Number of rays for the Stardist model.</p> <code>32</code> <code>enc_name</code> <code>str</code> <p>Name of the pytorch-image-models encoder.</p> <code>'efficientnet_b5'</code> <code>enc_pretrain</code> <code>bool</code> <p>Whether to use pretrained weights in the encoder.</p> <code>True</code> <code>enc_freeze</code> <code>bool</code> <p>Freeze encoder weights for training.</p> <code>False</code> <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>src/histolytics/models/cppnet_panoptic.py</code> <pre><code>def __init__(\n    self,\n    n_nuc_classes: int,\n    n_tissue_classes: int,\n    n_rays: int = 32,\n    enc_name: str = \"efficientnet_b5\",\n    enc_pretrain: bool = True,\n    enc_freeze: bool = False,\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n) -&gt; None:\n    \"\"\"CPPNetPanoptic model for panoptic segmentation of nuclei and tissues.\n\n    Note:\n        [CPP-Net article](https://arxiv.org/abs/2102.06867)\n\n    Parameters:\n        n_nuc_classes (int):\n            Number of nuclei type classes.\n        n_tissue_classes (int):\n            Number of tissue type classes.\n        n_rays (int):\n            Number of rays for the Stardist model.\n        enc_name (str):\n            Name of the pytorch-image-models encoder.\n        enc_pretrain (bool):\n            Whether to use pretrained weights in the encoder.\n        enc_freeze (bool):\n            Freeze encoder weights for training.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (dict):\n            Additional keyword arguments for the model.\n    \"\"\"\n    super().__init__()\n    self.model = cppnet_panoptic(\n        n_rays=n_rays,\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=enc_pretrain,\n        enc_freeze=enc_freeze,\n        **model_kwargs,\n    )\n\n    self.device = device\n    self.model.to(device)\n</code></pre>"},{"location":"api/models/cppnet_panoptic/#histolytics.models.cppnet_panoptic.CPPNetPanoptic.set_inference_mode","title":"set_inference_mode","text":"<pre><code>set_inference_mode(mixed_precision: bool = True, postproc_kwargs: Dict[str, Any] = {'trim_bboxes': True}) -&gt; None\n</code></pre> <p>Set model to inference mode.</p> Source code in <code>src/histolytics/models/cppnet_panoptic.py</code> <pre><code>def set_inference_mode(\n    self,\n    mixed_precision: bool = True,\n    postproc_kwargs: Dict[str, Any] = {\"trim_bboxes\": True},\n) -&gt; None:\n    \"\"\"Set model to inference mode.\"\"\"\n    self.model.eval()\n    self.predictor = Predictor(\n        model=self.model,\n        mixed_precision=mixed_precision,\n    )\n    self.post_processor = PostProcessor(\n        postproc_method=\"stardist\",\n        postproc_kwargs=postproc_kwargs,\n    )\n    self.inference_mode = True\n</code></pre>"},{"location":"api/models/cppnet_panoptic/#histolytics.models.cppnet_panoptic.CPPNetPanoptic.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(weights: Union[str, Path], device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {})\n</code></pre> <p>Load the model from pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the pretrained model.</p> required <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments for the model.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    weights: Union[str, Path],\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n):\n    \"\"\"Load the model from pretrained weights.\n\n    Parameters:\n        model_name (str):\n            Name of the pretrained model.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (Dict[str, Any]):\n            Additional arguments for the model.\n\n    Examples:\n        &gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n    \"\"\"\n    weights_path = Path(weights)\n    if not weights_path.is_file():\n        if weights_path.as_posix() in PRETRAINED_MODELS[cls.model_name].keys():\n            weights_path = Path(\n                hf_hub_download(\n                    repo_id=PRETRAINED_MODELS[cls.model_name][weights][\"repo_id\"],\n                    filename=PRETRAINED_MODELS[cls.model_name][weights][\"filename\"],\n                )\n            )\n\n        else:\n            raise ValueError(\n                \"Please provide a valid path. or a pre-trained model downloaded from the\"\n                f\" histolytics-hub. One of {list(PRETRAINED_MODELS[cls.model_name].keys())}.\"\n            )\n\n    enc_name, n_nuc_classes, n_tissue_classes, state_dict = cls._get_state_dict(\n        weights_path, device=device\n    )\n\n    model_inst = cls(\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=False,\n        enc_freeze=False,\n        device=device,\n        model_kwargs=model_kwargs,\n    )\n\n    if weights_path.suffix == \".safetensors\":\n        try:\n            from safetensors.torch import load_model\n        except ImportError:\n            raise ImportError(\n                \"Please install `safetensors` package to load .safetensors files.\"\n            )\n        load_model(model_inst.model, weights_path, device.type)\n    else:\n        model_inst.model.load_state_dict(state_dict, strict=True)\n\n    try:\n        cls.nuc_classes = MODEL_CLASS_DICTS[weights][\"nuc\"]\n        cls.tissue_classes = MODEL_CLASS_DICTS[weights][\"tissue\"]\n    except KeyError:\n        # if the model is not in the class dict, set to None\n        cls.nuc_classes = None\n        cls.tissue_classes = None\n\n    return model_inst\n</code></pre>"},{"location":"api/models/cppnet_panoptic/#histolytics.models.cppnet_panoptic.CPPNetPanoptic.predict","title":"predict","text":"<pre><code>predict(x: Union[Tensor, ndarray, Image], *, use_sliding_win: bool = False, window_size: Tuple[int, int] = None, stride: int = None) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]\n</code></pre> <p>Predict the input image or image batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, ndarray, Image]</code> <p>Input image (H, W, C) or input image batch (B, C, H, W).</p> required <code>use_sliding_win</code> <code>bool</code> <p>Whether to use sliding window for prediction.</p> <code>False</code> <code>window_size</code> <code>Tuple[int, int]</code> <p>The height and width of the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride for the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]: Dictionary of soft outputs:</p> <pre><code>- \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n- \"tissue\": SoftSemanticOutput (type_map).\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; # with sliding window if image is large\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n&gt;&gt;&gt; # without sliding window if image is small enough\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def predict(\n    self,\n    x: Union[torch.Tensor, np.ndarray, Image],\n    *,\n    use_sliding_win: bool = False,\n    window_size: Tuple[int, int] = None,\n    stride: int = None,\n) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n    \"\"\"Predict the input image or image batch.\n\n    Parameters:\n        x (Union[torch.Tensor, np.ndarray, Image]):\n            Input image (H, W, C) or input image batch (B, C, H, W).\n        use_sliding_win (bool):\n            Whether to use sliding window for prediction.\n        window_size (Tuple[int, int]):\n            The height and width of the sliding window. If `use_sliding_win` is False\n            this argument is ignored.\n        stride (int):\n            The stride for the sliding window. If `use_sliding_win` is False this\n            argument is ignored.\n\n    Returns:\n        Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n            Dictionary of soft outputs:\n\n                - \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n                - \"tissue\": SoftSemanticOutput (type_map).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; # with sliding window if image is large\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n        &gt;&gt;&gt; # without sliding window if image is small enough\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\"Run `.set_inference_mode()` before running `predict`\")\n\n    if not use_sliding_win:\n        x = self.predictor.predict(x=x, apply_boundary_weight=False)\n    else:\n        if window_size is None:\n            raise ValueError(\n                \"`window_size` must be provided when using sliding window.\"\n            )\n        if stride is None:\n            raise ValueError(\"`stride` must be provided when using sliding window.\")\n\n        x = self.predictor.predict_sliding_win(\n            x=x, window_size=window_size, stride=stride, apply_boundary_weight=True\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/cppnet_panoptic/#histolytics.models.cppnet_panoptic.CPPNetPanoptic.post_process","title":"post_process","text":"<pre><code>post_process(x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]], *, use_async_postproc: bool = True, start_method: str = 'threading', n_jobs: int = 4, save_paths_nuc: List[Union[Path, str]] = None, save_paths_cyto: List[Union[Path, str]] = None, save_paths_tissue: List[Union[Path, str]] = None, coords: List[Tuple[int, int, int, int]] = None, class_dict_nuc: Dict[int, str] = None, class_dict_cyto: Dict[int, str] = None, class_dict_tissue: Dict[int, str] = None, nuc_smooth_func: Callable = gaussian_smooth, cyto_smooth_func: Callable = gaussian_smooth, tissue_smooth_func: Callable = None) -&gt; Dict[str, List[np.ndarray]]\n</code></pre> <p>Post-process the output of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>The output of the .predict() method.</p> required <code>use_async_postproc</code> <code>bool</code> <p>Whether to use async post-processing. Can give some run-time benefits.</p> <code>True</code> <code>start_method</code> <code>str</code> <p>The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.</p> <code>'threading'</code> <code>n_jobs</code> <code>int</code> <p>The number of workers for the post-processing.</p> <code>4</code> <code>save_paths_nuc</code> <code>List[Union[Path, str]]</code> <p>The paths to save the panlei masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_cyto</code> <code>List[Union[Path, str]]</code> <p>The paths to save the cytoplasm masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_tissue</code> <code>List[Union[Path, str]]</code> <p>The paths to save the tissue masks. If None, the masks are not saved.</p> <code>None</code> <code>coords</code> <code>List[Tuple[int, int, int, int]]</code> <p>The XYWH coordinates of the image patch. If not None, the coordinates are saved in the filenames of outputs.</p> <code>None</code> <code>class_dict_nuc</code> <code>Dict[int, str]</code> <p>The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}</p> <code>None</code> <code>class_dict_cyto</code> <code>Dict[int, str]</code> <p>The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}</p> <code>None</code> <code>class_dict_tissue</code> <code>Dict[int, str]</code> <p>The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}</p> <code>None</code> <code>nuc_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the nuclei instance maps before post-processing. If None, no smoothing is applied. This is only used when nuclei segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_nuc is None.</p> <code>gaussian_smooth</code> <code>cyto_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the cytoplasm instance maps before post-processing. If None, no smoothing is applied. This is only used when cytoplasm segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_cyto is None.</p> <code>gaussian_smooth</code> <code>tissue_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the tissue type maps before post-processing. If None, no smoothing is applied. This is only used when tissue segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_tissue is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[ndarray]]</code> <p>Dict[str, List[np.ndarray]]: Dictionary of post-processed outputs:</p> <ul> <li>\"nuclei\": List of output nuclei masks (H, W).</li> <li>\"cyto\": List of output cytoplasm masks (H, W).</li> <li>\"tissue\": List of output tissue masks (H, W).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n&gt;&gt;&gt; x = my_model.post_process(\n...     x,\n...     use_async_postproc=True,\n...     start_method=\"threading\",\n...     n_jobs=4,\n... )\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def post_process(\n    self,\n    x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]],\n    *,\n    use_async_postproc: bool = True,\n    start_method: str = \"threading\",\n    n_jobs: int = 4,\n    save_paths_nuc: List[Union[Path, str]] = None,\n    save_paths_cyto: List[Union[Path, str]] = None,\n    save_paths_tissue: List[Union[Path, str]] = None,\n    coords: List[Tuple[int, int, int, int]] = None,\n    class_dict_nuc: Dict[int, str] = None,\n    class_dict_cyto: Dict[int, str] = None,\n    class_dict_tissue: Dict[int, str] = None,\n    nuc_smooth_func: Callable = gaussian_smooth,\n    cyto_smooth_func: Callable = gaussian_smooth,\n    tissue_smooth_func: Callable = None,\n) -&gt; Dict[str, List[np.ndarray]]:\n    \"\"\"Post-process the output of the model.\n\n    Parameters:\n        x (Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]):\n            The output of the .predict() method.\n        use_async_postproc (bool):\n            Whether to use async post-processing. Can give some run-time benefits.\n        start_method (str):\n            The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.\n        n_jobs (int):\n            The number of workers for the post-processing.\n        save_paths_nuc (List[Union[Path, str]]):\n            The paths to save the panlei masks. If None, the masks are not saved.\n        save_paths_cyto (List[Union[Path, str]]):\n            The paths to save the cytoplasm masks. If None, the masks are not saved.\n        save_paths_tissue (List[Union[Path, str]]):\n            The paths to save the tissue masks. If None, the masks are not saved.\n        coords (List[Tuple[int, int, int, int]]):\n            The XYWH coordinates of the image patch. If not None, the coordinates are\n            saved in the filenames of outputs.\n        class_dict_nuc (Dict[int, str]):\n            The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}\n        class_dict_cyto (Dict[int, str]):\n            The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}\n        class_dict_tissue (Dict[int, str]):\n            The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}\n        nuc_smooth_func (Callable):\n            The smoothing function to apply to the nuclei instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            nuclei segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_nuc is None.\n        cyto_smooth_func (Callable):\n            The smoothing function to apply to the cytoplasm instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            cytoplasm segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_cyto is None.\n        tissue_smooth_func (Callable):\n            The smoothing function to apply to the tissue type maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            tissue segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_tissue is None.\n\n    Returns:\n        Dict[str, List[np.ndarray]]:\n            Dictionary of post-processed outputs:\n\n            - \"nuclei\": List of output nuclei masks (H, W).\n            - \"cyto\": List of output cytoplasm masks (H, W).\n            - \"tissue\": List of output tissue masks (H, W).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n        &gt;&gt;&gt; x = my_model.post_process(\n        ...     x,\n        ...     use_async_postproc=True,\n        ...     start_method=\"threading\",\n        ...     n_jobs=4,\n        ... )\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\n            \"Run `.set_inference_mode()` before running `post_process`\"\n        )\n\n    # if batch size is 1, run serially\n    if x[\"tissue\"].type_map.shape[0] == 1:\n        return self.post_processor.postproc_serial(\n            x,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n        )\n\n    if use_async_postproc:\n        x = self.post_processor.postproc_parallel_async(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n    else:\n        x = self.post_processor.postproc_parallel(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/hovernet_panoptic/","title":"HoverNetPanoptic","text":"<p>               Bases: <code>BaseModelPanoptic</code></p> Source code in <code>src/histolytics/models/hovernet_panoptic.py</code> <pre><code>class HoverNetPanoptic(BaseModelPanoptic):\n    model_name = \"hovernet_panoptic\"\n\n    def __init__(\n        self,\n        n_nuc_classes: int,\n        n_tissue_classes: int,\n        enc_name: str = \"efficientnet_b5\",\n        enc_pretrain: bool = True,\n        enc_freeze: bool = False,\n        device: torch.device = torch.device(\"cuda\"),\n        model_kwargs: Dict[str, Any] = {},\n    ) -&gt; None:\n        \"\"\"HovernetPanoptic model for panoptic segmentation of nuclei and tissues.\n\n        Note:\n            [HoVer-Net article](https://www.sciencedirect.com/science/article/pii/S1361841519301045?via%3Dihub)\n\n        Parameters:\n            n_nuc_classes (int):\n                Number of nuclei type classes.\n            n_tissue_classes (int):\n                Number of tissue type classes.\n            enc_name (str):\n                Name of the pytorch-image-models encoder.\n            enc_pretrain (bool):\n                Whether to use pretrained weights in the encoder.\n            enc_freeze (bool):\n                Freeze encoder weights for training.\n            device (torch.device):\n                Device to run the model on.\n            model_kwargs (Dict[str, Any], default={}):\n                Additional keyword arguments for the model.\n        \"\"\"\n        super().__init__()\n        self.model = hovernet_panoptic(\n            n_nuc_classes,\n            n_tissue_classes,\n            enc_name=enc_name,\n            enc_pretrain=enc_pretrain,\n            enc_freeze=enc_freeze,\n            **model_kwargs,\n        )\n\n        self.device = device\n        self.model.to(device)\n\n    def set_inference_mode(self, mixed_precision: bool = True) -&gt; None:\n        \"\"\"Set model to inference mode.\"\"\"\n        self.model.eval()\n        self.predictor = Predictor(\n            model=self.model,\n            mixed_precision=mixed_precision,\n        )\n        self.post_processor = PostProcessor(postproc_method=\"hovernet\")\n        self.inference_mode = True\n</code></pre>"},{"location":"api/models/hovernet_panoptic/#histolytics.models.hovernet_panoptic.HoverNetPanoptic.__init__","title":"__init__","text":"<pre><code>__init__(n_nuc_classes: int, n_tissue_classes: int, enc_name: str = 'efficientnet_b5', enc_pretrain: bool = True, enc_freeze: bool = False, device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {}) -&gt; None\n</code></pre> <p>HovernetPanoptic model for panoptic segmentation of nuclei and tissues.</p> Note <p>HoVer-Net article</p> <p>Parameters:</p> Name Type Description Default <code>n_nuc_classes</code> <code>int</code> <p>Number of nuclei type classes.</p> required <code>n_tissue_classes</code> <code>int</code> <p>Number of tissue type classes.</p> required <code>enc_name</code> <code>str</code> <p>Name of the pytorch-image-models encoder.</p> <code>'efficientnet_b5'</code> <code>enc_pretrain</code> <code>bool</code> <p>Whether to use pretrained weights in the encoder.</p> <code>True</code> <code>enc_freeze</code> <code>bool</code> <p>Freeze encoder weights for training.</p> <code>False</code> <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>Dict[str, Any], default={}</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>src/histolytics/models/hovernet_panoptic.py</code> <pre><code>def __init__(\n    self,\n    n_nuc_classes: int,\n    n_tissue_classes: int,\n    enc_name: str = \"efficientnet_b5\",\n    enc_pretrain: bool = True,\n    enc_freeze: bool = False,\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n) -&gt; None:\n    \"\"\"HovernetPanoptic model for panoptic segmentation of nuclei and tissues.\n\n    Note:\n        [HoVer-Net article](https://www.sciencedirect.com/science/article/pii/S1361841519301045?via%3Dihub)\n\n    Parameters:\n        n_nuc_classes (int):\n            Number of nuclei type classes.\n        n_tissue_classes (int):\n            Number of tissue type classes.\n        enc_name (str):\n            Name of the pytorch-image-models encoder.\n        enc_pretrain (bool):\n            Whether to use pretrained weights in the encoder.\n        enc_freeze (bool):\n            Freeze encoder weights for training.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (Dict[str, Any], default={}):\n            Additional keyword arguments for the model.\n    \"\"\"\n    super().__init__()\n    self.model = hovernet_panoptic(\n        n_nuc_classes,\n        n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=enc_pretrain,\n        enc_freeze=enc_freeze,\n        **model_kwargs,\n    )\n\n    self.device = device\n    self.model.to(device)\n</code></pre>"},{"location":"api/models/hovernet_panoptic/#histolytics.models.hovernet_panoptic.HoverNetPanoptic.set_inference_mode","title":"set_inference_mode","text":"<pre><code>set_inference_mode(mixed_precision: bool = True) -&gt; None\n</code></pre> <p>Set model to inference mode.</p> Source code in <code>src/histolytics/models/hovernet_panoptic.py</code> <pre><code>def set_inference_mode(self, mixed_precision: bool = True) -&gt; None:\n    \"\"\"Set model to inference mode.\"\"\"\n    self.model.eval()\n    self.predictor = Predictor(\n        model=self.model,\n        mixed_precision=mixed_precision,\n    )\n    self.post_processor = PostProcessor(postproc_method=\"hovernet\")\n    self.inference_mode = True\n</code></pre>"},{"location":"api/models/hovernet_panoptic/#histolytics.models.hovernet_panoptic.HoverNetPanoptic.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(weights: Union[str, Path], device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {})\n</code></pre> <p>Load the model from pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the pretrained model.</p> required <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments for the model.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    weights: Union[str, Path],\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n):\n    \"\"\"Load the model from pretrained weights.\n\n    Parameters:\n        model_name (str):\n            Name of the pretrained model.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (Dict[str, Any]):\n            Additional arguments for the model.\n\n    Examples:\n        &gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n    \"\"\"\n    weights_path = Path(weights)\n    if not weights_path.is_file():\n        if weights_path.as_posix() in PRETRAINED_MODELS[cls.model_name].keys():\n            weights_path = Path(\n                hf_hub_download(\n                    repo_id=PRETRAINED_MODELS[cls.model_name][weights][\"repo_id\"],\n                    filename=PRETRAINED_MODELS[cls.model_name][weights][\"filename\"],\n                )\n            )\n\n        else:\n            raise ValueError(\n                \"Please provide a valid path. or a pre-trained model downloaded from the\"\n                f\" histolytics-hub. One of {list(PRETRAINED_MODELS[cls.model_name].keys())}.\"\n            )\n\n    enc_name, n_nuc_classes, n_tissue_classes, state_dict = cls._get_state_dict(\n        weights_path, device=device\n    )\n\n    model_inst = cls(\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=False,\n        enc_freeze=False,\n        device=device,\n        model_kwargs=model_kwargs,\n    )\n\n    if weights_path.suffix == \".safetensors\":\n        try:\n            from safetensors.torch import load_model\n        except ImportError:\n            raise ImportError(\n                \"Please install `safetensors` package to load .safetensors files.\"\n            )\n        load_model(model_inst.model, weights_path, device.type)\n    else:\n        model_inst.model.load_state_dict(state_dict, strict=True)\n\n    try:\n        cls.nuc_classes = MODEL_CLASS_DICTS[weights][\"nuc\"]\n        cls.tissue_classes = MODEL_CLASS_DICTS[weights][\"tissue\"]\n    except KeyError:\n        # if the model is not in the class dict, set to None\n        cls.nuc_classes = None\n        cls.tissue_classes = None\n\n    return model_inst\n</code></pre>"},{"location":"api/models/hovernet_panoptic/#histolytics.models.hovernet_panoptic.HoverNetPanoptic.predict","title":"predict","text":"<pre><code>predict(x: Union[Tensor, ndarray, Image], *, use_sliding_win: bool = False, window_size: Tuple[int, int] = None, stride: int = None) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]\n</code></pre> <p>Predict the input image or image batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, ndarray, Image]</code> <p>Input image (H, W, C) or input image batch (B, C, H, W).</p> required <code>use_sliding_win</code> <code>bool</code> <p>Whether to use sliding window for prediction.</p> <code>False</code> <code>window_size</code> <code>Tuple[int, int]</code> <p>The height and width of the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride for the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]: Dictionary of soft outputs:</p> <pre><code>- \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n- \"tissue\": SoftSemanticOutput (type_map).\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; # with sliding window if image is large\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n&gt;&gt;&gt; # without sliding window if image is small enough\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def predict(\n    self,\n    x: Union[torch.Tensor, np.ndarray, Image],\n    *,\n    use_sliding_win: bool = False,\n    window_size: Tuple[int, int] = None,\n    stride: int = None,\n) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n    \"\"\"Predict the input image or image batch.\n\n    Parameters:\n        x (Union[torch.Tensor, np.ndarray, Image]):\n            Input image (H, W, C) or input image batch (B, C, H, W).\n        use_sliding_win (bool):\n            Whether to use sliding window for prediction.\n        window_size (Tuple[int, int]):\n            The height and width of the sliding window. If `use_sliding_win` is False\n            this argument is ignored.\n        stride (int):\n            The stride for the sliding window. If `use_sliding_win` is False this\n            argument is ignored.\n\n    Returns:\n        Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n            Dictionary of soft outputs:\n\n                - \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n                - \"tissue\": SoftSemanticOutput (type_map).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; # with sliding window if image is large\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n        &gt;&gt;&gt; # without sliding window if image is small enough\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\"Run `.set_inference_mode()` before running `predict`\")\n\n    if not use_sliding_win:\n        x = self.predictor.predict(x=x, apply_boundary_weight=False)\n    else:\n        if window_size is None:\n            raise ValueError(\n                \"`window_size` must be provided when using sliding window.\"\n            )\n        if stride is None:\n            raise ValueError(\"`stride` must be provided when using sliding window.\")\n\n        x = self.predictor.predict_sliding_win(\n            x=x, window_size=window_size, stride=stride, apply_boundary_weight=True\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/hovernet_panoptic/#histolytics.models.hovernet_panoptic.HoverNetPanoptic.post_process","title":"post_process","text":"<pre><code>post_process(x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]], *, use_async_postproc: bool = True, start_method: str = 'threading', n_jobs: int = 4, save_paths_nuc: List[Union[Path, str]] = None, save_paths_cyto: List[Union[Path, str]] = None, save_paths_tissue: List[Union[Path, str]] = None, coords: List[Tuple[int, int, int, int]] = None, class_dict_nuc: Dict[int, str] = None, class_dict_cyto: Dict[int, str] = None, class_dict_tissue: Dict[int, str] = None, nuc_smooth_func: Callable = gaussian_smooth, cyto_smooth_func: Callable = gaussian_smooth, tissue_smooth_func: Callable = None) -&gt; Dict[str, List[np.ndarray]]\n</code></pre> <p>Post-process the output of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>The output of the .predict() method.</p> required <code>use_async_postproc</code> <code>bool</code> <p>Whether to use async post-processing. Can give some run-time benefits.</p> <code>True</code> <code>start_method</code> <code>str</code> <p>The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.</p> <code>'threading'</code> <code>n_jobs</code> <code>int</code> <p>The number of workers for the post-processing.</p> <code>4</code> <code>save_paths_nuc</code> <code>List[Union[Path, str]]</code> <p>The paths to save the panlei masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_cyto</code> <code>List[Union[Path, str]]</code> <p>The paths to save the cytoplasm masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_tissue</code> <code>List[Union[Path, str]]</code> <p>The paths to save the tissue masks. If None, the masks are not saved.</p> <code>None</code> <code>coords</code> <code>List[Tuple[int, int, int, int]]</code> <p>The XYWH coordinates of the image patch. If not None, the coordinates are saved in the filenames of outputs.</p> <code>None</code> <code>class_dict_nuc</code> <code>Dict[int, str]</code> <p>The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}</p> <code>None</code> <code>class_dict_cyto</code> <code>Dict[int, str]</code> <p>The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}</p> <code>None</code> <code>class_dict_tissue</code> <code>Dict[int, str]</code> <p>The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}</p> <code>None</code> <code>nuc_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the nuclei instance maps before post-processing. If None, no smoothing is applied. This is only used when nuclei segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_nuc is None.</p> <code>gaussian_smooth</code> <code>cyto_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the cytoplasm instance maps before post-processing. If None, no smoothing is applied. This is only used when cytoplasm segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_cyto is None.</p> <code>gaussian_smooth</code> <code>tissue_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the tissue type maps before post-processing. If None, no smoothing is applied. This is only used when tissue segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_tissue is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[ndarray]]</code> <p>Dict[str, List[np.ndarray]]: Dictionary of post-processed outputs:</p> <ul> <li>\"nuclei\": List of output nuclei masks (H, W).</li> <li>\"cyto\": List of output cytoplasm masks (H, W).</li> <li>\"tissue\": List of output tissue masks (H, W).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n&gt;&gt;&gt; x = my_model.post_process(\n...     x,\n...     use_async_postproc=True,\n...     start_method=\"threading\",\n...     n_jobs=4,\n... )\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def post_process(\n    self,\n    x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]],\n    *,\n    use_async_postproc: bool = True,\n    start_method: str = \"threading\",\n    n_jobs: int = 4,\n    save_paths_nuc: List[Union[Path, str]] = None,\n    save_paths_cyto: List[Union[Path, str]] = None,\n    save_paths_tissue: List[Union[Path, str]] = None,\n    coords: List[Tuple[int, int, int, int]] = None,\n    class_dict_nuc: Dict[int, str] = None,\n    class_dict_cyto: Dict[int, str] = None,\n    class_dict_tissue: Dict[int, str] = None,\n    nuc_smooth_func: Callable = gaussian_smooth,\n    cyto_smooth_func: Callable = gaussian_smooth,\n    tissue_smooth_func: Callable = None,\n) -&gt; Dict[str, List[np.ndarray]]:\n    \"\"\"Post-process the output of the model.\n\n    Parameters:\n        x (Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]):\n            The output of the .predict() method.\n        use_async_postproc (bool):\n            Whether to use async post-processing. Can give some run-time benefits.\n        start_method (str):\n            The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.\n        n_jobs (int):\n            The number of workers for the post-processing.\n        save_paths_nuc (List[Union[Path, str]]):\n            The paths to save the panlei masks. If None, the masks are not saved.\n        save_paths_cyto (List[Union[Path, str]]):\n            The paths to save the cytoplasm masks. If None, the masks are not saved.\n        save_paths_tissue (List[Union[Path, str]]):\n            The paths to save the tissue masks. If None, the masks are not saved.\n        coords (List[Tuple[int, int, int, int]]):\n            The XYWH coordinates of the image patch. If not None, the coordinates are\n            saved in the filenames of outputs.\n        class_dict_nuc (Dict[int, str]):\n            The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}\n        class_dict_cyto (Dict[int, str]):\n            The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}\n        class_dict_tissue (Dict[int, str]):\n            The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}\n        nuc_smooth_func (Callable):\n            The smoothing function to apply to the nuclei instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            nuclei segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_nuc is None.\n        cyto_smooth_func (Callable):\n            The smoothing function to apply to the cytoplasm instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            cytoplasm segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_cyto is None.\n        tissue_smooth_func (Callable):\n            The smoothing function to apply to the tissue type maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            tissue segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_tissue is None.\n\n    Returns:\n        Dict[str, List[np.ndarray]]:\n            Dictionary of post-processed outputs:\n\n            - \"nuclei\": List of output nuclei masks (H, W).\n            - \"cyto\": List of output cytoplasm masks (H, W).\n            - \"tissue\": List of output tissue masks (H, W).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n        &gt;&gt;&gt; x = my_model.post_process(\n        ...     x,\n        ...     use_async_postproc=True,\n        ...     start_method=\"threading\",\n        ...     n_jobs=4,\n        ... )\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\n            \"Run `.set_inference_mode()` before running `post_process`\"\n        )\n\n    # if batch size is 1, run serially\n    if x[\"tissue\"].type_map.shape[0] == 1:\n        return self.post_processor.postproc_serial(\n            x,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n        )\n\n    if use_async_postproc:\n        x = self.post_processor.postproc_parallel_async(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n    else:\n        x = self.post_processor.postproc_parallel(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/stardist_panoptic/","title":"StarDistPanoptic","text":"<p>               Bases: <code>BaseModelPanoptic</code></p> Source code in <code>src/histolytics/models/stardist_panoptic.py</code> <pre><code>class StarDistPanoptic(BaseModelPanoptic):\n    model_name = \"stardist_panoptic\"\n\n    def __init__(\n        self,\n        n_nuc_classes: int,\n        n_tissue_classes: int,\n        n_rays: int = 32,\n        enc_name: str = \"efficientnet_b5\",\n        enc_pretrain: bool = True,\n        enc_freeze: bool = False,\n        device: torch.device = torch.device(\"cuda\"),\n        model_kwargs: Dict[str, Any] = {},\n    ) -&gt; None:\n        \"\"\"StardistPanoptic model for panoptic segmentation of nuclei and tissues.\n\n        Note:\n            [Stardist article](https://arxiv.org/abs/1806.03535)\n\n        Parameters:\n            n_nuc_classes (int):\n                Number of nuclei type classes.\n            n_tissue_classes (int):\n                Number of tissue type classes.\n            n_rays (int):\n                Number of rays for the Stardist model.\n            enc_name (str):\n                Name of the pytorch-image-models encoder.\n            enc_pretrain (bool):\n                Whether to use pretrained weights in the encoder.\n            enc_freeze (bool):\n                Freeze encoder weights for training.\n            device (torch.device):\n                Device to run the model on.\n            model_kwargs (dict):\n                Additional keyword arguments for the model.\n        \"\"\"\n        super().__init__()\n        self.model = stardist_panoptic(\n            n_rays=n_rays,\n            n_nuc_classes=n_nuc_classes,\n            n_tissue_classes=n_tissue_classes,\n            enc_name=enc_name,\n            enc_pretrain=enc_pretrain,\n            enc_freeze=enc_freeze,\n            **model_kwargs,\n        )\n\n        self.device = device\n        self.model.to(device)\n\n    def set_inference_mode(\n        self,\n        mixed_precision: bool = True,\n        postproc_kwargs: Dict[str, Any] = {\"trim_bboxes\": True},\n    ) -&gt; None:\n        \"\"\"Set model to inference mode.\"\"\"\n        self.model.eval()\n        self.predictor = Predictor(\n            model=self.model,\n            mixed_precision=mixed_precision,\n        )\n        self.post_processor = PostProcessor(\n            postproc_method=\"stardist\",\n            postproc_kwargs=postproc_kwargs,\n        )\n        self.inference_mode = True\n</code></pre>"},{"location":"api/models/stardist_panoptic/#histolytics.models.stardist_panoptic.StarDistPanoptic.__init__","title":"__init__","text":"<pre><code>__init__(n_nuc_classes: int, n_tissue_classes: int, n_rays: int = 32, enc_name: str = 'efficientnet_b5', enc_pretrain: bool = True, enc_freeze: bool = False, device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {}) -&gt; None\n</code></pre> <p>StardistPanoptic model for panoptic segmentation of nuclei and tissues.</p> Note <p>Stardist article</p> <p>Parameters:</p> Name Type Description Default <code>n_nuc_classes</code> <code>int</code> <p>Number of nuclei type classes.</p> required <code>n_tissue_classes</code> <code>int</code> <p>Number of tissue type classes.</p> required <code>n_rays</code> <code>int</code> <p>Number of rays for the Stardist model.</p> <code>32</code> <code>enc_name</code> <code>str</code> <p>Name of the pytorch-image-models encoder.</p> <code>'efficientnet_b5'</code> <code>enc_pretrain</code> <code>bool</code> <p>Whether to use pretrained weights in the encoder.</p> <code>True</code> <code>enc_freeze</code> <code>bool</code> <p>Freeze encoder weights for training.</p> <code>False</code> <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>src/histolytics/models/stardist_panoptic.py</code> <pre><code>def __init__(\n    self,\n    n_nuc_classes: int,\n    n_tissue_classes: int,\n    n_rays: int = 32,\n    enc_name: str = \"efficientnet_b5\",\n    enc_pretrain: bool = True,\n    enc_freeze: bool = False,\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n) -&gt; None:\n    \"\"\"StardistPanoptic model for panoptic segmentation of nuclei and tissues.\n\n    Note:\n        [Stardist article](https://arxiv.org/abs/1806.03535)\n\n    Parameters:\n        n_nuc_classes (int):\n            Number of nuclei type classes.\n        n_tissue_classes (int):\n            Number of tissue type classes.\n        n_rays (int):\n            Number of rays for the Stardist model.\n        enc_name (str):\n            Name of the pytorch-image-models encoder.\n        enc_pretrain (bool):\n            Whether to use pretrained weights in the encoder.\n        enc_freeze (bool):\n            Freeze encoder weights for training.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (dict):\n            Additional keyword arguments for the model.\n    \"\"\"\n    super().__init__()\n    self.model = stardist_panoptic(\n        n_rays=n_rays,\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=enc_pretrain,\n        enc_freeze=enc_freeze,\n        **model_kwargs,\n    )\n\n    self.device = device\n    self.model.to(device)\n</code></pre>"},{"location":"api/models/stardist_panoptic/#histolytics.models.stardist_panoptic.StarDistPanoptic.set_inference_mode","title":"set_inference_mode","text":"<pre><code>set_inference_mode(mixed_precision: bool = True, postproc_kwargs: Dict[str, Any] = {'trim_bboxes': True}) -&gt; None\n</code></pre> <p>Set model to inference mode.</p> Source code in <code>src/histolytics/models/stardist_panoptic.py</code> <pre><code>def set_inference_mode(\n    self,\n    mixed_precision: bool = True,\n    postproc_kwargs: Dict[str, Any] = {\"trim_bboxes\": True},\n) -&gt; None:\n    \"\"\"Set model to inference mode.\"\"\"\n    self.model.eval()\n    self.predictor = Predictor(\n        model=self.model,\n        mixed_precision=mixed_precision,\n    )\n    self.post_processor = PostProcessor(\n        postproc_method=\"stardist\",\n        postproc_kwargs=postproc_kwargs,\n    )\n    self.inference_mode = True\n</code></pre>"},{"location":"api/models/stardist_panoptic/#histolytics.models.stardist_panoptic.StarDistPanoptic.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(weights: Union[str, Path], device: device = torch.device('cuda'), model_kwargs: Dict[str, Any] = {})\n</code></pre> <p>Load the model from pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the pretrained model.</p> required <code>device</code> <code>device</code> <p>Device to run the model on.</p> <code>device('cuda')</code> <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments for the model.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    weights: Union[str, Path],\n    device: torch.device = torch.device(\"cuda\"),\n    model_kwargs: Dict[str, Any] = {},\n):\n    \"\"\"Load the model from pretrained weights.\n\n    Parameters:\n        model_name (str):\n            Name of the pretrained model.\n        device (torch.device):\n            Device to run the model on.\n        model_kwargs (Dict[str, Any]):\n            Additional arguments for the model.\n\n    Examples:\n        &gt;&gt;&gt; model = Model.from_pretrained(&lt;str or Path to weights&gt;, device=torch.device(\"cuda\"))\n    \"\"\"\n    weights_path = Path(weights)\n    if not weights_path.is_file():\n        if weights_path.as_posix() in PRETRAINED_MODELS[cls.model_name].keys():\n            weights_path = Path(\n                hf_hub_download(\n                    repo_id=PRETRAINED_MODELS[cls.model_name][weights][\"repo_id\"],\n                    filename=PRETRAINED_MODELS[cls.model_name][weights][\"filename\"],\n                )\n            )\n\n        else:\n            raise ValueError(\n                \"Please provide a valid path. or a pre-trained model downloaded from the\"\n                f\" histolytics-hub. One of {list(PRETRAINED_MODELS[cls.model_name].keys())}.\"\n            )\n\n    enc_name, n_nuc_classes, n_tissue_classes, state_dict = cls._get_state_dict(\n        weights_path, device=device\n    )\n\n    model_inst = cls(\n        n_nuc_classes=n_nuc_classes,\n        n_tissue_classes=n_tissue_classes,\n        enc_name=enc_name,\n        enc_pretrain=False,\n        enc_freeze=False,\n        device=device,\n        model_kwargs=model_kwargs,\n    )\n\n    if weights_path.suffix == \".safetensors\":\n        try:\n            from safetensors.torch import load_model\n        except ImportError:\n            raise ImportError(\n                \"Please install `safetensors` package to load .safetensors files.\"\n            )\n        load_model(model_inst.model, weights_path, device.type)\n    else:\n        model_inst.model.load_state_dict(state_dict, strict=True)\n\n    try:\n        cls.nuc_classes = MODEL_CLASS_DICTS[weights][\"nuc\"]\n        cls.tissue_classes = MODEL_CLASS_DICTS[weights][\"tissue\"]\n    except KeyError:\n        # if the model is not in the class dict, set to None\n        cls.nuc_classes = None\n        cls.tissue_classes = None\n\n    return model_inst\n</code></pre>"},{"location":"api/models/stardist_panoptic/#histolytics.models.stardist_panoptic.StarDistPanoptic.predict","title":"predict","text":"<pre><code>predict(x: Union[Tensor, ndarray, Image], *, use_sliding_win: bool = False, window_size: Tuple[int, int] = None, stride: int = None) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]\n</code></pre> <p>Predict the input image or image batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, ndarray, Image]</code> <p>Input image (H, W, C) or input image batch (B, C, H, W).</p> required <code>use_sliding_win</code> <code>bool</code> <p>Whether to use sliding window for prediction.</p> <code>False</code> <code>window_size</code> <code>Tuple[int, int]</code> <p>The height and width of the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride for the sliding window. If <code>use_sliding_win</code> is False this argument is ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]: Dictionary of soft outputs:</p> <pre><code>- \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n- \"tissue\": SoftSemanticOutput (type_map).\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; # with sliding window if image is large\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n&gt;&gt;&gt; # without sliding window if image is small enough\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def predict(\n    self,\n    x: Union[torch.Tensor, np.ndarray, Image],\n    *,\n    use_sliding_win: bool = False,\n    window_size: Tuple[int, int] = None,\n    stride: int = None,\n) -&gt; Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n    \"\"\"Predict the input image or image batch.\n\n    Parameters:\n        x (Union[torch.Tensor, np.ndarray, Image]):\n            Input image (H, W, C) or input image batch (B, C, H, W).\n        use_sliding_win (bool):\n            Whether to use sliding window for prediction.\n        window_size (Tuple[int, int]):\n            The height and width of the sliding window. If `use_sliding_win` is False\n            this argument is ignored.\n        stride (int):\n            The stride for the sliding window. If `use_sliding_win` is False this\n            argument is ignored.\n\n    Returns:\n        Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]:\n            Dictionary of soft outputs:\n\n                - \"nuclei\": SoftInstanceOutput (type_map, aux_map).\n                - \"tissue\": SoftSemanticOutput (type_map).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; # with sliding window if image is large\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=True, window_size=(256, 256), stride=128)\n        &gt;&gt;&gt; # without sliding window if image is small enough\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\"Run `.set_inference_mode()` before running `predict`\")\n\n    if not use_sliding_win:\n        x = self.predictor.predict(x=x, apply_boundary_weight=False)\n    else:\n        if window_size is None:\n            raise ValueError(\n                \"`window_size` must be provided when using sliding window.\"\n            )\n        if stride is None:\n            raise ValueError(\"`stride` must be provided when using sliding window.\")\n\n        x = self.predictor.predict_sliding_win(\n            x=x, window_size=window_size, stride=stride, apply_boundary_weight=True\n        )\n\n    return x\n</code></pre>"},{"location":"api/models/stardist_panoptic/#histolytics.models.stardist_panoptic.StarDistPanoptic.post_process","title":"post_process","text":"<pre><code>post_process(x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]], *, use_async_postproc: bool = True, start_method: str = 'threading', n_jobs: int = 4, save_paths_nuc: List[Union[Path, str]] = None, save_paths_cyto: List[Union[Path, str]] = None, save_paths_tissue: List[Union[Path, str]] = None, coords: List[Tuple[int, int, int, int]] = None, class_dict_nuc: Dict[int, str] = None, class_dict_cyto: Dict[int, str] = None, class_dict_tissue: Dict[int, str] = None, nuc_smooth_func: Callable = gaussian_smooth, cyto_smooth_func: Callable = gaussian_smooth, tissue_smooth_func: Callable = None) -&gt; Dict[str, List[np.ndarray]]\n</code></pre> <p>Post-process the output of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]</code> <p>The output of the .predict() method.</p> required <code>use_async_postproc</code> <code>bool</code> <p>Whether to use async post-processing. Can give some run-time benefits.</p> <code>True</code> <code>start_method</code> <code>str</code> <p>The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.</p> <code>'threading'</code> <code>n_jobs</code> <code>int</code> <p>The number of workers for the post-processing.</p> <code>4</code> <code>save_paths_nuc</code> <code>List[Union[Path, str]]</code> <p>The paths to save the panlei masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_cyto</code> <code>List[Union[Path, str]]</code> <p>The paths to save the cytoplasm masks. If None, the masks are not saved.</p> <code>None</code> <code>save_paths_tissue</code> <code>List[Union[Path, str]]</code> <p>The paths to save the tissue masks. If None, the masks are not saved.</p> <code>None</code> <code>coords</code> <code>List[Tuple[int, int, int, int]]</code> <p>The XYWH coordinates of the image patch. If not None, the coordinates are saved in the filenames of outputs.</p> <code>None</code> <code>class_dict_nuc</code> <code>Dict[int, str]</code> <p>The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}</p> <code>None</code> <code>class_dict_cyto</code> <code>Dict[int, str]</code> <p>The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}</p> <code>None</code> <code>class_dict_tissue</code> <code>Dict[int, str]</code> <p>The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}</p> <code>None</code> <code>nuc_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the nuclei instance maps before post-processing. If None, no smoothing is applied. This is only used when nuclei segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_nuc is None.</p> <code>gaussian_smooth</code> <code>cyto_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the cytoplasm instance maps before post-processing. If None, no smoothing is applied. This is only used when cytoplasm segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_cyto is None.</p> <code>gaussian_smooth</code> <code>tissue_smooth_func</code> <code>Callable</code> <p>The smoothing function to apply to the tissue type maps before post-processing. If None, no smoothing is applied. This is only used when tissue segmentation masks are saved into vectorized format (e.g. parquet). Ignored save_paths_tissue is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[ndarray]]</code> <p>Dict[str, List[np.ndarray]]: Dictionary of post-processed outputs:</p> <ul> <li>\"nuclei\": List of output nuclei masks (H, W).</li> <li>\"cyto\": List of output cytoplasm masks (H, W).</li> <li>\"tissue\": List of output tissue masks (H, W).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; my_model.set_inference_mode()\n&gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n&gt;&gt;&gt; x = my_model.post_process(\n...     x,\n...     use_async_postproc=True,\n...     start_method=\"threading\",\n...     n_jobs=4,\n... )\n</code></pre> Source code in <code>src/histolytics/models/_base_model.py</code> <pre><code>def post_process(\n    self,\n    x: Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]],\n    *,\n    use_async_postproc: bool = True,\n    start_method: str = \"threading\",\n    n_jobs: int = 4,\n    save_paths_nuc: List[Union[Path, str]] = None,\n    save_paths_cyto: List[Union[Path, str]] = None,\n    save_paths_tissue: List[Union[Path, str]] = None,\n    coords: List[Tuple[int, int, int, int]] = None,\n    class_dict_nuc: Dict[int, str] = None,\n    class_dict_cyto: Dict[int, str] = None,\n    class_dict_tissue: Dict[int, str] = None,\n    nuc_smooth_func: Callable = gaussian_smooth,\n    cyto_smooth_func: Callable = gaussian_smooth,\n    tissue_smooth_func: Callable = None,\n) -&gt; Dict[str, List[np.ndarray]]:\n    \"\"\"Post-process the output of the model.\n\n    Parameters:\n        x (Dict[str, Union[SoftSemanticOutput, SoftInstanceOutput]]):\n            The output of the .predict() method.\n        use_async_postproc (bool):\n            Whether to use async post-processing. Can give some run-time benefits.\n        start_method (str):\n            The start method. One of: \"threading\", \"fork\", \"spawn\". See mpire docs.\n        n_jobs (int):\n            The number of workers for the post-processing.\n        save_paths_nuc (List[Union[Path, str]]):\n            The paths to save the panlei masks. If None, the masks are not saved.\n        save_paths_cyto (List[Union[Path, str]]):\n            The paths to save the cytoplasm masks. If None, the masks are not saved.\n        save_paths_tissue (List[Union[Path, str]]):\n            The paths to save the tissue masks. If None, the masks are not saved.\n        coords (List[Tuple[int, int, int, int]]):\n            The XYWH coordinates of the image patch. If not None, the coordinates are\n            saved in the filenames of outputs.\n        class_dict_nuc (Dict[int, str]):\n            The dictionary of panlei classes. E.g. {0: \"bg\", 1: \"neoplastic\"}\n        class_dict_cyto (Dict[int, str]):\n            The dictionary of cytoplasm classes. E.g. {0: \"bg\", 1: \"macrophage_cyto\"}\n        class_dict_tissue (Dict[int, str]):\n            The dictionary of tissue classes. E.g. {0: \"bg\", 1: \"stroma\", 2: \"tumor\"}\n        nuc_smooth_func (Callable):\n            The smoothing function to apply to the nuclei instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            nuclei segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_nuc is None.\n        cyto_smooth_func (Callable):\n            The smoothing function to apply to the cytoplasm instance maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            cytoplasm segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_cyto is None.\n        tissue_smooth_func (Callable):\n            The smoothing function to apply to the tissue type maps before\n            post-processing. If None, no smoothing is applied. This is only used when\n            tissue segmentation masks are saved into vectorized format (e.g. parquet).\n            Ignored save_paths_tissue is None.\n\n    Returns:\n        Dict[str, List[np.ndarray]]:\n            Dictionary of post-processed outputs:\n\n            - \"nuclei\": List of output nuclei masks (H, W).\n            - \"cyto\": List of output cytoplasm masks (H, W).\n            - \"tissue\": List of output tissue masks (H, W).\n\n    Examples:\n        &gt;&gt;&gt; my_model.set_inference_mode()\n        &gt;&gt;&gt; x = my_model.predict(x=image, use_sliding_win=False)\n        &gt;&gt;&gt; x = my_model.post_process(\n        ...     x,\n        ...     use_async_postproc=True,\n        ...     start_method=\"threading\",\n        ...     n_jobs=4,\n        ... )\n    \"\"\"\n    if not self.inference_mode:\n        raise ValueError(\n            \"Run `.set_inference_mode()` before running `post_process`\"\n        )\n\n    # if batch size is 1, run serially\n    if x[\"tissue\"].type_map.shape[0] == 1:\n        return self.post_processor.postproc_serial(\n            x,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n        )\n\n    if use_async_postproc:\n        x = self.post_processor.postproc_parallel_async(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n    else:\n        x = self.post_processor.postproc_parallel(\n            x,\n            start_method=start_method,\n            n_jobs=n_jobs,\n            save_paths_nuc=save_paths_nuc,\n            save_paths_cyto=save_paths_cyto,\n            save_paths_tissue=save_paths_tissue,\n            coords=coords,\n            class_dict_nuc=class_dict_nuc,\n            class_dict_cyto=class_dict_cyto,\n            class_dict_tissue=class_dict_tissue,\n            nuc_smooth_func=gaussian_smooth,\n            cyto_smooth_func=gaussian_smooth,\n            tissue_smooth_func=None,\n        )\n\n    return x\n</code></pre>"},{"location":"api/nuc_feats/chromatin_feats/","title":"chromatin_feats","text":"<p>Extracts chromatin features from the HE image and instance segmentation mask.</p> Note <p>This function extracts features related to the chromatin distribution within nuclei. These features include the total pixel area occupied by chromatin clumps within each nucleus, proportion of chromatin area to total nucleus area, number of distinct connected components (clumps) of chromatin within each nucleus, and proportion of chromatin that intersects with nucleus boundary.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Image to extract chromatin clumps from. Shape (H, W, 3).</p> required <code>label</code> <code>ndarray</code> <p>Label map of the cells/nuclei. Shape (H, W).</p> required <code>metrics</code> <code>Tuple[str, ...]</code> <p>Metrics to compute. Options are:</p> <pre><code>- \"chrom_area\"\n- \"chrom_nuc_prop\"\n- \"n_chrom_clumps\"\n- \"chrom_boundary_prop\"\n- \"manders_coloc_coeff\"\n</code></pre> <code>('chrom_area', 'chrom_nuc_prop')</code> <code>mean</code> <code>float</code> <p>Mean intensity of the image.</p> <code>0</code> <code>std</code> <code>float</code> <p>Standard deviation of the image.</p> <code>1</code> <code>erode</code> <code>bool</code> <p>Whether to apply erosion to the chromatin clumps.</p> <code>False</code> <code>mask</code> <code>ndarray</code> <p>Optional binary mask to apply to the image to restrict the region of interest. Shape (H, W).</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for computation. \"cpu\" or \"cuda\".</p> <code>'cpu'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shape of <code>img</code> and <code>label</code> do not match.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the extracted chromatin features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n&gt;&gt;&gt; from histolytics.nuc_feats.chromatin import chromatin_feats\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load example data\n&gt;&gt;&gt; he_image = hgsc_cancer_he()\n&gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Filter for a specific cell type if needed\n&gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert nuclei GeoDataFrame to instance segmentation mask\n&gt;&gt;&gt; inst_mask = gdf2inst(neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0])\n&gt;&gt;&gt; # Extract chromatin clumps\n&gt;&gt;&gt;\n&gt;&gt;&gt; metrics = (\"chrom_area\", \"chrom_nuc_prop\", \"n_chrom_clumps\", \"chrom_boundary_prop\")\n&gt;&gt;&gt; chrom_feats = chromatin_feats(he_image, inst_mask, metrics=metrics)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(chrom_feats.head(3))\n        chrom_area  chrom_nuc_prop  n_chrom_clumps  chrom_boundary_prop\n292         155        0.210027             3.0             0.163043\n316         421        0.990588             1.0             0.625641\n340         334        0.582897             2.0             0.527027\n</code></pre> Source code in <code>src/histolytics/nuc_feats/chromatin.py</code> <pre><code>def chromatin_feats(\n    img: np.ndarray,\n    label: np.ndarray,\n    metrics: Tuple[str, ...] = (\"chrom_area\", \"chrom_nuc_prop\"),\n    mean: float = 0,\n    std: float = 1,\n    erode: bool = False,\n    mask: np.ndarray = None,\n    device: str = \"cpu\",\n) -&gt; pd.DataFrame:\n    \"\"\"Extracts chromatin features from the HE image and instance segmentation mask.\n\n    Note:\n        This function extracts features related to the chromatin distribution within\n        nuclei. These features include the total pixel area occupied by chromatin clumps within\n        each nucleus, proportion of chromatin area to total nucleus area, number of distinct\n        connected components (clumps) of chromatin within each nucleus, and proportion\n        of chromatin that intersects with nucleus boundary.\n\n    Parameters:\n        img (np.ndarray):\n            Image to extract chromatin clumps from. Shape (H, W, 3).\n        label (np.ndarray):\n            Label map of the cells/nuclei. Shape (H, W).\n        metrics (Tuple[str, ...]):\n            Metrics to compute. Options are:\n\n                - \"chrom_area\"\n                - \"chrom_nuc_prop\"\n                - \"n_chrom_clumps\"\n                - \"chrom_boundary_prop\"\n                - \"manders_coloc_coeff\"\n        mean (float):\n            Mean intensity of the image.\n        std (float):\n            Standard deviation of the image.\n        erode (bool):\n            Whether to apply erosion to the chromatin clumps.\n        mask (np.ndarray):\n            Optional binary mask to apply to the image to restrict the region of interest.\n            Shape (H, W).\n        device (str):\n            Device to use for computation. \"cpu\" or \"cuda\".\n\n    Raises:\n        ValueError: If the shape of `img` and `label` do not match.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the extracted chromatin features.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n        &gt;&gt;&gt; from histolytics.nuc_feats.chromatin import chromatin_feats\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load example data\n        &gt;&gt;&gt; he_image = hgsc_cancer_he()\n        &gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Filter for a specific cell type if needed\n        &gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert nuclei GeoDataFrame to instance segmentation mask\n        &gt;&gt;&gt; inst_mask = gdf2inst(neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0])\n        &gt;&gt;&gt; # Extract chromatin clumps\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; metrics = (\"chrom_area\", \"chrom_nuc_prop\", \"n_chrom_clumps\", \"chrom_boundary_prop\")\n        &gt;&gt;&gt; chrom_feats = chromatin_feats(he_image, inst_mask, metrics=metrics)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; print(chrom_feats.head(3))\n                chrom_area  chrom_nuc_prop  n_chrom_clumps  chrom_boundary_prop\n        292         155        0.210027             3.0             0.163043\n        316         421        0.990588             1.0             0.625641\n        340         334        0.582897             2.0             0.527027\n    \"\"\"\n    allowed = (\n        \"chrom_area\",\n        \"chrom_nuc_prop\",\n        \"n_chrom_clumps\",\n        \"chrom_boundary_prop\",\n        \"manders_coloc_coeff\",\n    )\n    if not all(m in allowed for m in metrics):\n        raise ValueError(f\"Invalid metrics: {metrics}. Allowed: {allowed}\")\n\n    if device == \"cuda\" and not _has_cp:\n        raise RuntimeError(\n            \"CuPy and cucim are required for GPU acceleration (device='cuda'). \"\n            \"Please install them with:\\n\"\n            \"  pip install cupy cucim\\n\"\n            \"or set device='cpu'.\"\n        )\n\n    chrom_clumps = extract_chromatin_clumps(img, label, mask, mean, std, device=device)\n\n    if chrom_clumps is None or np.max(chrom_clumps) == 0:\n        return pd.DataFrame([], columns=metrics)\n\n    # Apply erosion if requested (cpu side due to some cupy bug)\n    if erode:\n        chrom_clumps = erosion(chrom_clumps, disk(2))\n\n    if _has_cp and device == \"cuda\":\n        return _chrom_feats_cp(img, chrom_clumps, label, metrics)\n    else:\n        return _chrom_feats_np(img, chrom_clumps, label, metrics)\n</code></pre>"},{"location":"api/nuc_feats/extract_chromatin_clumps/","title":"extract_chromatin_clumps","text":"<p>Extract chromatin clumps from a given image and label-map.</p> Note <p>Applies a normalization to the image before extracting chromatin clumps.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Input H&amp;E image from which to extract chromatin clumps. Shape (H, W, 3).</p> required <code>label</code> <code>ndarray</code> <p>Nuclei label map indicating the nuclei of interest. Shape (H, W).</p> required <code>mask</code> <code>ndarray</code> <p>Binary mask to restrict the region of interest. Shape (H, W). For example, it can be used to mask out tissues that are not of interest.</p> <code>None</code> <code>mean</code> <code>float</code> <p>Mean intensity for normalization.</p> <code>0.0</code> <code>std</code> <code>float</code> <p>Standard deviation for normalization.</p> <code>1.0</code> <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda', CuPy and cucim will be used for GPU acceleration.</p> <code>'cpu'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shape of <code>img</code> and <code>label</code> do not match.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Binary mask of the extracted chromatin clumps. Shape (H, W).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n&gt;&gt;&gt; from histolytics.nuc_feats.chromatin import extract_chromatin_clumps\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load example data\n&gt;&gt;&gt; he_image = hgsc_cancer_he()\n&gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Filter for a specific cell type if needed\n&gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert nuclei GeoDataFrame to instance segmentation mask\n&gt;&gt;&gt; inst_mask = gdf2inst(neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0])\n&gt;&gt;&gt; # Extract chromatin clumps\n&gt;&gt;&gt; chrom_mask = extract_chromatin_clumps(he_image, inst_mask)\n&gt;&gt;&gt; fig,ax = plt.subplots(1, 2, figsize=(8, 4))\n&gt;&gt;&gt; ax[0].imshow(chrom_mask)\n&gt;&gt;&gt; ax[0].set_axis_off()\n&gt;&gt;&gt; ax[1].imshow(he_image)\n&gt;&gt;&gt; ax[1].set_axis_off()\n&gt;&gt;&gt; fig.tight_layout()\n</code></pre> <p></p> Source code in <code>src/histolytics/nuc_feats/chromatin.py</code> <pre><code>def extract_chromatin_clumps(\n    img: np.ndarray,\n    label: np.ndarray,\n    mask: np.ndarray = None,\n    mean: float = 0.0,\n    std: float = 1.0,\n    device: str = \"cpu\",\n) -&gt; np.ndarray:\n    \"\"\"Extract chromatin clumps from a given image and label-map.\n\n    Note:\n        Applies a normalization to the image before extracting chromatin clumps.\n\n    Parameters:\n        img (np.ndarray):\n            Input H&amp;E image from which to extract chromatin clumps. Shape (H, W, 3).\n        label (np.ndarray):\n            Nuclei label map indicating the nuclei of interest. Shape (H, W).\n        mask (np.ndarray):\n            Binary mask to restrict the region of interest. Shape (H, W). For example,\n            it can be used to mask out tissues that are not of interest.\n        mean (float):\n            Mean intensity for normalization.\n        std (float):\n            Standard deviation for normalization.\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda',\n            CuPy and cucim will be used for GPU acceleration.\n\n    Raises:\n        ValueError: If the shape of `img` and `label` do not match.\n\n    Returns:\n        np.ndarray: Binary mask of the extracted chromatin clumps. Shape (H, W).\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n        &gt;&gt;&gt; from histolytics.nuc_feats.chromatin import extract_chromatin_clumps\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load example data\n        &gt;&gt;&gt; he_image = hgsc_cancer_he()\n        &gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Filter for a specific cell type if needed\n        &gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert nuclei GeoDataFrame to instance segmentation mask\n        &gt;&gt;&gt; inst_mask = gdf2inst(neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0])\n        &gt;&gt;&gt; # Extract chromatin clumps\n        &gt;&gt;&gt; chrom_mask = extract_chromatin_clumps(he_image, inst_mask)\n        &gt;&gt;&gt; fig,ax = plt.subplots(1, 2, figsize=(8, 4))\n        &gt;&gt;&gt; ax[0].imshow(chrom_mask)\n        &gt;&gt;&gt; ax[0].set_axis_off()\n        &gt;&gt;&gt; ax[1].imshow(he_image)\n        &gt;&gt;&gt; ax[1].set_axis_off()\n        &gt;&gt;&gt; fig.tight_layout()\n    ![out](../../img/chrom_clump_noerode.png)\n    \"\"\"\n    if img.shape[:2] != label.shape:\n        raise ValueError(\n            f\"Shape mismatch: img has shape {img.shape}, but label has shape {label.shape}.\"\n        )\n\n    if mask is not None and mask.shape != label.shape:\n        raise ValueError(\n            f\"Shape mismatch: mask has shape {mask.shape}, but label has shape {label.shape}.\"\n        )\n\n    if device == \"cuda\" and not _has_cp:\n        raise RuntimeError(\n            \"CuPy and cucim are required for GPU acceleration (device='cuda'). \"\n            \"Please install them with:\\n\"\n            \"  pip install cupy-cuda12x cucim-cu12\\n\"\n            \"or set device='cpu'.\"\n        )\n\n    # return zeros if label empty\n    if np.max(label) == 0:\n        return np.zeros_like(label)\n\n    if _has_cp and device == \"cuda\":\n        return _extract_chrom_cp(img, label, mask, mean, std)\n    else:\n        return _extract_chrom_np(img, label, mask, mean, std)\n</code></pre>"},{"location":"api/nuc_feats/grayscale_intensity/","title":"grayscale_intensity_feats","text":"<p>Computes grayscale intensity features of labeled objects in <code>img</code>.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>H&amp;E image to compute properties from. Shape (H, W).</p> required <code>label</code> <code>ndarray</code> <p>Nuclei label map. Shape (H, W).</p> required <code>metrics</code> <code>Tuple[str, ...]</code> <p>Metrics to compute for each object. Options are:</p> <pre><code>- \"min\"\n- \"max\"\n- \"mean\"\n- \"median\"\n- \"std\"\n- \"quantiles\"\n- \"meanmediandiff\"\n- \"mad\"\n- \"iqr\"\n- \"skewness\"\n- \"kurtosis\"\n- \"histenergy\"\n- \"histentropy\"\n</code></pre> <code>('mean', 'std', 'quantiles')</code> <code>quantiles</code> <code>Tuple[float, ...]</code> <p>Quantiles to compute for each object. Ignored if <code>metrics</code> does not include \"quantiles\".</p> <code>(0.25, 0.5, 0.75)</code> <code>n_bins</code> <code>int</code> <p>Number of bins to use for histogram-based features. Ignored if <code>metrics</code> does not include \"histenergy\" or \"histentropy\".</p> <code>32</code> <code>hist_range</code> <code>Tuple[float, float]</code> <p>Range of pixel values to use for histogram-based features. Ignored if <code>metrics</code> does not include \"histenergy\" or \"histentropy\".</p> <code>None</code> <code>mask</code> <code>ndarray</code> <p>Optional binary mask to apply to the image to restrict the region of interest. Shape (H, W). For example, it can be used to mask out tissues that are not of interest.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda', CuPy and cucim will be used for GPU acceleration.</p> <code>'cpu'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shape of <code>img</code> and <code>label</code> do not match.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the computed features for each label object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n&gt;&gt;&gt; from histolytics.nuc_feats.intensity import grayscale_intensity_feats\n&gt;&gt;&gt;\n&gt;&gt;&gt; he_image = hgsc_cancer_he()\n&gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n&gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; inst_mask = gdf2inst(\n...     neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0]\n... )\n&gt;&gt;&gt; # Extract grayscale intensity features from the neoplastic nuclei\n&gt;&gt;&gt; feats = grayscale_intensity_feats(he_image, inst_mask)\n&gt;&gt;&gt; print(feats.iloc[..., 0:3].head(3))\n            mean       std  quantile_0.25\n    292  0.236541  0.068776       0.194504\n    316  0.124629  0.025052       0.105769\n    340  0.168674  0.060852       0.120324\n</code></pre> Source code in <code>src/histolytics/nuc_feats/intensity.py</code> <pre><code>def grayscale_intensity_feats(\n    img: np.ndarray,\n    label: np.ndarray,\n    metrics: Tuple[str, ...] = (\"mean\", \"std\", \"quantiles\"),\n    quantiles: Tuple[float, ...] = (0.25, 0.5, 0.75),\n    n_bins: int = 32,\n    hist_range: Tuple[float, float] = None,\n    mask: np.ndarray = None,\n    device: str = \"cpu\",\n) -&gt; pd.DataFrame:\n    \"\"\"Computes grayscale intensity features of labeled objects in `img`.\n\n    Parameters:\n        img (np.ndarray):\n            H&amp;E image to compute properties from. Shape (H, W).\n        label (np.ndarray):\n            Nuclei label map. Shape (H, W).\n        metrics (Tuple[str, ...]):\n            Metrics to compute for each object. Options are:\n\n                - \"min\"\n                - \"max\"\n                - \"mean\"\n                - \"median\"\n                - \"std\"\n                - \"quantiles\"\n                - \"meanmediandiff\"\n                - \"mad\"\n                - \"iqr\"\n                - \"skewness\"\n                - \"kurtosis\"\n                - \"histenergy\"\n                - \"histentropy\"\n        quantiles (Tuple[float, ...]):\n            Quantiles to compute for each object. Ignored if `metrics` does not include\n            \"quantiles\".\n        n_bins (int):\n            Number of bins to use for histogram-based features. Ignored if `metrics`\n            does not include \"histenergy\" or \"histentropy\".\n        hist_range (Tuple[float, float]):\n            Range of pixel values to use for histogram-based features. Ignored if `metrics`\n            does not include \"histenergy\" or \"histentropy\".\n        mask (np.ndarray):\n            Optional binary mask to apply to the image to restrict the region of interest.\n            Shape (H, W). For example, it can be used to mask out tissues that are not\n            of interest.\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda',\n            CuPy and cucim will be used for GPU acceleration.\n\n    Raises:\n        ValueError: If the shape of `img` and `label` do not match.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the computed features for each label object.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n        &gt;&gt;&gt; from histolytics.nuc_feats.intensity import grayscale_intensity_feats\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; he_image = hgsc_cancer_he()\n        &gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; inst_mask = gdf2inst(\n        ...     neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0]\n        ... )\n        &gt;&gt;&gt; # Extract grayscale intensity features from the neoplastic nuclei\n        &gt;&gt;&gt; feats = grayscale_intensity_feats(he_image, inst_mask)\n        &gt;&gt;&gt; print(feats.iloc[..., 0:3].head(3))\n                    mean       std  quantile_0.25\n            292  0.236541  0.068776       0.194504\n            316  0.124629  0.025052       0.105769\n            340  0.168674  0.060852       0.120324\n    \"\"\"\n    if label is not None and img.shape[:2] != label.shape:\n        raise ValueError(\n            f\"Shape mismatch: img.shape[:2]={img.shape[:2]}, label.shape={label.shape}\"\n        )\n\n    if device == \"cuda\" and not _has_cp:\n        raise RuntimeError(\n            \"CuPy and cucim are required for GPU acceleration (device='cuda'). \"\n            \"Please install them with:\\n\"\n            \"  pip install cupy-cuda12x cucim-cu12\\n\"\n            \"or set device='cpu'.\"\n        )\n\n    if _has_cp and device == \"cuda\":\n        img, label = _to_grayscale_cp(img, label, mask)\n        feats_df = _intensity_features_cp(\n            img,\n            label,\n            metrics=metrics,\n            quantiles=quantiles,\n            n_bins=n_bins,\n            hist_range=hist_range,\n        )\n    else:\n        img, label = _to_grayscale_np(img, label, mask)\n        feats_df = _intensity_features_np(\n            img,\n            label,\n            metrics=metrics,\n            quantiles=quantiles,\n            n_bins=n_bins,\n            hist_range=hist_range,\n        )\n\n    return feats_df\n</code></pre>"},{"location":"api/nuc_feats/rgb_intensity/","title":"rgb_intensity_feats","text":"<p>Computes rgb-intensity features of labeled objects in <code>img</code>.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Image to compute properties from. Shape (H, W, 3).</p> required <code>label</code> <code>ndarray</code> <p>Label image. Shape (H, W).</p> required <code>metrics</code> <code>Tuple[str, ...]</code> <p>Metrics to compute for each object. Options are:</p> <pre><code>- \"max\"\n- \"min\"\n- \"mean\"\n- \"median\"\n- \"std\"\n- \"quantiles\"\n- \"meanmediandiff\"\n- \"mad\"\n- \"iqr\"\n- \"skewness\"\n- \"kurtosis\"\n- \"histenergy\"\n- \"histentropy\"\n</code></pre> <code>('mean', 'std', 'quantiles')</code> <code>quantiles</code> <code>Tuple[float, ...]</code> <p>Quantiles to compute for each object. Ignored if <code>metrics</code> does not include \"quantiles\".</p> <code>(0.25, 0.5, 0.75)</code> <code>n_bins</code> <code>int</code> <p>Number of bins to use for histogram-based features. Ignored if <code>metrics</code> does not include \"histenergy\" or \"histentropy\".</p> <code>32</code> <code>hist_range</code> <code>Tuple[float, float]</code> <p>Range of pixel values to use for histogram-based features. Ignored if <code>metrics</code> does not include \"histenergy\" or \"histentropy\".</p> <code>None</code> <code>mask</code> <code>ndarray</code> <p>Optional binary mask to apply to the image to restrict the region of interest. Shape (H, W). For example, it can be used to mask out tissues that are not of interest.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda', CuPy and cucim will be used for GPU acceleration.</p> <code>'cpu'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shape of <code>img</code> and <code>label</code> do not match.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the computed features for each RGB-channel for each object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n&gt;&gt;&gt; from histolytics.nuc_feats.intensity import rgb_intensity_feats\n&gt;&gt;&gt;\n&gt;&gt;&gt; he_image = hgsc_cancer_he()\n&gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n&gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; inst_mask = gdf2inst(\n...     neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0]\n... )\n&gt;&gt;&gt; # Extract RGB intensity features from the neoplastic nuclei\n&gt;&gt;&gt; feats = rgb_intensity_feats(he_image, inst_mask)\n&gt;&gt;&gt; print(feats.iloc[..., 0:3].head(3))\n            R_mean     R_std  R_quantile_0.25\n    292    0.390361  0.071453         0.349138\n    316    0.279746  0.032215         0.254310\n    340    0.319236  0.071267         0.267241\n</code></pre> Source code in <code>src/histolytics/nuc_feats/intensity.py</code> <pre><code>def rgb_intensity_feats(\n    img: np.ndarray,\n    label: np.ndarray,\n    metrics: Tuple[str, ...] = (\"mean\", \"std\", \"quantiles\"),\n    quantiles: Tuple[float, ...] = (0.25, 0.5, 0.75),\n    n_bins: int = 32,\n    hist_range: Tuple[float, float] = None,\n    mask: np.ndarray = None,\n    device: str = \"cpu\",\n) -&gt; pd.DataFrame:\n    \"\"\"Computes rgb-intensity features of labeled objects in `img`.\n\n    Parameters:\n        img (np.ndarray):\n            Image to compute properties from. Shape (H, W, 3).\n        label (np.ndarray):\n            Label image. Shape (H, W).\n        metrics (Tuple[str, ...]):\n            Metrics to compute for each object. Options are:\n\n                - \"max\"\n                - \"min\"\n                - \"mean\"\n                - \"median\"\n                - \"std\"\n                - \"quantiles\"\n                - \"meanmediandiff\"\n                - \"mad\"\n                - \"iqr\"\n                - \"skewness\"\n                - \"kurtosis\"\n                - \"histenergy\"\n                - \"histentropy\"\n        quantiles (Tuple[float, ...]):\n            Quantiles to compute for each object. Ignored if `metrics` does not include\n            \"quantiles\".\n        n_bins (int):\n            Number of bins to use for histogram-based features. Ignored if `metrics`\n            does not include \"histenergy\" or \"histentropy\".\n        hist_range (Tuple[float, float]):\n            Range of pixel values to use for histogram-based features. Ignored if `metrics`\n            does not include \"histenergy\" or \"histentropy\".\n        mask (np.ndarray):\n            Optional binary mask to apply to the image to restrict the region of interest.\n            Shape (H, W). For example, it can be used to mask out tissues that are not\n            of interest.\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda',\n            CuPy and cucim will be used for GPU acceleration.\n\n    Raises:\n        ValueError: If the shape of `img` and `label` do not match.\n\n    Returns:\n        pd.DataFrame:\n            A DataFrame containing the computed features for each RGB-channel for each object.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n        &gt;&gt;&gt; from histolytics.nuc_feats.intensity import rgb_intensity_feats\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; he_image = hgsc_cancer_he()\n        &gt;&gt;&gt; nuclei = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; neoplastic_nuclei = nuclei[nuclei[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; inst_mask = gdf2inst(\n        ...     neoplastic_nuclei, width=he_image.shape[1], height=he_image.shape[0]\n        ... )\n        &gt;&gt;&gt; # Extract RGB intensity features from the neoplastic nuclei\n        &gt;&gt;&gt; feats = rgb_intensity_feats(he_image, inst_mask)\n        &gt;&gt;&gt; print(feats.iloc[..., 0:3].head(3))\n                    R_mean     R_std  R_quantile_0.25\n            292    0.390361  0.071453         0.349138\n            316    0.279746  0.032215         0.254310\n            340    0.319236  0.071267         0.267241\n    \"\"\"\n    if label is not None and img.shape[:2] != label.shape:\n        raise ValueError(\n            f\"Shape mismatch: img.shape[:2]={img.shape[:2]}, label.shape={label.shape}\"\n        )\n\n    if device == \"cuda\" and not _has_cp:\n        raise RuntimeError(\n            \"CuPy and cucim are required for GPU acceleration (device='cuda'). \"\n            \"Please install them with:\\n\"\n            \"  pip install cupy-cuda12x cucim-cu12\\n\"\n            \"or set device='cpu'.\"\n        )\n\n    if _has_cp and device == \"cuda\":\n        img, label = _norm_cp(img, label, mask, out_range=(0, 1))\n    else:\n        img, label = _norm_np(img, label, mask, out_range=(0, 1))\n\n    channel_codes = [\"R\", \"G\", \"B\"]\n    channel_feat_dfs = []\n    for c in range(3):\n        if _has_cp and device == \"cuda\":\n            channel_feat_df = _intensity_features_cp(\n                img[..., c],\n                label,\n                metrics=metrics,\n                quantiles=quantiles,\n                n_bins=n_bins,\n                hist_range=hist_range,\n            )\n        else:\n            channel_feat_df = _intensity_features_np(\n                img[..., c],\n                label,\n                metrics=metrics,\n                quantiles=quantiles,\n                n_bins=n_bins,\n                hist_range=hist_range,\n            )\n        # Prefix columns with channel code\n        channel_code = channel_codes[c]\n        channel_feat_df = channel_feat_df.add_prefix(f\"{channel_code}_\")\n        channel_feat_dfs.append(channel_feat_df)\n\n    # Concatenate along columns (axis=1)\n    feats_df = pd.concat(channel_feat_dfs, axis=1)\n\n    return feats_df\n</code></pre>"},{"location":"api/nuc_feats/textural_feats/","title":"textural_feats","text":"<p>Compute GLCM texture features from a grayscale image.</p> Note <p>Uses <code>skimage.feature.graycomatrix</code> and <code>skimage.feature.graycoprops</code> See scikit-image docs</p> <p>Parameters:</p> Name Type Description Default <code>im_gray</code> <code>ndarray</code> <p>Grayscale image. Shape (H, W), Dtype: uint8.</p> required <code>label</code> <code>ndarray</code> <p>Instance label map. Shape (H, W), Dtype: int.</p> required <code>metrics</code> <code>Sequence[str]</code> <p>Texture metrics to compute. Allowed values are:</p> <ul> <li>\"contrast\"</li> <li>\"dissimilarity\"</li> <li>\"homogeneity\"</li> <li>\"ASM\"</li> <li>\"energy\"</li> <li>\"correlation\"</li> <li>\"mean\"</li> <li>\"variance\"</li> <li>\"std\"</li> <li>\"entropy\"</li> </ul> <code>('contrast', 'dissimilarity')</code> <code>distances</code> <code>Sequence[int]</code> <p>Specifies the pixel distances at which the relationships are computed. A distance of 1 compares adjacent pixels, while larger distances allow for the analysis of texture at different scales, capturing relationships between pixels that are further apart.</p> <code>(1,)</code> <code>angles</code> <code>Sequence[float]</code> <p>Defines the direction of the pixel relationships for GLCM computation. Angles of 0, \u03c0/4, \u03c0/2, and 3\u03c0/4 radians correspond to horizontal, diagonal, vertical, and anti-diagonal directions, respectively. This parameter allows you to analyze textures that may be directionally dependent or anisotropic.</p> <code>(0,)</code> <code>mask</code> <code>ndarray</code> <p>Optional binary mask to apply to the image to restrict the region of interest. Shape (H, W). For example, it can be used to mask out tissues that are not of interest.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for computation. \"cpu\" or \"cuda\". If cuda, the pre-processing is done on the GPU. The CLCM computation is performed on the CPU.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the computed texture features for each nucleus.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_inst_mask\n&gt;&gt;&gt; from histolytics.nuc_feats.texture import textural_feats\n&gt;&gt;&gt; # Load data\n&gt;&gt;&gt; img = hgsc_cancer_he()\n&gt;&gt;&gt; inst_label = hgsc_cancer_inst_mask()\n&gt;&gt;&gt;\n&gt;&gt;&gt; metrics = [\"contrast\", \"dissimilarity\"]\n&gt;&gt;&gt; distances = (1,)\n&gt;&gt;&gt; angles = (0,)\n&gt;&gt;&gt; feats = textural_feats(\n...     img,\n...     inst_label,\n...     distances=distances,\n...     metrics=metrics,\n...     angles=angles,\n...     device=\"cuda\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(feats.head(3))\n    contrast_d-1_a-0.00  dissimilarity_d-1_a-0.00\n1           172.885714                  6.247619\n2           535.854839                 10.635484\n3           881.203704                 13.500000\n</code></pre> Source code in <code>src/histolytics/nuc_feats/texture.py</code> <pre><code>def textural_feats(\n    img: np.ndarray,\n    label: np.ndarray,\n    metrics: Sequence[str] = (\"contrast\", \"dissimilarity\"),\n    distances: Sequence[int] = (1,),\n    angles: Sequence[float] = (0,),\n    mask: np.ndarray = None,\n    device: str = \"cpu\",\n) -&gt; pd.DataFrame:\n    \"\"\"Compute GLCM texture features from a grayscale image.\n\n    Note:\n        Uses `skimage.feature.graycomatrix` and `skimage.feature.graycoprops`\n        See [scikit-image docs](https://scikit-image.org/docs/0.25.x/api/skimage.feature.html#skimage.feature.graycoprops)\n\n    Parameters:\n        im_gray (np.ndarray):\n            Grayscale image. Shape (H, W), Dtype: uint8.\n        label (np.ndarray):\n            Instance label map. Shape (H, W), Dtype: int.\n        metrics (Sequence[str]): Texture metrics to compute. Allowed values are:\n\n            - \"contrast\"\n            - \"dissimilarity\"\n            - \"homogeneity\"\n            - \"ASM\"\n            - \"energy\"\n            - \"correlation\"\n            - \"mean\"\n            - \"variance\"\n            - \"std\"\n            - \"entropy\"\n        distances (Sequence[int]):\n            Specifies the pixel distances at which the relationships are computed.\n            A distance of 1 compares adjacent pixels, while larger distances allow for\n            the analysis of texture at different scales, capturing relationships between\n            pixels that are further apart.\n        angles (Sequence[float]):\n            Defines the direction of the pixel relationships for GLCM computation. Angles\n            of 0, \u03c0/4, \u03c0/2, and 3\u03c0/4 radians correspond to horizontal, diagonal,\n            vertical, and anti-diagonal directions, respectively. This parameter allows\n            you to analyze textures that may be directionally dependent or anisotropic.\n        mask (np.ndarray):\n            Optional binary mask to apply to the image to restrict the region of interest.\n            Shape (H, W). For example, it can be used to mask out tissues that are not\n            of interest.\n        device (str):\n            Device to use for computation. \"cpu\" or \"cuda\". If cuda, the pre-processing\n            is done on the GPU. The CLCM computation is performed on the CPU.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the computed texture features for each nucleus.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_he, hgsc_cancer_inst_mask\n        &gt;&gt;&gt; from histolytics.nuc_feats.texture import textural_feats\n        &gt;&gt;&gt; # Load data\n        &gt;&gt;&gt; img = hgsc_cancer_he()\n        &gt;&gt;&gt; inst_label = hgsc_cancer_inst_mask()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; metrics = [\"contrast\", \"dissimilarity\"]\n        &gt;&gt;&gt; distances = (1,)\n        &gt;&gt;&gt; angles = (0,)\n        &gt;&gt;&gt; feats = textural_feats(\n        ...     img,\n        ...     inst_label,\n        ...     distances=distances,\n        ...     metrics=metrics,\n        ...     angles=angles,\n        ...     device=\"cuda\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; print(feats.head(3))\n            contrast_d-1_a-0.00  dissimilarity_d-1_a-0.00\n        1           172.885714                  6.247619\n        2           535.854839                 10.635484\n        3           881.203704                 13.500000\n    \"\"\"\n    if device == \"cuda\" and _has_cp:\n        im_gray = img_as_ubyte_cp(rgb2gray_cp(cp.array(img))).get()\n        nuc_lab = cp.unique(cp.array(label))[1:].get()\n    else:\n        im_gray = img_as_ubyte(rgb2gray(img))\n        nuc_lab = np.unique(label)[1:]\n\n    if mask is not None:\n        if mask.dtype != bool:\n            mask = mask &gt; 0\n        label = label * mask\n\n    nuc_pos = ndimage.find_objects(label)\n\n    nuc_textures = []\n    nuc_labels = []\n    for slc, lab in zip(nuc_pos, nuc_lab):\n        if slc is None:\n            nuc_textures.append(np.zeros(len(metrics)))\n            nuc_labels.append(lab)\n            continue\n\n        nuc_gray = im_gray[slc] * (label[slc] == lab)\n\n        if nuc_gray.sum() == 0 or nuc_gray.shape[0] &lt; 4 or nuc_gray.shape[1] &lt; 4:\n            nuc_textures.append(np.zeros(len(metrics)))\n            nuc_labels.append(lab)\n            continue\n\n        texture_feats = _compute_texture_feats_np(\n            nuc_gray, metrics=metrics, distances=distances, angles=angles\n        )\n\n        nuc_textures.append(texture_feats)\n        nuc_labels.append(lab)\n\n    return pd.DataFrame(data=nuc_textures, index=nuc_labels)\n</code></pre>"},{"location":"api/spatial_agg/gini_index/","title":"gini_index","text":"<p>Compute the gini coefficient of inequality for species.</p> Note <p>This is based on http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm.</p> <p>Gini-index: $$ G = \\frac{\\sum_{i=1}^n (2i - n - 1)x_i} {n \\sum_{i=1}^n x_i} $$</p> <p>where \\(x_i\\) is the count of species \\(i\\) and \\(n\\) is the total count of species.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence</code> <p>The input value-vector. Shape (n, )</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are negative input values.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed Gini coefficient.</p> Source code in <code>src/histolytics/spatial_agg/diversity.py</code> <pre><code>def gini_index(x: Sequence) -&gt; float:\n    \"\"\"Compute the gini coefficient of inequality for species.\n\n    Note:\n        This is based on\n        http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm.\n\n    **Gini-index:**\n    $$\n    G = \\\\frac{\\\\sum_{i=1}^n (2i - n - 1)x_i} {n \\\\sum_{i=1}^n x_i}\n    $$\n\n    where $x_i$ is the count of species $i$ and $n$ is the total count of species.\n\n    Parameters:\n        x (Sequence):\n            The input value-vector. Shape (n, )\n\n    Raises:\n        ValueError:\n            If there are negative input values.\n\n    Returns:\n        float:\n            The computed Gini coefficient.\n    \"\"\"\n    if np.min(x) &lt; 0:\n        raise ValueError(\"Input values need to be positive for Gini coeff\")\n\n    n = len(x)\n    s = np.sum(x)\n    nx = n * s + SMALL\n\n    rx = (2.0 * np.arange(1, n + 1) * x[np.argsort(x)]).sum()\n    return (rx - nx - s) / nx\n</code></pre>"},{"location":"api/spatial_agg/grid_agg/","title":"grid_agg","text":"<p>Compute a metric for each grid cell based on the objects within/intersecting it.</p> Note <p>This function can be used to spatially aggregate tissue regions. The spatial aggregation metric function is self defined and can be any function that takes in a GeoDataFrame of objects and returns a single value.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>GeoDataFrame</code> <p>The grid cells to aggregate.</p> required <code>objs</code> <code>GeoDataFrame</code> <p>The objects to use for classification.</p> required <code>metric_func</code> <code>Callable</code> <p>The metric/heuristic function to use for aggregation.</p> required <code>predicate</code> <code>str</code> <p>The predicate to use for the spatial join. Allowed values are \"intersects\" and \"within\", \"contains\", \"contains_properly\".</p> required <code>new_col_names</code> <code>Union[Tuple[str, ...], str]</code> <p>The name of the new column(s) in the grid gdf.</p> required <code>parallel</code> <code>bool</code> <p>Whether to use parallel processing.</p> <code>True</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use. If -1, uses all available cores. Ignored if parallel=False.</p> <code>-1</code> <code>pbar</code> <code>bool</code> <p>Whether to show a progress bar. Ignored if parallel=False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If predicate is not one of \"intersects\" or \"within\".</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The grid with the new columns added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_ops.h3 import h3_grid\n&gt;&gt;&gt; from histolytics.data import cervix_tissue, cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_agg.grid_agg import grid_aggregate\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define the immune density metric function\n&gt;&gt;&gt; def immune_density(nuclei):\n&gt;&gt;&gt;     if \"inflammatory\" in nuclei.value_counts(\"class_name\"):\n&gt;&gt;&gt;         frac = nuclei.value_counts(\"class_name\", normalize=True)[\"inflammatory\"]\n&gt;&gt;&gt;     else:\n&gt;&gt;&gt;         frac = 0\n&gt;&gt;&gt;     return float(frac)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load the cervix nuclei and tissue data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; tis = cervix_tissue()\n&gt;&gt;&gt; # get the stromal tissue\n&gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n&gt;&gt;&gt; # Fit an H3 grid to the stromal tissue\n&gt;&gt;&gt; h3_gr = h3_grid(stroma, resolution=9)\n&gt;&gt;&gt; # Compute the immune density within the H3 grid cells\n&gt;&gt;&gt; grid = grid_aggregate(\n...     objs=nuc,\n...     grid=h3_gr,\n...     metric_func=immune_density,\n...     new_col_names=[\"immune_density\"],\n...     predicate=\"intersects\",\n...     num_processes=1,\n... )\n&gt;&gt;&gt; print(grid.head(3))\n                                                          geometry  immune_density\n8982a939503ffff  POLYGON ((6672.79721 859.08743, 6647.90711 661...        0.500000\n8982a939877ffff  POLYGON ((2556.61731 5658.46273, 2581.53692 58...        0.621951\n8982a939c4bffff  POLYGON ((4546.44516 4059.58249, 4366.53531 39...        0.045455\n&gt;&gt;&gt; # Plot the results\n&gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n&gt;&gt;&gt; h3_gr.plot(\n...     ax=ax,\n...     column=\"immune_density\",\n...     legend=True,\n...     facecolor=\"none\",\n...     lw=1,\n...     cmap=\"turbo\",\n... )\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_agg/grid_agg.py</code> <pre><code>def grid_aggregate(\n    grid: gpd.GeoDataFrame,\n    objs: gpd.GeoDataFrame,\n    metric_func: Callable,\n    predicate: str,\n    new_col_names: Union[Tuple[str, ...], str],\n    parallel: bool = True,\n    num_processes: int = -1,\n    pbar: bool = False,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute a metric for each grid cell based on the objects within/intersecting it.\n\n    Note:\n        This function can be used to spatially aggregate tissue regions. The spatial\n        aggregation metric function is self defined and can be any function that takes in\n        a GeoDataFrame of objects and returns a single value.\n\n    Parameters:\n        grid (gpd.GeoDataFrame):\n            The grid cells to aggregate.\n        objs (gpd.GeoDataFrame):\n            The objects to use for classification.\n        metric_func (Callable):\n            The metric/heuristic function to use for aggregation.\n        predicate (str):\n            The predicate to use for the spatial join. Allowed values are \"intersects\"\n            and \"within\", \"contains\", \"contains_properly\".\n        new_col_names (Union[Tuple[str, ...], str]):\n            The name of the new column(s) in the grid gdf.\n        parallel (bool):\n            Whether to use parallel processing.\n        num_processes (int):\n            The number of processes to use. If -1, uses all available cores.\n            Ignored if parallel=False.\n        pbar (bool):\n            Whether to show a progress bar. Ignored if parallel=False.\n\n    Raises:\n        ValueError: If predicate is not one of \"intersects\" or \"within\".\n\n    Returns:\n        gpd.GeoDataFrame:\n            The grid with the new columns added.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_ops.h3 import h3_grid\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue, cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_agg.grid_agg import grid_aggregate\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define the immune density metric function\n        &gt;&gt;&gt; def immune_density(nuclei):\n        &gt;&gt;&gt;     if \"inflammatory\" in nuclei.value_counts(\"class_name\"):\n        &gt;&gt;&gt;         frac = nuclei.value_counts(\"class_name\", normalize=True)[\"inflammatory\"]\n        &gt;&gt;&gt;     else:\n        &gt;&gt;&gt;         frac = 0\n        &gt;&gt;&gt;     return float(frac)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load the cervix nuclei and tissue data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; tis = cervix_tissue()\n        &gt;&gt;&gt; # get the stromal tissue\n        &gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n        &gt;&gt;&gt; # Fit an H3 grid to the stromal tissue\n        &gt;&gt;&gt; h3_gr = h3_grid(stroma, resolution=9)\n        &gt;&gt;&gt; # Compute the immune density within the H3 grid cells\n        &gt;&gt;&gt; grid = grid_aggregate(\n        ...     objs=nuc,\n        ...     grid=h3_gr,\n        ...     metric_func=immune_density,\n        ...     new_col_names=[\"immune_density\"],\n        ...     predicate=\"intersects\",\n        ...     num_processes=1,\n        ... )\n        &gt;&gt;&gt; print(grid.head(3))\n                                                                  geometry  immune_density\n        8982a939503ffff  POLYGON ((6672.79721 859.08743, 6647.90711 661...        0.500000\n        8982a939877ffff  POLYGON ((2556.61731 5658.46273, 2581.53692 58...        0.621951\n        8982a939c4bffff  POLYGON ((4546.44516 4059.58249, 4366.53531 39...        0.045455\n        &gt;&gt;&gt; # Plot the results\n        &gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n        &gt;&gt;&gt; h3_gr.plot(\n        ...     ax=ax,\n        ...     column=\"immune_density\",\n        ...     legend=True,\n        ...     facecolor=\"none\",\n        ...     lw=1,\n        ...     cmap=\"turbo\",\n        ... )\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/grid_aggregate.png)\n    \"\"\"\n    allowed = [\"intersects\", \"within\", \"contains\", \"contains_properly\"]\n    if predicate not in allowed:\n        raise ValueError(f\"predicate must be one of {allowed}. Got {predicate}\")\n\n    if isinstance(new_col_names, str):\n        new_col_names = [new_col_names]\n\n    func = partial(\n        get_cell_metric, objs=objs, predicate=predicate, metric_func=metric_func\n    )\n    grid.loc[:, list(new_col_names)] = gdf_apply(\n        grid,\n        func=func,\n        parallel=parallel,\n        pbar=pbar,\n        num_processes=num_processes,\n        columns=[\"geometry\"],\n        **kwargs,\n    )\n\n    return grid\n</code></pre>"},{"location":"api/spatial_agg/local_character/","title":"local_character","text":"<p>Compute the summary characteristics of neighboring feature values</p> Note <p>Neighborhoods are defined by the <code>spatial_weights</code> object, which can be created with the <code>fit_graph</code> function. The function should be applied to the input GeoDataFrame before using this function.</p> Note <p>Option to weight the nhood values by their area before reductions.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The input GeoDataFrame.</p> required <code>spatial_weights</code> <code>W</code> <p>Libpysal spatial weights object.</p> required <code>val_cols</code> <code>Tuple[str, ...]</code> <p>The name of the columns in the gdf for which the reduction is computed.</p> required <code>normalize</code> <code>bool</code> <p>Flag whether to column (quantile) normalize the computed metrics or not.</p> <code>False</code> <code>id_col</code> <code>str</code> <p>The unique id column in the gdf. If None, this uses <code>set_uid</code> to set it. Defaults to None.</p> <code>None</code> <code>reductions</code> <code>Tuple[str, ...]</code> <p>A list of reduction methods for the neighborhood feature values. Allowed are \"sum\", \"mean\", \"median\", \"min\", \"max\", \"std\", \"var\".</p> <code>('mean',)</code> <code>weight_by_area</code> <code>bool</code> <p>Flag whether to weight the neighborhood values by the area of the object. Defaults to False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Flag whether to use parallel apply operations when computing the character. Defaults to False.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use when parallel=True. If -1, this will use all available cores.</p> <code>1</code> <code>rm_nhood_cols</code> <code>bool</code> <p>Flag, whether to remove the extra neighborhood columns from the result gdf. Defaults to True.</p> <code>True</code> <code>col_prefix</code> <code>str</code> <p>Prefix for the new column names.</p> <code>None</code> <code>create_copy</code> <code>bool</code> <p>Flag whether to create a copy of the input gdf and return that. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The input geodataframe with computed character column added.</p> <p>Examples:</p> <p>Compute the mean of area values for each cell neighborhood</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt; from histolytics.spatial_agg.local_character import local_character\n&gt;&gt;&gt;\n&gt;&gt;&gt; # input data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; nuc = set_uid(nuc)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate shape metrics\n&gt;&gt;&gt; nuc = shape_metric(nuc, [\"area\"])\n&gt;&gt;&gt; # Fit delaunay graph\n&gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n&gt;&gt;&gt; # Compute local neighborhood summaries for shape metrics\n&gt;&gt;&gt; nuc = local_character(\n...     nuc,\n...     w,\n...     val_cols=[\"area\"],\n...     id_col=\"uid\",\n...     reductions=[\"mean\"],\n...     num_processes=6,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; print(nuc.head(3))\n        geometry        class_name  uid              uid\n    0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n    1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n    2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n            area  eccentricity  area_nhood_mean\n    uid\n    0    429.58790      0.960195       159.663283\n    1    408.46570      0.041712       237.720661\n    2    369.49285      0.610266       292.279720\n</code></pre> Source code in <code>src/histolytics/spatial_agg/local_character.py</code> <pre><code>def local_character(\n    gdf: gpd.GeoDataFrame,\n    spatial_weights: W,\n    val_cols: Tuple[str, ...],\n    normalize: bool = False,\n    id_col: str = None,\n    reductions: Tuple[str, ...] = (\"mean\",),\n    weight_by_area: bool = False,\n    parallel: bool = False,\n    num_processes: int = 1,\n    rm_nhood_cols: bool = True,\n    col_prefix: str = None,\n    create_copy: bool = True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute the summary characteristics of neighboring feature values\n\n    Note:\n        Neighborhoods are defined by the `spatial_weights` object, which can be created\n        with the `fit_graph` function. The function should be applied to the input\n        GeoDataFrame before using this function.\n\n    Note:\n        Option to weight the nhood values by their area before reductions.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The input GeoDataFrame.\n        spatial_weights (libysal.weights.W):\n            Libpysal spatial weights object.\n        val_cols (Tuple[str, ...]):\n            The name of the columns in the gdf for which the reduction is computed.\n        normalize (bool):\n            Flag whether to column (quantile) normalize the computed metrics or not.\n        id_col (str):\n            The unique id column in the gdf. If None, this uses `set_uid` to set it.\n            Defaults to None.\n        reductions (Tuple[str, ...]):\n            A list of reduction methods for the neighborhood feature values. Allowed are\n            \"sum\", \"mean\", \"median\", \"min\", \"max\", \"std\", \"var\".\n        weight_by_area (bool):\n            Flag whether to weight the neighborhood values by the area of the object.\n            Defaults to False.\n        parallel (bool):\n            Flag whether to use parallel apply operations when computing the character.\n            Defaults to False.\n        num_processes (int):\n            The number of processes to use when parallel=True. If -1,\n            this will use all available cores.\n        rm_nhood_cols (bool):\n            Flag, whether to remove the extra neighborhood columns from the result gdf.\n            Defaults to True.\n        col_prefix (str):\n            Prefix for the new column names.\n        create_copy (bool):\n            Flag whether to create a copy of the input gdf and return that.\n            Defaults to True.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The input geodataframe with computed character column added.\n\n    Examples:\n        Compute the mean of area values for each cell neighborhood\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt; from histolytics.spatial_agg.local_character import local_character\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # input data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; nuc = set_uid(nuc)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate shape metrics\n        &gt;&gt;&gt; nuc = shape_metric(nuc, [\"area\"])\n        &gt;&gt;&gt; # Fit delaunay graph\n        &gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n        &gt;&gt;&gt; # Compute local neighborhood summaries for shape metrics\n        &gt;&gt;&gt; nuc = local_character(\n        ...     nuc,\n        ...     w,\n        ...     val_cols=[\"area\"],\n        ...     id_col=\"uid\",\n        ...     reductions=[\"mean\"],\n        ...     num_processes=6,\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(nuc.head(3))\n                geometry        class_name  uid  \\\n            uid\n            0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n            1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n            2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n                    area  eccentricity  area_nhood_mean\n            uid\n            0    429.58790      0.960195       159.663283\n            1    408.46570      0.041712       237.720661\n            2    369.49285      0.610266       292.279720\n    \"\"\"\n    allowed = (\"sum\", \"mean\", \"median\", \"min\", \"max\", \"std\", \"var\")\n    if not all(r in allowed for r in reductions):\n        raise ValueError(\n            f\"Illegal reduction in `reductions`. Got: {reductions}. \"\n            f\"Allowed reductions: {allowed}.\"\n        )\n\n    if create_copy:\n        gdf = gdf.copy()\n\n    # set uid\n    if id_col is None:\n        id_col = \"uid\"\n        gdf = set_uid(gdf)\n\n    # Get the immediate node neighborhood\n    func = partial(nhood, spatial_weights=spatial_weights)\n    gdf[\"nhood\"] = gdf_apply(\n        gdf,\n        func,\n        columns=[id_col],\n        axis=1,\n        parallel=parallel,\n        num_processes=num_processes,\n    )\n\n    # get areas\n    area_col = None\n    if weight_by_area:\n        area_col = \"nhood_areas\"\n        func = partial(nhood_vals, values=gdf.geometry.area)\n        gdf[area_col] = gdf_apply(\n            gdf,\n            func,\n            columns=[\"nhood\"],\n            axis=1,\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n\n    # get character values\n    # Compute the neighborhood characters\n    col_prefix = \"\" if col_prefix is None else col_prefix\n    for col in val_cols:\n        values = gdf[col]\n        char_col = f\"{col}_nhood_vals\"\n        func = partial(nhood_vals, values=values)\n        gdf[char_col] = gdf_apply(\n            gdf,\n            func,\n            columns=[\"nhood\"],\n            axis=1,\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n\n        # loop over the reduction methods\n        for r in reductions:\n            columns = [char_col]\n            new_col = f\"{col_prefix}{col}_nhood_{r}\"\n            if area_col in gdf.columns:\n                columns.append(area_col)\n                new_col = f\"{col_prefix}{col}_nhood_{r}_area_weighted\"\n\n            func = partial(reduce, how=r)\n            gdf[new_col] = gdf_apply(\n                gdf,\n                func,\n                columns=columns,\n                axis=1,\n                parallel=parallel,\n                num_processes=num_processes,\n            )\n        # normalize the character values\n        if normalize:\n            gdf[new_col] = col_norm(gdf[new_col])\n\n        if rm_nhood_cols:\n            gdf = gdf.drop(labels=[char_col], axis=1)\n\n    if rm_nhood_cols:\n        labs = [\"nhood\"]\n        if weight_by_area:\n            labs.append(area_col)\n        gdf = gdf.drop(labels=labs, axis=1)\n\n    return gdf\n</code></pre>"},{"location":"api/spatial_agg/local_distances/","title":"local_distances","text":"<p>Compute the distances to the neighboring objects for every object in a GeoDataFrame. and aggregate them by the specified reduction methods.</p> Note <p>Neighborhoods are defined by the <code>spatial_weights</code> object, which can be created with the <code>fit_graph</code> function. The function should be applied to the input GeoDataFrame before using this function.</p> Note <p>Option to weight the nhood values by their area before reductions.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The input GeoDataFrame.</p> required <code>spatial_weights</code> <code>W</code> <p>Libpysal spatial weights object.</p> required <code>normalize</code> <code>bool</code> <p>Flag whether to column (quantile) normalize the computed metrics or not.</p> <code>False</code> <code>id_col</code> <code>str</code> <p>The unique id column in the gdf. If None, this uses <code>set_uid</code> to set it. Defaults to None.</p> <code>None</code> <code>reductions</code> <code>Tuple[str, ...], default=(\"mean\",)</code> <p>A list of reduction methods for the neighborhood feature values. Allowed are \"sum\", \"mean\", \"median\", \"min\", \"max\", \"std\".</p> <code>('mean',)</code> <code>weight_by_area</code> <code>bool</code> <p>Flag whether to weight the neighborhood values by the area of the object. Defaults to False.</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Flag whether to invert the distances. Defaults to False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Flag whether to use parallel apply operations when computing the character. Defaults to False.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use when parallel=True. If -1, this will use all available cores.</p> <code>1</code> <code>rm_nhood_cols</code> <code>bool</code> <p>Flag, whether to remove the extra neighborhood columns from the result gdf. Defaults to True.</p> <code>True</code> <code>col_prefix</code> <code>str</code> <p>Prefix for the new column names.</p> <code>None</code> <code>create_copy</code> <code>bool</code> <p>Flag whether to create a copy of the input gdf and return that. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>reductions</code> parameter contains an illegal reduction method.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The input geodataframe with computed distances column added.</p> <p>Examples:</p> <p>Compute the mean of eccentricity values for each neighborhood</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt; from histolytics.spatial_agg.local_distances import local_distances\n&gt;&gt;&gt;\n&gt;&gt;&gt; # input data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; nuc = set_uid(nuc)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit delaunay graph\n&gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n&gt;&gt;&gt; # Compute local neighborhood distances for shape metrics\n&gt;&gt;&gt; nuc = local_distances(\n...     nuc,\n...     w,\n...     id_col=\"uid\",\n...     reductions=[\"mean\"],\n...     num_processes=6,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; print(nuc.head(3))\n        geometry        class_name  uid              uid\n    0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n    1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n    2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n            nhood_dists_mean\n    uid\n    0        48.500637\n    1        55.802475\n    2        37.081177\n</code></pre> Source code in <code>src/histolytics/spatial_agg/local_distances.py</code> <pre><code>def local_distances(\n    gdf: gpd.GeoDataFrame,\n    spatial_weights: W,\n    normalize: bool = False,\n    id_col: str = None,\n    reductions: Tuple[str, ...] = (\"mean\",),\n    weight_by_area: bool = False,\n    invert: bool = False,\n    parallel: bool = False,\n    num_processes: int = 1,\n    rm_nhood_cols: bool = True,\n    col_prefix: str = None,\n    create_copy: bool = True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute the distances to the neighboring objects for every object in a GeoDataFrame.\n    and aggregate them by the specified reduction methods.\n\n    Note:\n        Neighborhoods are defined by the `spatial_weights` object, which can be created\n        with the `fit_graph` function. The function should be applied to the input\n        GeoDataFrame before using this function.\n\n    Note:\n        Option to weight the nhood values by their area before reductions.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The input GeoDataFrame.\n        spatial_weights (libysal.weights.W):\n            Libpysal spatial weights object.\n        normalize (bool):\n            Flag whether to column (quantile) normalize the computed metrics or not.\n        id_col (str):\n            The unique id column in the gdf. If None, this uses `set_uid` to set it.\n            Defaults to None.\n        reductions (Tuple[str, ...], default=(\"mean\",)):\n            A list of reduction methods for the neighborhood feature values. Allowed are\n            \"sum\", \"mean\", \"median\", \"min\", \"max\", \"std\".\n        weight_by_area (bool):\n            Flag whether to weight the neighborhood values by the area of the object.\n            Defaults to False.\n        invert (bool):\n            Flag whether to invert the distances. Defaults to False.\n        parallel (bool):\n            Flag whether to use parallel apply operations when computing the character.\n            Defaults to False.\n        num_processes (int):\n            The number of processes to use when parallel=True. If -1,\n            this will use all available cores.\n        rm_nhood_cols (bool):\n            Flag, whether to remove the extra neighborhood columns from the result gdf.\n            Defaults to True.\n        col_prefix (str):\n            Prefix for the new column names.\n        create_copy (bool):\n            Flag whether to create a copy of the input gdf and return that.\n            Defaults to True.\n\n    Raises:\n        ValueError: If the `reductions` parameter contains an illegal reduction method.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The input geodataframe with computed distances column added.\n\n    Examples:\n        Compute the mean of eccentricity values for each neighborhood\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt; from histolytics.spatial_agg.local_distances import local_distances\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # input data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; nuc = set_uid(nuc)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit delaunay graph\n        &gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n        &gt;&gt;&gt; # Compute local neighborhood distances for shape metrics\n        &gt;&gt;&gt; nuc = local_distances(\n        ...     nuc,\n        ...     w,\n        ...     id_col=\"uid\",\n        ...     reductions=[\"mean\"],\n        ...     num_processes=6,\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(nuc.head(3))\n                geometry        class_name  uid  \\\n            uid\n            0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n            1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n            2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n                    nhood_dists_mean\n            uid\n            0        48.500637\n            1        55.802475\n            2        37.081177\n    \"\"\"\n    allowed = (\"sum\", \"mean\", \"median\", \"min\", \"max\", \"std\")\n    if not all(r in allowed for r in reductions):\n        raise ValueError(\n            f\"Illegal reduction in `reductions`. Got: {reductions}. \"\n            f\"Allowed reductions: {allowed}.\"\n        )\n\n    if create_copy:\n        gdf = gdf.copy()\n\n    # set uid\n    if id_col is None:\n        id_col = \"uid\"\n        gdf = set_uid(gdf)\n\n    # get the immediate node neighborhood\n    func = partial(nhood, spatial_weights=spatial_weights)\n    gdf[\"nhood\"] = gdf_apply(\n        gdf,\n        func,\n        columns=[id_col],\n        axis=1,\n        parallel=parallel,\n        num_processes=num_processes,\n    )\n\n    # get areas\n    area_col = None\n    if weight_by_area:\n        func = partial(nhood_vals, values=gdf.geometry.area)\n        gdf[area_col] = gdf_apply(\n            gdf,\n            func,\n            columns=[\"nhood\"],\n            axis=1,\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n\n    # get distances\n    func = partial(nhood_dists, centroids=gdf.centroid, invert=invert)\n    gdf[\"nhood_dists\"] = gdf_apply(\n        gdf,\n        func,\n        columns=[\"nhood\"],\n        axis=1,\n        parallel=parallel,\n        num_processes=num_processes,\n    )\n\n    col_prefix = \"\" if col_prefix is None else col_prefix\n\n    # loop over the reduction methods\n    for r in reductions:\n        columns = [\"nhood_dists\"]\n        new_col = f\"{col_prefix}nhood_dists_{r}\"\n        if area_col in gdf.columns:\n            columns.append(area_col)\n            new_col = f\"{col_prefix}nhood_dists_{r}_area_weighted\"\n\n        func = partial(reduce, how=r)\n        gdf[new_col] = gdf_apply(\n            gdf,\n            func,\n            columns=columns,\n            axis=1,\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n        # normalize the character values\n        if normalize:\n            gdf[new_col] = col_norm(gdf[new_col])\n\n    if rm_nhood_cols:\n        labs = [\"nhood\", \"nhood_dists\"]\n        if weight_by_area:\n            labs.append(area_col)\n        gdf = gdf.drop(labels=labs, axis=1)\n\n    return gdf\n</code></pre>"},{"location":"api/spatial_agg/local_diversity/","title":"local_diversity","text":"<p>Compute the diversity of neighboring feature values for every object in a GeoDataFrame.</p> Note <p>Neighborhoods are defined by the <code>spatial_weights</code> object, which can be created with the <code>fit_graph</code> function. The function should be applied to the input GeoDataFrame before using this function.</p> Note <p>Allowed diversity metrics:</p> <ul> <li><code>simpson_index</code> - for both categorical and real valued neighborhoods</li> <li><code>shannon_index</code> - for both categorical and real valued neighborhoods</li> <li><code>gini_index</code> - for only real valued neighborhoods</li> <li><code>theil_index</code> - for only real valued neighborhoods</li> </ul> Note <p>If <code>val_cols</code> is not categorical, the values are binned using <code>mapclassify</code>. The bins are then used to compute the diversity metrics. If <code>val_cols</code> is categorical, the values are used directly.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The input GeoDataFrame.</p> required <code>spatial_weights</code> <code>W</code> <p>Libpysal spatial weights object.</p> required <code>val_cols</code> <code>Tuple[str, ...]</code> <p>The name of the column in the gdf for which the diversity is computed. You can also pass in a list of columns, in which case the diversity is computed for each column.</p> required <code>normalize</code> <code>bool</code> <p>Flag whether to column (quantile) normalize the computed metrics or not.</p> <code>False</code> <code>id_col</code> <code>str</code> <p>The unique id column in the gdf. If None, this uses <code>set_uid</code> to set it. Defaults to None.</p> <code>None</code> <code>metrics</code> <code>Tuple[str, ...], default=(\"simpson_index\",)</code> <p>A Tuple/List of diversity metrics. Allowed metrics: \"shannon_index\", \"simpson_index\", \"gini_index\", \"theil_index\".</p> <code>('simpson_index',)</code> <code>scheme</code> <code>str</code> <p><code>mapclassify</code> classification scheme. Defaults to \"FisherJenks\". One of: - quantiles - boxplot - equalinterval - fisherjenks - fisherjenkssampled - headtailbreaks - jenkscaspall - jenkscaspallsampled - jenks_caspallforced - maxp - maximumbreaks - naturalbreaks - percentiles - prettybreaks - stdmean - userdefined</p> <code>'fisherjenks'</code> <code>k</code> <code>int</code> <p>Number of classes for the classification scheme. Defaults to 5.</p> <code>5</code> <code>parallel</code> <code>bool</code> <p>Flag whether to use parallel apply operations when computing the diversities. Defaults to False.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use when parallel=True. If -1, this will use all available cores.</p> <code>1</code> <code>rm_nhood_cols</code> <code>bool</code> <p>Flag, whether to remove the extra neighborhood columns from the result gdf. Defaults to True.</p> <code>True</code> <code>col_prefix</code> <code>str</code> <p>Prefix for the new column names. Defaults to None.</p> <code>None</code> <code>create_copy</code> <code>bool</code> <p>Flag whether to create a copy of the input gdf or not. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an illegal metric is given.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The input geodataframe with computed diversity metric columns added.</p> <p>Examples:</p> <p>Compute the simpson diversity of cell types in the neighborhood of nuclei</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.spatial_agg.local_diversity import local_diversity\n&gt;&gt;&gt; from histolytics.data import cervix_nuclei, cervix_tissue\n&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt;\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; nuc = set_uid(nuc)  # ensure unique IDs for nuclei\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit delaunay graph\n&gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute local cell type diversity with simpson index and shannon entropy\n&gt;&gt;&gt; nuc = local_diversity(\n...     nuc,\n...     w,\n...     id_col=\"uid\",\n...     val_cols=[\"class_name\"],\n...     metrics=[\"simpson_index\"],\n...     num_processes=6,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; print(nuc.head(3))\n            geometry        class_name  uid              uid\n    0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n    1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n    2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n        class_name_shannon_index\n    uid\n    0                    0.636514\n    1                    0.636514\n    2                    1.332179\n</code></pre> Source code in <code>src/histolytics/spatial_agg/local_diversity.py</code> <pre><code>def local_diversity(\n    gdf: gpd.GeoDataFrame,\n    spatial_weights: W,\n    val_cols: Tuple[str, ...],\n    normalize: bool = False,\n    id_col: str = None,\n    metrics: Tuple[str, ...] = (\"simpson_index\",),\n    scheme: str = \"fisherjenks\",\n    k: int = 5,\n    parallel: bool = False,\n    num_processes: int = 1,\n    rm_nhood_cols: bool = True,\n    col_prefix: str = None,\n    create_copy: bool = True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute the diversity of neighboring feature values for every object in a GeoDataFrame.\n\n    Note:\n        Neighborhoods are defined by the `spatial_weights` object, which can be created\n        with the `fit_graph` function. The function should be applied to the input\n        GeoDataFrame before using this function.\n\n    Note:\n        Allowed diversity metrics:\n\n        - `simpson_index` - for both categorical and real valued neighborhoods\n        - `shannon_index` - for both categorical and real valued neighborhoods\n        - `gini_index` - for only real valued neighborhoods\n        - `theil_index` - for only real valued neighborhoods\n\n    Note:\n        If `val_cols` is not categorical, the values are binned using `mapclassify`.\n        The bins are then used to compute the diversity metrics. If `val_cols` is\n        categorical, the values are used directly.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The input GeoDataFrame.\n        spatial_weights (libysal.weights.W):\n            Libpysal spatial weights object.\n        val_cols (Tuple[str, ...]):\n            The name of the column in the gdf for which the diversity is computed.\n            You can also pass in a list of columns, in which case the diversity is\n            computed for each column.\n        normalize (bool):\n            Flag whether to column (quantile) normalize the computed metrics or not.\n        id_col (str):\n            The unique id column in the gdf. If None, this uses `set_uid` to set it.\n            Defaults to None.\n        metrics (Tuple[str, ...], default=(\"simpson_index\",)):\n            A Tuple/List of diversity metrics. Allowed metrics: \"shannon_index\",\n            \"simpson_index\", \"gini_index\", \"theil_index\".\n        scheme (str):\n            `mapclassify` classification scheme. Defaults to \"FisherJenks\". One of:\n            - quantiles\n            - boxplot\n            - equalinterval\n            - fisherjenks\n            - fisherjenkssampled\n            - headtailbreaks\n            - jenkscaspall\n            - jenkscaspallsampled\n            - jenks_caspallforced\n            - maxp\n            - maximumbreaks\n            - naturalbreaks\n            - percentiles\n            - prettybreaks\n            - stdmean\n            - userdefined\n        k (int):\n            Number of classes for the classification scheme. Defaults to 5.\n        parallel (bool):\n            Flag whether to use parallel apply operations when computing the diversities.\n            Defaults to False.\n        num_processes (int):\n            The number of processes to use when parallel=True. If -1,\n            this will use all available cores.\n        rm_nhood_cols (bool):\n            Flag, whether to remove the extra neighborhood columns from the result gdf.\n            Defaults to True.\n        col_prefix (str):\n            Prefix for the new column names. Defaults to None.\n        create_copy (bool):\n            Flag whether to create a copy of the input gdf or not. Defaults to True.\n\n    Raises:\n        ValueError:\n            If an illegal metric is given.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The input geodataframe with computed diversity metric columns added.\n\n    Examples:\n        Compute the simpson diversity of cell types in the neighborhood of nuclei\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.spatial_agg.local_diversity import local_diversity\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei, cervix_tissue\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; nuc = set_uid(nuc)  # ensure unique IDs for nuclei\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit delaunay graph\n        &gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute local cell type diversity with simpson index and shannon entropy\n        &gt;&gt;&gt; nuc = local_diversity(\n        ...     nuc,\n        ...     w,\n        ...     id_col=\"uid\",\n        ...     val_cols=[\"class_name\"],\n        ...     metrics=[\"simpson_index\"],\n        ...     num_processes=6,\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(nuc.head(3))\n                    geometry        class_name  uid  \\\n            uid\n            0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n            1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n            2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n                class_name_shannon_index\n            uid\n            0                    0.636514\n            1                    0.636514\n            2                    1.332179\n    \"\"\"\n    allowed = list(DIVERSITY_LOOKUP.keys())\n    if not all(m in allowed for m in metrics):\n        raise ValueError(\n            f\"Illegal metric in `metrics`. Got: {metrics}. Allowed metrics: {allowed}.\"\n        )\n\n    if create_copy:\n        gdf = gdf.copy()\n\n    # set uid\n    if id_col is None:\n        id_col = \"uid\"\n        gdf = set_uid(gdf)\n\n    # If shannon or simpson index in metrics, counts are needed\n    ret_counts = False\n    if any([m in metrics for m in (\"simpson_index\", \"shannon_index\")]):\n        ret_counts = True\n\n    # If Gini is in metrics, neighboring values are needed\n    gt = (\"gini_index\", \"theil_index\")\n    ret_vals = False\n    if any([m in metrics for m in gt]):\n        ret_vals = True\n\n    # Get the immediate node neighborhood\n    func = partial(nhood, spatial_weights=spatial_weights)\n    gdf[\"nhood\"] = gdf_apply(\n        gdf,\n        func,\n        columns=[id_col],\n        axis=1,\n        parallel=parallel,\n        num_processes=num_processes,\n    )\n\n    for col in val_cols:\n        values = gdf[col]\n\n        # Get bins if data not categorical\n        if not is_categorical(values):\n            bins = mapclassify.classify(values, scheme=scheme, k=k).bins\n        else:\n            bins = None\n\n        # Get the counts of the binned metric inside the neighborhoods\n        if ret_counts:\n            func = partial(nhood_counts, values=values, bins=bins)\n            gdf[f\"{col}_nhood_counts\"] = gdf_apply(\n                gdf,\n                func,\n                columns=[\"nhood\"],\n                axis=1,\n                parallel=parallel,\n                num_processes=num_processes,\n            )\n\n        if ret_vals:\n            func = partial(nhood_vals, values=values)\n            gdf[f\"{col}_nhood_vals\"] = gdf_apply(\n                gdf,\n                func,\n                columns=[\"nhood\"],\n                axis=1,\n                parallel=parallel,\n                num_processes=num_processes,\n            )\n\n        # Compute the diversity metrics for the neighborhood counts\n        for metric in metrics:\n            colname = f\"{col}_nhood_counts\" if metric not in gt else f\"{col}_nhood_vals\"\n\n            col_prefix = \"\" if col_prefix is None else col_prefix\n            gdf[f\"{col_prefix}{col}_{metric}\"] = gdf_apply(\n                gdf,\n                DIVERSITY_LOOKUP[metric],\n                columns=[colname],\n                parallel=parallel,\n                num_processes=num_processes,\n            )\n            # normalize the character values\n            if normalize:\n                gdf[f\"{col_prefix}{col}_{metric}\"] = col_norm(\n                    gdf[f\"{col_prefix}{col}_{metric}\"]\n                )\n\n        if rm_nhood_cols:\n            gdf = gdf.drop(labels=[colname], axis=1)\n\n    if rm_nhood_cols:\n        gdf = gdf.drop(labels=[\"nhood\"], axis=1)\n\n    return gdf\n</code></pre>"},{"location":"api/spatial_agg/local_type_counts/","title":"local_type_counts","text":"<p>Get the neighboring cell/nuclei type counts for every object in a GeoDataFrame.</p> Note <p>Neighborhoods are defined by the <code>spatial_weights</code> object, which can be created with the <code>fit_graph</code> function. The function should be applied to the input GeoDataFrame before using this function.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the spatial data.</p> required <code>spatial_weights</code> <code>W</code> <p>A libpysal weights object defining the spatial relationships.</p> required <code>class_name</code> <code>str</code> <p>The name of the class for which to count local types.</p> required <code>id_col</code> <code>str</code> <p>The unique id column in the gdf. If None, this uses <code>set_uid</code> to set it. Defaults to None.</p> <code>None</code> <code>frac</code> <code>bool</code> <p>Whether to return the counts as fractions of the total neighborhood size. Defaults to False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to apply the function in parallel. Defaults to False.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use if <code>parallel</code> is True. Defaults to 1.</p> <code>1</code> <code>create_copy</code> <code>bool</code> <p>Flag whether to create a copy of the input gdf and return that. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The original GeoDataFrame with an additional column for local type counts.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt; from histolytics.spatial_agg.local_values import local_type_counts\n&gt;&gt;&gt;\n&gt;&gt;&gt; # input data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; nuc = set_uid(nuc)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit delaunay graph\n&gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n&gt;&gt;&gt; # Get the local counts of inflammatory cells in each neighborhood\n&gt;&gt;&gt; nuc = local_type_counts(\n...     nuc,\n...     w,\n...     class_name=\"inflammatory\",\n...     id_col=\"uid\",\n...     num_processes=6,\n... )\n&gt;&gt;&gt; print(nuc.head(3))\n        geometry        class_name  uid              uid\n    0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n    1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n    2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n                                            nhood_classes  inflammatory_cnt\n    uid\n    0    [connective, connective, connective, inflammat...               2.0\n    1    [connective, connective, connective, connectiv...               0.0\n    2    [squamous_epithel, connective, connective, gla...               0.0\n</code></pre> Source code in <code>src/histolytics/spatial_agg/local_values.py</code> <pre><code>def local_type_counts(\n    gdf: gpd.GeoDataFrame,\n    spatial_weights: W,\n    class_name: str,\n    id_col: str = None,\n    frac: bool = False,\n    parallel: bool = False,\n    num_processes: int = 1,\n    create_copy: bool = True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get the neighboring cell/nuclei type counts for every object in a GeoDataFrame.\n\n    Note:\n        Neighborhoods are defined by the `spatial_weights` object, which can be created\n        with the `fit_graph` function. The function should be applied to the input\n        GeoDataFrame before using this function.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the spatial data.\n        spatial_weights (W):\n            A libpysal weights object defining the spatial relationships.\n        class_name (str):\n            The name of the class for which to count local types.\n        id_col (str):\n            The unique id column in the gdf. If None, this uses `set_uid` to set it.\n            Defaults to None.\n        frac (bool):\n            Whether to return the counts as fractions of the total neighborhood size.\n            Defaults to False.\n        parallel (bool):\n            Whether to apply the function in parallel. Defaults to False.\n        num_processes (int):\n            The number of processes to use if `parallel` is True. Defaults to 1.\n        create_copy (bool):\n            Flag whether to create a copy of the input gdf and return that.\n            Defaults to True.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The original GeoDataFrame with an additional column for local type counts.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt; from histolytics.spatial_agg.local_values import local_type_counts\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # input data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; nuc = set_uid(nuc)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit delaunay graph\n        &gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n        &gt;&gt;&gt; # Get the local counts of inflammatory cells in each neighborhood\n        &gt;&gt;&gt; nuc = local_type_counts(\n        ...     nuc,\n        ...     w,\n        ...     class_name=\"inflammatory\",\n        ...     id_col=\"uid\",\n        ...     num_processes=6,\n        ... )\n        &gt;&gt;&gt; print(nuc.head(3))\n                geometry        class_name  uid  \\\n            uid\n            0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n            1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n            2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n                                                    nhood_classes  inflammatory_cnt\n            uid\n            0    [connective, connective, connective, inflammat...               2.0\n            1    [connective, connective, connective, connectiv...               0.0\n            2    [squamous_epithel, connective, connective, gla...               0.0\n    \"\"\"\n    if \"nhood_classes\" not in gdf.columns:\n        gdf = local_vals(\n            gdf,\n            spatial_weights,\n            val_col=\"class_name\",\n            new_col_name=\"nhood_classes\",\n            id_col=id_col,\n            parallel=parallel,\n            num_processes=num_processes,\n            create_copy=create_copy,\n        )\n\n    func = partial(nhood_type_count, value=class_name, frac=frac)\n    name = f\"{class_name}_frac\" if frac else f\"{class_name}_cnt\"\n    gdf[name] = gdf_apply(\n        gdf,\n        func=func,\n        axis=1,\n        parallel=parallel,\n        num_processes=num_processes,\n        columns=[\"nhood_classes\"],\n    )\n\n    return gdf\n</code></pre>"},{"location":"api/spatial_agg/local_vals/","title":"local_vals","text":"<p>Get the neighboring feature values for every object in a GeoDataFrame.</p> Note <p>Neighborhoods are defined by the <code>spatial_weights</code> object, which can be created with the <code>fit_graph</code> function. The function should be applied to the input GeoDataFrame before using this function.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the spatial data.</p> required <code>spatial_weights</code> <code>W</code> <p>A libpysal weights object defining the spatial relationships.</p> required <code>val_col</code> <code>str</code> <p>The column name in <code>gdf</code> from which to derive neighborhood values.</p> required <code>new_col_name</code> <code>str</code> <p>The name of the new column to store neighborhood values.</p> required <code>id_col</code> <code>str</code> <p>The unique id column in the gdf. If None, this uses <code>set_uid</code> to set it. Defaults to None.</p> <code>None</code> <code>parallel</code> <code>bool</code> <p>Whether to apply the function in parallel. Defaults to False.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use if <code>parallel</code> is True. Defaults to 1.</p> <code>1</code> <code>create_copy</code> <code>bool</code> <p>Flag whether to create a copy of the input gdf and return that. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The original GeoDataFrame with an additional column for neighborhood values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt; from histolytics.spatial_agg.local_values import local_vals\n&gt;&gt;&gt;\n&gt;&gt;&gt; # input data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; nuc = set_uid(nuc)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate shape metrics\n&gt;&gt;&gt; nuc = shape_metric(nuc, [\"area\"])\n&gt;&gt;&gt; nuc[\"area\"] = nuc[\"area\"].round(3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit delaunay graph\n&gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n&gt;&gt;&gt; # Get the local areas of nuclei in each neighborhood\n&gt;&gt;&gt; nuc = local_vals(\n...     nuc,\n...     w,\n...     val_col=\"area\",\n...     new_col_name=\"local_areas\",\n...     id_col=\"uid\",\n...     num_processes=6,\n... )\n&gt;&gt;&gt; print(nuc.head(3))\n            geometry        class_name  uid              uid\n    0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n    1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n    2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n            area                                        local_areas\n    uid\n    0    429.588  [429.588, 171.402, 130.916, 129.895, 52.101, 4...\n    1    408.466  [408.466, 226.671, 151.296, 107.531, 67.125, 5...\n    2    369.493  [369.493, 330.894, 215.215, 127.846, 417.95]7, 2...\n</code></pre> Source code in <code>src/histolytics/spatial_agg/local_values.py</code> <pre><code>def local_vals(\n    gdf: gpd.GeoDataFrame,\n    spatial_weights: W,\n    val_col: str,\n    new_col_name: str,\n    id_col: str = None,\n    parallel: bool = False,\n    num_processes: int = 1,\n    create_copy: bool = True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get the neighboring feature values for every object in a GeoDataFrame.\n\n    Note:\n        Neighborhoods are defined by the `spatial_weights` object, which can be created\n        with the `fit_graph` function. The function should be applied to the input\n        GeoDataFrame before using this function.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the spatial data.\n        spatial_weights (W):\n            A libpysal weights object defining the spatial relationships.\n        val_col (str):\n            The column name in `gdf` from which to derive neighborhood values.\n        new_col_name (str):\n            The name of the new column to store neighborhood values.\n        id_col (str):\n            The unique id column in the gdf. If None, this uses `set_uid` to set it.\n            Defaults to None.\n        parallel (bool):\n            Whether to apply the function in parallel. Defaults to False.\n        num_processes (int):\n            The number of processes to use if `parallel` is True. Defaults to 1.\n        create_copy (bool):\n            Flag whether to create a copy of the input gdf and return that.\n            Defaults to True.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The original GeoDataFrame with an additional column for neighborhood values.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt; from histolytics.spatial_agg.local_values import local_vals\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # input data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; nuc = set_uid(nuc)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate shape metrics\n        &gt;&gt;&gt; nuc = shape_metric(nuc, [\"area\"])\n        &gt;&gt;&gt; nuc[\"area\"] = nuc[\"area\"].round(3)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit delaunay graph\n        &gt;&gt;&gt; w, _ = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100, use_polars=True)\n        &gt;&gt;&gt; # Get the local areas of nuclei in each neighborhood\n        &gt;&gt;&gt; nuc = local_vals(\n        ...     nuc,\n        ...     w,\n        ...     val_col=\"area\",\n        ...     new_col_name=\"local_areas\",\n        ...     id_col=\"uid\",\n        ...     num_processes=6,\n        ... )\n        &gt;&gt;&gt; print(nuc.head(3))\n                    geometry        class_name  uid  \\\n            uid\n            0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n            1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n            2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n                    area                                        local_areas\n            uid\n            0    429.588  [429.588, 171.402, 130.916, 129.895, 52.101, 4...\n            1    408.466  [408.466, 226.671, 151.296, 107.531, 67.125, 5...\n            2    369.493  [369.493, 330.894, 215.215, 127.846, 417.95]7, 2...\n    \"\"\"\n    if create_copy:\n        gdf = gdf.copy()\n\n    # set uid\n    if id_col is None:\n        id_col = \"uid\"\n        gdf = set_uid(gdf)\n\n    nhoods = partial(nhood, spatial_weights=spatial_weights)\n    gdf[\"nhood\"] = gdf_apply(\n        gdf,\n        nhoods,\n        columns=[\"uid\"],\n        axis=1,\n        parallel=parallel,\n        num_processes=num_processes,\n    )\n\n    nhood_val_func = partial(nhood_vals, values=gdf[val_col])\n    gdf[new_col_name] = gdf_apply(\n        gdf,\n        nhood_val_func,\n        columns=[\"nhood\"],\n        axis=1,\n        parallel=parallel,\n        num_processes=num_processes,\n    )\n\n    return gdf.drop(columns=[\"nhood\"])\n</code></pre>"},{"location":"api/spatial_agg/shannon_index/","title":"shannon_index","text":"<p>Compute the Shannon Weiner index/entropy on a count vector.</p> Note <p>\"The Shannon index is related to the concept of uncertainty. If for example, a community has very low diversity, we can be fairly certain of the identity of an organism we might choose by random (high certainty or low uncertainty). If a community is highly diverse and we choose an organism by random, we have a greater uncertainty of which species we will choose (low certainty or high uncertainty).\" - A. Wilson, N. Gownaris</p> <p>Shannon index: $$ H^{\\prime} = -\\sum_{i=1}^n p_i \\ln(p_i) $$</p> <p>where \\(p_i\\) is the proportion of species \\(i\\) and \\(n\\) is the total count of species.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>Sequence</code> <p>A count vector/list of shape (C, ).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed Shannon diversity index.</p> Source code in <code>src/histolytics/spatial_agg/diversity.py</code> <pre><code>def shannon_index(counts: Sequence) -&gt; float:\n    \"\"\"Compute the Shannon Weiner index/entropy on a count vector.\n\n    Note:\n        \"*The Shannon index is related to the concept of uncertainty. If for example,\n        a community has very low diversity, we can be fairly certain of the identity of\n        an organism we might choose by random (high certainty or low uncertainty). If a\n        community is highly diverse and we choose an organism by random, we have a\n        greater uncertainty of which species we will choose (low certainty or high\n        uncertainty).*\"\n        - [A. Wilson, N. Gownaris](https://bio.libretexts.org/Courses/Gettysburg_College/01%3A_Ecology_for_All/22%3A_Biodiversity/22.02%3A_Diversity_Indices)\n\n    **Shannon index:**\n    $$\n    H^{\\\\prime} = -\\\\sum_{i=1}^n p_i \\\\ln(p_i)\n    $$\n\n    where $p_i$ is the proportion of species $i$ and $n$ is the total count of species.\n\n    Parameters:\n        counts (Sequence):\n            A count vector/list of shape (C, ).\n\n    Returns:\n        float:\n            The computed Shannon diversity index.\n    \"\"\"\n    N = np.sum(counts) + SMALL\n    probs = [float(n) / N for n in counts]\n\n    entropy = -np.sum([p * np.log(p) for p in probs if p != 0])\n\n    if entropy == 0:\n        return 0.0\n\n    return entropy\n</code></pre>"},{"location":"api/spatial_agg/simpson_index/","title":"simpson_index","text":"<p>Compute the Simpson diversity index on a count vector.</p> Note <p>Simpson diversity index is a quantitative measure that reflects how many different types (such as species) there are in a dataset (a community). It is a probability measure, when it is low, the greater the probability that two randomly selected individuals will be the same species. - A. Wilson, N. Gownaris</p> <p>Simpson index: $$ D = 1 - \\sum_{i=1}^n \\left(\\frac{n_i}{N}\\right)^2 $$</p> <p>where \\(n_i\\) is the count of species \\(i\\) and \\(N\\) is the total count of species.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>Sequence</code> <p>A count vector/list of shape (C, ).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed Simpson diversity index.</p> Source code in <code>src/histolytics/spatial_agg/diversity.py</code> <pre><code>def simpson_index(counts: Sequence) -&gt; float:\n    \"\"\"Compute the Simpson diversity index on a count vector.\n\n    Note:\n        Simpson diversity index is a quantitative measure that reflects how many\n        different types (such as species) there are in a dataset (a community). It\n        is a probability measure, when it is low, the greater the probability that\n        two randomly selected individuals will be the same species.\n        - [A. Wilson, N. Gownaris](https://bio.libretexts.org/Courses/Gettysburg_College/01%3A_Ecology_for_All/22%3A_Biodiversity/22.02%3A_Diversity_Indices)\n\n\n    **Simpson index:**\n    $$\n    D = 1 - \\\\sum_{i=1}^n \\\\left(\\\\frac{n_i}{N}\\\\right)^2\n    $$\n\n    where $n_i$ is the count of species $i$ and $N$ is the total count of species.\n\n    Parameters:\n        counts (Sequence):\n            A count vector/list of shape (C, ).\n\n    Returns:\n        float:\n            The computed Simpson diversity index.\n    \"\"\"\n    N = np.sum(counts) + SMALL\n    return 1 - np.sum([(n / N) ** 2 for n in counts if n != 0])\n</code></pre>"},{"location":"api/spatial_agg/theil_between_group/","title":"theil_between_group","text":"<p>Compute the between group Theil index.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence</code> <p>The input value-vector. Shape (n, )</p> required <code>partition</code> <code>Sequence</code> <p>The groups for each x value. Shape (n, ).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed between group Theil index.</p> Source code in <code>src/histolytics/spatial_agg/diversity.py</code> <pre><code>def theil_between_group(x: Sequence, partition: Sequence) -&gt; float:\n    \"\"\"Compute the between group Theil index.\n\n    Parameters:\n        x (Sequence):\n            The input value-vector. Shape (n, )\n        partition (Sequence):\n            The groups for each x value. Shape (n, ).\n\n    Returns:\n        float:\n            The computed between group Theil index.\n    \"\"\"\n    groups = np.unique(partition)\n    x_total = x.sum(0) + SMALL\n\n    # group totals\n    g_total = np.array([x[partition == gid].sum(axis=0) for gid in groups])\n\n    if x_total.size == 1:  # y is 1-d\n        sg = g_total / (x_total * 1.0)\n        sg.shape = (sg.size, 1)\n    else:\n        sg = np.dot(g_total, np.diag(1.0 / x_total))\n\n    ng = np.array([np.sum(partition == gid) for gid in groups])\n    ng.shape = (ng.size,)  # ensure ng is 1-d\n    n = x.shape[0]\n\n    # between group inequality\n    sg = sg + (sg == 0)  # handle case when a partition has 0 for sum\n    bg = np.multiply(sg, np.log(np.dot(np.diag(n * 1.0 / ng), sg))).sum(axis=0)\n\n    return float(bg)\n</code></pre>"},{"location":"api/spatial_agg/theil_index/","title":"theil_index","text":"<p>Compute the gini coefficient of inequality for species.</p> Note <p>This is based on http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm.</p> <p>Gini-index: $$ G = \\frac{\\sum_{i=1}^n (2i - n - 1)x_i} {n \\sum_{i=1}^n x_i} $$</p> <p>where \\(x_i\\) is the count of species \\(i\\) and \\(n\\) is the total count of species.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence</code> <p>The input value-vector. Shape (n, )</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are negative input values.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed Gini coefficient.</p> Source code in <code>src/histolytics/spatial_agg/diversity.py</code> <pre><code>def gini_index(x: Sequence) -&gt; float:\n    \"\"\"Compute the gini coefficient of inequality for species.\n\n    Note:\n        This is based on\n        http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm.\n\n    **Gini-index:**\n    $$\n    G = \\\\frac{\\\\sum_{i=1}^n (2i - n - 1)x_i} {n \\\\sum_{i=1}^n x_i}\n    $$\n\n    where $x_i$ is the count of species $i$ and $n$ is the total count of species.\n\n    Parameters:\n        x (Sequence):\n            The input value-vector. Shape (n, )\n\n    Raises:\n        ValueError:\n            If there are negative input values.\n\n    Returns:\n        float:\n            The computed Gini coefficient.\n    \"\"\"\n    if np.min(x) &lt; 0:\n        raise ValueError(\"Input values need to be positive for Gini coeff\")\n\n    n = len(x)\n    s = np.sum(x)\n    nx = n * s + SMALL\n\n    rx = (2.0 * np.arange(1, n + 1) * x[np.argsort(x)]).sum()\n    return (rx - nx - s) / nx\n</code></pre>"},{"location":"api/spatial_agg/theil_within_group/","title":"theil_within_group","text":"<p>Compute the within group Theil index.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence</code> <p>The input value-vector. Shape (n, )</p> required <code>partition</code> <code>Sequence</code> <p>The groups for each x value. Shape (n, ).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed within group Theil index.</p> Source code in <code>src/histolytics/spatial_agg/diversity.py</code> <pre><code>def theil_within_group(x: Sequence, partition: Sequence) -&gt; float:\n    \"\"\"Compute the within group Theil index.\n\n    Parameters:\n        x (Sequence):\n            The input value-vector. Shape (n, )\n        partition (Sequence):\n            The groups for each x value. Shape (n, ).\n\n    Returns:\n        float:\n            The computed within group Theil index.\n    \"\"\"\n    theil = theil_index(x)\n    theil_bg = theil_between_group(x, partition)\n\n    return float(theil - theil_bg)\n</code></pre>"},{"location":"api/spatial_clust/cluster_area/","title":"cluster_area","text":"<p>Compute the area of a hull of a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the cluster data.</p> required <code>hull_type</code> <code>str</code> <p>The type of hull to compute. One of: \"alpha_shape\", \"convex_hull\", \"ellipse\".</p> <code>'alpha_shape'</code> <code>**kwargs</code> <p>Additional keyword arguments for the hull computation (e.g., <code>step</code> for alpha shape).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The area of the computed hull.</p> Source code in <code>src/histolytics/spatial_clust/clust_metrics.py</code> <pre><code>def cluster_area(\n    gdf: gpd.GeoDataFrame, hull_type: str = \"alpha_shape\", **kwargs\n) -&gt; float:\n    \"\"\"Compute the area of a hull of a GeoDataFrame.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the cluster data.\n        hull_type (str):\n            The type of hull to compute. One of: \"alpha_shape\", \"convex_hull\", \"ellipse\".\n        **kwargs:\n            Additional keyword arguments for the hull computation\n            (e.g., `step` for alpha shape).\n\n    Returns:\n        float: The area of the computed hull.\n    \"\"\"\n    allowed_hulls = [\"alpha_shape\", \"convex_hull\", \"ellipse\"]\n    if hull_type not in allowed_hulls:\n        raise ValueError(f\"Invalid hull type. Allowed values are: {allowed_hulls}\")\n\n    xy = get_centroid_numpy(gdf)\n    hull_poly = hull(xy, hull_type=hull_type, **kwargs)\n\n    return hull_poly.area\n</code></pre>"},{"location":"api/spatial_clust/cluster_dispersion/","title":"cluster_dispersion","text":"<p>Compute the dispersion of a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the cluster data.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The standard distance of the cluster.</p> Source code in <code>src/histolytics/spatial_clust/clust_metrics.py</code> <pre><code>def cluster_dispersion(gdf: gpd.GeoDataFrame) -&gt; float:\n    \"\"\"Compute the dispersion of a GeoDataFrame.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the cluster data.\n\n    Returns:\n        float: The standard distance of the cluster.\n    \"\"\"\n    xy = get_centroid_numpy(gdf)\n    return std_distance(xy)\n</code></pre>"},{"location":"api/spatial_clust/cluster_dists_to_tissue/","title":"Cluster dists to tissue","text":"<p>Calculate the distance of objects to a specific tissue class.</p> <p>Parameters:</p> Name Type Description Default <code>objs</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the objects (e.g., nuc) for which distances are calculated.</p> required <code>tissues</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing tissue polygons with a \"class_name\" column.</p> required <code>tissue_class</code> <code>str</code> <p>The class of tissue to which distances are calculated (e.g., \"tumor\").</p> required <p>Returns (gpd.GeoDataFrame):     The input <code>objs</code> GeoDataFrame with an additional column containing the minimum</p> Source code in <code>src/histolytics/spatial_clust/clust_metrics.py</code> <pre><code>def cluster_dists_to_tissue(\n    objs: gpd.GeoDataFrame, tissues: gpd.GeoDataFrame, tissue_class: str\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Calculate the distance of objects to a specific tissue class.\n\n    Parameters:\n        objs (gpd.GeoDataFrame):\n            A GeoDataFrame containing the objects (e.g., nuc) for which distances are\n            calculated.\n        tissues (gpd.GeoDataFrame):\n            A GeoDataFrame containing tissue polygons with a \"class_name\" column.\n        tissue_class (str):\n            The class of tissue to which distances are calculated (e.g., \"tumor\").\n\n    Returns (gpd.GeoDataFrame):\n        The input `objs` GeoDataFrame with an additional column containing the minimum\n    \"\"\"\n    tissue = tissues.loc[tissues[\"class_name\"] == tissue_class]\n\n    distances = {}\n    for i, poly in tissue.iterrows():\n        dist = objs.distance(poly.geometry)\n        distances[i] = dist\n\n    min_dists = pd.DataFrame(distances).min(axis=1)\n    min_dists.name = f\"dist_to_{tissue_class}\"\n\n    return objs.join(other=min_dists, how=\"left\")\n</code></pre>"},{"location":"api/spatial_clust/cluster_feats/","title":"cluster_feats","text":"<p>Compute centrography features of a cluster represented by a GeoDataFrame.</p> Note <p>Computes the following features:</p> <ul> <li><code>area</code>: The area of the cluster.</li> <li><code>dispersion</code>: The dispersion of the cluster.</li> <li><code>size</code>: The size of the cluster (number of objects).</li> <li><code>orientation</code>: The orientation angle of the cluster.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the cluster data.</p> required <code>hull_type</code> <code>str</code> <p>The type of hull to compute. One of: \"alpha_shape\", \"convex_hull\", \"ellipse\". The hull is used to compute the area and orientation of the cluster.</p> <code>'alpha_shape'</code> <code>normalize_orientation</code> <code>bool</code> <p>Whether to normalize the orientation angle to be within [0, 90].</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the hull computation (e.g., <code>step</code> for alpha shape).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary containing the computed features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from histolytics.spatial_clust.density_clustering import density_clustering\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.spatial_clust.clust_metrics import cluster_feats\n&gt;&gt;&gt;\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; nuc_imm = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; labels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\")\n&gt;&gt;&gt; nuc_imm = nuc_imm.assign(labels=labels)\n&gt;&gt;&gt; # Calculate cluster features for each cluster label\n&gt;&gt;&gt; clust_features = (\n...     nuc_imm.groupby(\"labels\")\n...     .apply(\n...         lambda x: cluster_feats(x, hull_type=\"convex_hull\", normalize_orientation=True),\n...         include_groups=False,\n...     )\n...     .reset_index(drop=False)\n... )\n&gt;&gt;&gt; print(clust_features)\n    labels           area  dispersion   size  orientation\n0      -1  732641.332024  483.830111   83.0    34.979649\n1       0  368383.654562  249.680419  205.0    81.664728\n</code></pre> Source code in <code>src/histolytics/spatial_clust/clust_metrics.py</code> <pre><code>def cluster_feats(\n    gdf: gpd.GeoDataFrame,\n    hull_type: str = \"alpha_shape\",\n    normalize_orientation: bool = True,\n    **kwargs,\n) -&gt; Dict[str, float]:\n    \"\"\"Compute centrography features of a cluster represented by a GeoDataFrame.\n\n    Note:\n        Computes the following features:\n\n        - `area`: The area of the cluster.\n        - `dispersion`: The dispersion of the cluster.\n        - `size`: The size of the cluster (number of objects).\n        - `orientation`: The orientation angle of the cluster.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the cluster data.\n        hull_type (str):\n            The type of hull to compute. One of: \"alpha_shape\", \"convex_hull\", \"ellipse\".\n            The hull is used to compute the area and orientation of the cluster.\n        normalize_orientation (bool):\n            Whether to normalize the orientation angle to be within [0, 90].\n        **kwargs (Any):\n            Additional keyword arguments for the hull computation\n            (e.g., `step` for alpha shape).\n\n    Returns:\n        Dict[str, float]:\n            A dictionary containing the computed features.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from histolytics.spatial_clust.density_clustering import density_clustering\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_clust.clust_metrics import cluster_feats\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; nuc_imm = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; labels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\")\n        &gt;&gt;&gt; nuc_imm = nuc_imm.assign(labels=labels)\n        &gt;&gt;&gt; # Calculate cluster features for each cluster label\n        &gt;&gt;&gt; clust_features = (\n        ...     nuc_imm.groupby(\"labels\")\n        ...     .apply(\n        ...         lambda x: cluster_feats(x, hull_type=\"convex_hull\", normalize_orientation=True),\n        ...         include_groups=False,\n        ...     )\n        ...     .reset_index(drop=False)\n        ... )\n        &gt;&gt;&gt; print(clust_features)\n            labels           area  dispersion   size  orientation\n        0      -1  732641.332024  483.830111   83.0    34.979649\n        1       0  368383.654562  249.680419  205.0    81.664728\n    \"\"\"\n    return pd.Series(\n        {\n            \"area\": cluster_area(gdf, hull_type=hull_type, **kwargs),\n            \"dispersion\": cluster_dispersion(gdf),\n            \"size\": cluster_size(gdf),\n            \"orientation\": cluster_orientation(\n                gdf, hull_type=hull_type, normalize=normalize_orientation, **kwargs\n            ),\n        }\n    )\n</code></pre>"},{"location":"api/spatial_clust/cluster_orientation/","title":"cluster_orientation","text":"<p>Compute the orientation angle of a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the cluster data.</p> required <code>hull_type</code> <code>str</code> <p>The type of hull to compute. One of: \"alpha_shape\", \"convex_hull\", \"ellipse\".</p> <code>'alpha_shape'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the angle to be within [0, 90].</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the hull computation (e.g., <code>step</code> for alpha shape).</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid hull type is provided.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The orientation angle of the cluster, in degrees.</p> Source code in <code>src/histolytics/spatial_clust/clust_metrics.py</code> <pre><code>def cluster_orientation(\n    gdf: gpd.GeoDataFrame,\n    hull_type: str = \"alpha_shape\",\n    normalize: bool = True,\n    **kwargs,\n) -&gt; float:\n    \"\"\"Compute the orientation angle of a GeoDataFrame.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the cluster data.\n        hull_type (str):\n            The type of hull to compute. One of: \"alpha_shape\", \"convex_hull\", \"ellipse\".\n        normalize (bool):\n            Whether to normalize the angle to be within [0, 90].\n        **kwargs:\n            Additional keyword arguments for the hull computation\n            (e.g., `step` for alpha shape).\n\n    Raises:\n        ValueError: If an invalid hull type is provided.\n\n    Returns:\n        float: The orientation angle of the cluster, in degrees.\n    \"\"\"\n    xy = get_centroid_numpy(gdf)\n    hull_poly = hull(xy, hull_type=hull_type, **kwargs)\n\n    return axis_angle(hull_poly, normalize=normalize)\n</code></pre>"},{"location":"api/spatial_clust/cluster_size/","title":"cluster_size","text":"<p>Compute the size of a GeoDataFrame cluster.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the cluster data.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The size of the cluster.</p> Source code in <code>src/histolytics/spatial_clust/clust_metrics.py</code> <pre><code>def cluster_size(gdf: gpd.GeoDataFrame) -&gt; float:\n    \"\"\"Compute the size of a GeoDataFrame cluster.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the cluster data.\n\n    Returns:\n        float: The size of the cluster.\n    \"\"\"\n    return len(gdf)\n</code></pre>"},{"location":"api/spatial_clust/cluster_tendency/","title":"cluster_tendency","text":"<p>Get the centroid of a GeoDataFrame using specified method.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame with a properly set geometry column.</p> required <code>centroid_method</code> <code>str</code> <p>The method to use for calculating the centroid. Options are: - \"mean\": Calculate the mean center of the centroids. - \"median\": Calculate the median center of the centroids. - \"weighted_mean\": Calculate the weighted mean center of the centroids.</p> <code>'mean'</code> <code>weight_col</code> <code>str</code> <p>The name of the column to use for weights when calculating the weighted mean center. Required if <code>centroid_method</code> is \"weighted_mean\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Point</code> <code>Point</code> <p>A shapely Point object representing the centroid of the GeoDataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_clust.centrography import cluster_tendency\n&gt;&gt;&gt; from histolytics.spatial_clust.density_clustering import density_clustering\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt;\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; nuc_imm = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; labels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\")\n&gt;&gt;&gt; nuc_imm = nuc_imm.assign(labels=labels)\n&gt;&gt;&gt; # Calculate the centroids for each cluster\n&gt;&gt;&gt; clust_centroids = (\n...    nuc_imm.groupby(\"labels\")\n...    .apply(lambda g: cluster_tendency(g, \"mean\"), include_groups=False)\n...    .reset_index(name=\"centroid\")\n... )\n&gt;&gt;&gt; print(clust_centroids)\n    labels                                     centroid\n    0      -1  POINT (785.5438556958384 806.2601606856466)\n    1       0  POINT (665.1678800342971 695.4346142894398)\n</code></pre> Source code in <code>src/histolytics/spatial_clust/centrography.py</code> <pre><code>def cluster_tendency(\n    gdf: gpd.GeoDataFrame, centroid_method: str = \"mean\", weight_col: str = None\n) -&gt; Point:\n    \"\"\"Get the centroid of a GeoDataFrame using specified method.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            Input GeoDataFrame with a properly set geometry column.\n        centroid_method (str):\n            The method to use for calculating the centroid. Options are:\n            - \"mean\": Calculate the mean center of the centroids.\n            - \"median\": Calculate the median center of the centroids.\n            - \"weighted_mean\": Calculate the weighted mean center of the centroids.\n        weight_col (str, optional):\n            The name of the column to use for weights when calculating the weighted mean\n            center. Required if `centroid_method` is \"weighted_mean\".\n\n    Returns:\n        Point:\n            A shapely Point object representing the centroid of the GeoDataFrame.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_clust.centrography import cluster_tendency\n        &gt;&gt;&gt; from histolytics.spatial_clust.density_clustering import density_clustering\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; nuc_imm = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; labels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\")\n        &gt;&gt;&gt; nuc_imm = nuc_imm.assign(labels=labels)\n        &gt;&gt;&gt; # Calculate the centroids for each cluster\n        &gt;&gt;&gt; clust_centroids = (\n        ...    nuc_imm.groupby(\"labels\")\n        ...    .apply(lambda g: cluster_tendency(g, \"mean\"), include_groups=False)\n        ...    .reset_index(name=\"centroid\")\n        ... )\n        &gt;&gt;&gt; print(clust_centroids)\n            labels                                     centroid\n            0      -1  POINT (785.5438556958384 806.2601606856466)\n            1       0  POINT (665.1678800342971 695.4346142894398)\n\n    \"\"\"\n    allowed_centroid_methods = [\"mean\", \"median\", \"weighted_mean\"]\n    if centroid_method not in allowed_centroid_methods:\n        raise ValueError(\n            f\"centroid_method must be one of {allowed_centroid_methods}, \"\n            f\"got {centroid_method}.\"\n        )\n\n    xy = get_centroid_numpy(gdf)\n    if centroid_method == \"mean\":\n        centroid = mean_center(xy)\n    elif centroid_method == \"median\":\n        centroid = median_center(xy)\n    elif centroid_method == \"weighted_mean\":\n        centroid = weighted_mean_center(xy, weight_col=weight_col)\n\n    # Convert centroid coordinates to shapely Point\n    centroid_point = Point(centroid)\n    return centroid_point\n</code></pre>"},{"location":"api/spatial_clust/density_clustering/","title":"density_clustering","text":"<p>Apply a density based clustering to centroids in a gdf.</p> Note <p>This is a wrapper for a scikit-learn density clustering algorithms adapted to geodataframes.</p> Note <p>Allowed clustering methods are:</p> <ul> <li><code>dbscan</code> (sklearn.cluster.DBSCAN)</li> <li><code>hdbscan</code> (sklearn.cluster.HDBSCAN)</li> <li><code>optics</code> (sklearn.cluster.OPTICS)</li> <li><code>adbscan</code> (esda.adbscan.ADBSCAN)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input geo dataframe with a properly set geometry column.</p> required <code>eps</code> <code>float</code> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of gdf within a cluster.</p> <code>350.0</code> <code>min_samples</code> <code>int</code> <p>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</p> <code>30</code> <code>method</code> <code>str</code> <p>The clustering method to be used. Allowed: (\"dbscan\", \"adbscan\", \"optics\").</p> <code>'dbscan'</code> <code>num_processes</code> <code>int</code> <p>The number of parallel processes. None means 1. -1 means using all processors.</p> <code>1</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Arbitrary key-word arguments passed to the clustering methods.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If illegal method is given or input <code>gdf</code> is of wrong type.</p> <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray</code> <p>An array of cluster labels for each centroid in the gdf. Noise points are labeled as -1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from histolytics.spatial_clust.density_clustering import density_clustering\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt;\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; nuc_imm = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; labels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\")\n&gt;&gt;&gt; print(labels)\n[-1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  0 ...\n</code></pre> Source code in <code>src/histolytics/spatial_clust/density_clustering.py</code> <pre><code>def density_clustering(\n    gdf: gpd.GeoDataFrame,\n    eps: float = 350.0,\n    min_samples: int = 30,\n    method: str = \"dbscan\",\n    num_processes: int = 1,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Apply a density based clustering to centroids in a gdf.\n\n    Note:\n        This is a wrapper for a scikit-learn density clustering algorithms\n        adapted to geodataframes.\n\n    Note:\n        Allowed clustering methods are:\n\n        - `dbscan` (sklearn.cluster.DBSCAN)\n        - `hdbscan` (sklearn.cluster.HDBSCAN)\n        - `optics` (sklearn.cluster.OPTICS)\n        - `adbscan` (esda.adbscan.ADBSCAN)\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            Input geo dataframe with a properly set geometry column.\n        eps (float):\n            The maximum distance between two samples for one to be considered as in the\n            neighborhood of the other. This is not a maximum bound on the distances of\n            gdf within a cluster.\n        min_samples (int):\n            The number of samples (or total weight) in a neighborhood for a point to be\n            considered as a core point. This includes the point itself.\n        method (str):\n            The clustering method to be used. Allowed: (\"dbscan\", \"adbscan\", \"optics\").\n        num_processes (int):\n            The number of parallel processes. None means 1. -1 means using all\n            processors.\n        **kwargs (Dict[str, Any]):\n            Arbitrary key-word arguments passed to the clustering methods.\n\n    Raises:\n        ValueError:\n            If illegal method is given or input `gdf` is of wrong type.\n\n    Returns:\n        labels (np.ndarray):\n            An array of cluster labels for each centroid in the gdf. Noise points are\n            labeled as -1.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from histolytics.spatial_clust.density_clustering import density_clustering\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; nuc_imm = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; labels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\")\n        &gt;&gt;&gt; print(labels)\n        [-1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  0 ...\n    \"\"\"\n    allowed = (\"dbscan\", \"adbscan\", \"optics\", \"hdbscan\")\n    if method not in allowed:\n        raise ValueError(\n            f\"Illegal clustering method was given. Got: {method}, allowed: {allowed}\"\n        )\n\n    xy = get_centroid_numpy(gdf)\n\n    if method == \"adbscan\":\n        xy = pd.DataFrame({\"X\": xy[:, 0], \"Y\": xy[:, 1]})\n        clusterer = ADBSCAN(\n            eps=eps, min_samples=min_samples, n_jobs=num_processes, **kwargs\n        )\n    elif method == \"dbscan\":\n        clusterer = DBSCAN(\n            eps=eps, min_samples=min_samples, n_jobs=num_processes, **kwargs\n        )\n    elif method == \"hdbscan\":\n        clusterer = HDBSCAN(min_samples=min_samples, n_jobs=num_processes, **kwargs)\n    elif method == \"optics\":\n        clusterer = OPTICS(\n            max_eps=eps, min_samples=min_samples, n_jobs=num_processes, **kwargs\n        )\n\n    labels = clusterer.fit(xy).labels_.astype(int)\n\n    return labels\n</code></pre>"},{"location":"api/spatial_clust/global_autocorr/","title":"global_autocorr","text":"<p>Run global spatial autocorrelation for a GeoDataFrame.</p> Note <p>This is a wrapper function for the <code>esda.Moran</code> from <code>esda</code> package, returning only the relevant data: Moran's I statistic, expected value, variance, and p-value.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the spatial data.</p> required <code>w</code> <code>W</code> <p>The spatial weights object.</p> required <code>feat</code> <code>str</code> <p>The feature column to analyze.</p> required <code>permutations</code> <code>int</code> <p>Number of random permutations for calculation of pseudo p_values.</p> <code>999</code> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float, float, float]: A tuple containing: - I: Global Moran's I statistic. - p_sim: P-value under the null hypothesis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.spatial_clust.autocorr import global_autocorr\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt; # Load the HGSC cancer nuclei dataset\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; neo = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; neo = set_uid(neo)\n&gt;&gt;&gt; neo = shape_metric(neo, [\"area\"])\n&gt;&gt;&gt; # Fit a spatial graph to the neoplastic nuclei\n&gt;&gt;&gt; w, _ = fit_graph(neo, \"distband\", threshold=100)\n&gt;&gt;&gt; # Calculate local Moran's I for the area feature\n&gt;&gt;&gt; pval, moran_i = global_autocorr(neo, w, feat=\"area\")\n&gt;&gt;&gt; print(pval, moran_i)\n    0.00834165971467421 0.318\n</code></pre> Source code in <code>src/histolytics/spatial_clust/autocorr.py</code> <pre><code>def global_autocorr(\n    gdf: gpd.GeoDataFrame,\n    w: libpysal.weights.W,\n    feat: str,\n    permutations: int = 999,\n    num_processes: int = 1,\n) -&gt; Tuple[float, float]:\n    \"\"\"Run global spatial autocorrelation for a GeoDataFrame.\n\n    Note:\n        This is a wrapper function for the `esda.Moran` from `esda` package,\n        returning only the relevant data: Moran's I statistic, expected value,\n        variance, and p-value.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the spatial data.\n        w (libpysal.weights.W):\n            The spatial weights object.\n        feat (str):\n            The feature column to analyze.\n        permutations (int):\n            Number of random permutations for calculation of pseudo p_values.\n\n    Returns:\n        Tuple[float, float, float, float]:\n            A tuple containing:\n            - I: Global Moran's I statistic.\n            - p_sim: P-value under the null hypothesis.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_clust.autocorr import global_autocorr\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt; # Load the HGSC cancer nuclei dataset\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; neo = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; neo = set_uid(neo)\n        &gt;&gt;&gt; neo = shape_metric(neo, [\"area\"])\n        &gt;&gt;&gt; # Fit a spatial graph to the neoplastic nuclei\n        &gt;&gt;&gt; w, _ = fit_graph(neo, \"distband\", threshold=100)\n        &gt;&gt;&gt; # Calculate local Moran's I for the area feature\n        &gt;&gt;&gt; pval, moran_i = global_autocorr(neo, w, feat=\"area\")\n        &gt;&gt;&gt; print(pval, moran_i)\n            0.00834165971467421 0.318\n    \"\"\"\n    moran = esda.Moran(\n        gdf[feat],\n        w,\n        permutations=permutations,\n    )\n\n    return moran.I, moran.p_sim\n</code></pre>"},{"location":"api/spatial_clust/lisa_clustering/","title":"lisa_clustering","text":"<p>Perform Local Indicators of Spatial Association (LISA) clustering.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the data.</p> required <code>spatial_weights</code> <code>W</code> <p>The spatial weights object.</p> required <code>feat</code> <code>str</code> <p>The feature to use for clustering.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>permutations</code> <code>int</code> <p>Number of permutations for significance testing.</p> <code>100</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The cluster labels for each object in the GeoDataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_clust.lisa_clustering import lisa_clustering\n&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt;\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; # Fit distband graph to the neoplastic nuclei\n&gt;&gt;&gt; w, _ = fit_graph(nuc, \"distband\", threshold=100)\n&gt;&gt;&gt; # Compute the nuclei areas\n&gt;&gt;&gt; nuc = shape_metric(nuc, [\"area\"])\n&gt;&gt;&gt; # Perform LISA clustering on the area feature\n&gt;&gt;&gt; labels = lisa_clustering(nuc, w, feat=\"area\", seed=4, permutations=999)\n&gt;&gt;&gt; print(labels)\n    array(['LL', 'ns', 'LL', ..., 'ns', 'ns', 'ns'], dtype='&lt;U2')\n</code></pre> Source code in <code>src/histolytics/spatial_clust/lisa_clustering.py</code> <pre><code>def lisa_clustering(\n    gdf: gpd.GeoDataFrame,\n    spatial_weights: W,\n    feat: str,\n    seed: int = 42,\n    permutations: int = 100,\n) -&gt; np.ndarray:\n    \"\"\"Perform Local Indicators of Spatial Association (LISA) clustering.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the data.\n        spatial_weights (W):\n            The spatial weights object.\n        feat (str):\n            The feature to use for clustering.\n        seed (int):\n            Random seed for reproducibility.\n        permutations (int):\n            Number of permutations for significance testing.\n\n    Returns:\n        np.ndarray:\n            The cluster labels for each object in the GeoDataFrame.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_clust.lisa_clustering import lisa_clustering\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; # Fit distband graph to the neoplastic nuclei\n        &gt;&gt;&gt; w, _ = fit_graph(nuc, \"distband\", threshold=100)\n        &gt;&gt;&gt; # Compute the nuclei areas\n        &gt;&gt;&gt; nuc = shape_metric(nuc, [\"area\"])\n        &gt;&gt;&gt; # Perform LISA clustering on the area feature\n        &gt;&gt;&gt; labels = lisa_clustering(nuc, w, feat=\"area\", seed=4, permutations=999)\n        &gt;&gt;&gt; print(labels)\n            array(['LL', 'ns', 'LL', ..., 'ns', 'ns', 'ns'], dtype='&lt;U2')\n    \"\"\"\n    lisa = esda.Moran_Local(\n        gdf[feat],\n        spatial_weights,\n        island_weight=np.nan,\n        seed=seed,\n        permutations=permutations,\n    )\n\n    # Classify the gdf to HH, LL, LH, HL\n    clusters = moran_hot_cold_spots(lisa)\n\n    cluster_labels = [\"ns\", \"HH\", \"LH\", \"LL\", \"HL\"]\n    lisa_labels = np.array([cluster_labels[i] for i in clusters])\n\n    return lisa_labels\n</code></pre>"},{"location":"api/spatial_clust/local_autocorr/","title":"local_autocorr","text":"<p>Run local spatial autocorrelation for a GeoDataFrame.</p> Note <p>This is a wrapper function for the <code>esda.Moran_Local</code> from <code>esda</code> package, returning only the relevant data: p-values, local Moran's I, and quadrant places.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing the spatial data.</p> required <code>w</code> <code>W</code> <p>The spatial weights object.</p> required <code>feat</code> <code>str</code> <p>The feature column to analyze.</p> required <code>permutations</code> <code>int</code> <p>number of random permutations for calculation of pseudo p_values.</p> <code>999</code> <code>num_processes</code> <code>int</code> <p>Number of cores to be used in the conditional randomisation. If -1, all available cores are used.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing: - p_sim: Array of pseudo p-values for each feature. - Is: Array of local Moran's I values. - q: Array of quadrant places for each feature.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.spatial_clust.autocorr import local_autocorr\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt; # Load the HGSC cancer nuclei dataset\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; neo = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; neo = set_uid(neo)\n&gt;&gt;&gt; neo = shape_metric(neo, [\"area\"])\n&gt;&gt;&gt; # Fit a spatial graph to the neoplastic nuclei\n&gt;&gt;&gt; w, _ = fit_graph(neo, \"distband\", threshold=100)\n&gt;&gt;&gt; # Calculate local Moran's I for the area feature\n&gt;&gt;&gt; pval, moran_i, quadrants = local_autocorr(neo, w, feat=\"area\")\n&gt;&gt;&gt; print(moran_i)\n    [ 0.32793505  0.07546211 -0.00902539 -0.05775879  0.13344124  0.57178879 ...\n</code></pre> Source code in <code>src/histolytics/spatial_clust/autocorr.py</code> <pre><code>def local_autocorr(\n    gdf: gpd.GeoDataFrame,\n    w: libpysal.weights.W,\n    feat: str,\n    permutations: int = 999,\n    num_processes: int = 1,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Run local spatial autocorrelation for a GeoDataFrame.\n\n    Note:\n        This is a wrapper function for the `esda.Moran_Local` from `esda` package,\n        returning only the relevant data: p-values, local Moran's I, and quadrant places.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing the spatial data.\n        w (libpysal.weights.W):\n            The spatial weights object.\n        feat (str):\n            The feature column to analyze.\n        permutations (int):\n            number of random permutations for calculation of pseudo p_values.\n        num_processes (int):\n            Number of cores to be used in the conditional randomisation.\n            If -1, all available cores are used.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n            A tuple containing:\n            - p_sim: Array of pseudo p-values for each feature.\n            - Is: Array of local Moran's I values.\n            - q: Array of quadrant places for each feature.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_clust.autocorr import local_autocorr\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt; # Load the HGSC cancer nuclei dataset\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; neo = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; neo = set_uid(neo)\n        &gt;&gt;&gt; neo = shape_metric(neo, [\"area\"])\n        &gt;&gt;&gt; # Fit a spatial graph to the neoplastic nuclei\n        &gt;&gt;&gt; w, _ = fit_graph(neo, \"distband\", threshold=100)\n        &gt;&gt;&gt; # Calculate local Moran's I for the area feature\n        &gt;&gt;&gt; pval, moran_i, quadrants = local_autocorr(neo, w, feat=\"area\")\n        &gt;&gt;&gt; print(moran_i)\n            [ 0.32793505  0.07546211 -0.00902539 -0.05775879  0.13344124  0.57178879 ...\n    \"\"\"\n    moran = esda.Moran_Local(\n        gdf[feat],\n        w,\n        island_weight=np.nan,\n        permutations=permutations,\n        n_jobs=num_processes,\n    )\n\n    return moran.p_sim, moran.Is, moran.q\n</code></pre>"},{"location":"api/spatial_clust/mean_center/","title":"mean_center","text":"<p>Calculate the mean center of a GeoDataFrame containing nuclei.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> <code>ndarray</code> <p>A numpy array of shape (n, 2) containing the x and y coordinates. (centroids of the nuclei).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The mean center of the centroids as a numpy array with shape (2,).</p> Source code in <code>src/histolytics/spatial_clust/centrography.py</code> <pre><code>def mean_center(xy: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the mean center of a GeoDataFrame containing nuclei.\n\n    Parameters:\n        xy (np.ndarray):\n            A numpy array of shape (n, 2) containing the x and y coordinates.\n            (centroids of the nuclei).\n\n    Returns:\n        np.ndarray:\n            The mean center of the centroids as a numpy array with shape (2,).\n    \"\"\"\n    return xy.mean(axis=0)\n</code></pre>"},{"location":"api/spatial_clust/median_center/","title":"median_center","text":"<p>Calculate the manhattan median center of a GeoDataFrame containing nuclei.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> <code>ndarray</code> <p>A numpy array of shape (n, 2) containing the x and y coordinates. (centroids of the nuclei).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The median center of the centroids as a numpy array with shape (2,).</p> Source code in <code>src/histolytics/spatial_clust/centrography.py</code> <pre><code>def median_center(xy: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the manhattan median center of a GeoDataFrame containing nuclei.\n\n    Parameters:\n        xy (np.ndarray):\n            A numpy array of shape (n, 2) containing the x and y coordinates.\n            (centroids of the nuclei).\n\n    Returns:\n        np.ndarray:\n            The median center of the centroids as a numpy array with shape (2,).\n    \"\"\"\n    return xy.median(axis=0)\n</code></pre>"},{"location":"api/spatial_clust/ripley_test/","title":"ripley_test","text":"<p>Run a Ripley alphabet test on a GeoDataFrame.</p> <p>Simulates a random poisson point process and computes the Ripley alphabet function values for both the observed pattern and the simulated patterns.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the segmented objects.</p> required <code>distances</code> <code>ndarray</code> <p>An array of distances at which to compute the Ripley alphabet function.</p> required <code>ripley_alphabet</code> <code>str</code> <p>The Ripley alphabet statistic to compute. Must be one of \"k\", \"g\", or \"l\".</p> <code>'g'</code> <code>n_sim</code> <code>int</code> <p>The number of simulations to perform for the random point process.</p> <code>100</code> <code>hull_type</code> <code>str</code> <p>The type of hull to use for the Ripley test. Options are \"convex_hull\", \"alpha_shape\", \"ellipse\", or \"bbox\".</p> <code>'bbox'</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray]</code> <p>tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing: - ripley_stat: The observed Ripley alphabet function values. - sims: An array of simulated Ripley alphabet function values. - pvalues: An array of p-values for the observed Ripley alphabet function values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.spatial_clust.ripley import ripley_test\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load the HGSC cancer nuclei dataset\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; neo = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; distances = np.linspace(0, 100, 10)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run the Ripley G test for the neoplastic nuclei\n&gt;&gt;&gt; ripley_stat, sims, pvalues = ripley_test(\n...     neo,\n...     distances=distances,\n...     ripley_alphabet=\"g\",\n...     n_sim=100,\n...     hull_type=\"bbox\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(pvalues)\n    [0.         0.         0.         0.00990099 0.00990099 0.01980198\n    0.04950495 0.0990099  0.17821782 0.        ]\n</code></pre> Source code in <code>src/histolytics/spatial_clust/ripley.py</code> <pre><code>def ripley_test(\n    gdf: gpd.GeoDataFrame,\n    distances: np.ndarray,\n    ripley_alphabet: str = \"g\",\n    n_sim: int = 100,\n    hull_type: str = \"bbox\",\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Run a Ripley alphabet test on a GeoDataFrame.\n\n    Simulates a random poisson point process and computes the Ripley alphabet function\n    values for both the observed pattern and the simulated patterns.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            A GeoDataFrame containing the segmented objects.\n        distances (np.ndarray):\n            An array of distances at which to compute the Ripley alphabet function.\n        ripley_alphabet (str):\n            The Ripley alphabet statistic to compute. Must be one of \"k\", \"g\", or \"l\".\n        n_sim (int):\n            The number of simulations to perform for the random point process.\n        hull_type (str):\n            The type of hull to use for the Ripley test. Options are \"convex_hull\",\n            \"alpha_shape\", \"ellipse\", or \"bbox\".\n\n    Returns:\n        tuple[np.ndarray, np.ndarray, np.ndarray]:\n            A tuple containing:\n            - ripley_stat: The observed Ripley alphabet function values.\n            - sims: An array of simulated Ripley alphabet function values.\n            - pvalues: An array of p-values for the observed Ripley alphabet function values.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_clust.ripley import ripley_test\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load the HGSC cancer nuclei dataset\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; neo = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; distances = np.linspace(0, 100, 10)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run the Ripley G test for the neoplastic nuclei\n        &gt;&gt;&gt; ripley_stat, sims, pvalues = ripley_test(\n        ...     neo,\n        ...     distances=distances,\n        ...     ripley_alphabet=\"g\",\n        ...     n_sim=100,\n        ...     hull_type=\"bbox\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; print(pvalues)\n            [0.         0.         0.         0.00990099 0.00990099 0.01980198\n            0.04950495 0.0990099  0.17821782 0.        ]\n\n    \"\"\"\n    coords = get_centroid_numpy(gdf)\n    n_obs = len(coords)\n    _hull = hull(coords, hull_type)\n\n    # compute the observed Ripley alphabet function values\n    ripley_stat = RIPLEY_ALPHABET[ripley_alphabet](\n        coords, support=distances, hull_poly=_hull, dist_metric=\"euclidean\"\n    )\n\n    # simulate the Ripley alphabet function values for the random point process\n    sims = np.empty((len(ripley_stat), n_sim)).T\n    pvalues = np.ones_like(ripley_stat, dtype=float)\n    for i_repl in range(n_sim):\n        random_i = poisson(coords, n_obs=n_obs, hull_poly=_hull)\n        ripley_sim_i = RIPLEY_ALPHABET[ripley_alphabet](\n            random_i, support=distances, hull_poly=_hull, dist_metric=\"euclidean\"\n        )\n        sims[i_repl] = ripley_sim_i\n        pvalues += ripley_sim_i &gt;= ripley_stat\n\n    pvalues /= n_sim + 1\n    pvalues = np.minimum(pvalues, 1 - pvalues)\n\n    return ripley_stat, sims, pvalues\n</code></pre>"},{"location":"api/spatial_clust/std_distance/","title":"std_distance","text":"<p>Calculate the std_distance of xy-coords in a GeoDataFrame.</p> <p>The std_distance is defined as the square root of the variance of the distances of the xy-coords from their mean center.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> <code>ndarray</code> <p>A numpy array of shape (n, 2) containing the x and y coordinates. (centroids of the nuclei).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The std_distance of the xy-coords as a float value.</p> Source code in <code>src/histolytics/spatial_clust/centrography.py</code> <pre><code>def std_distance(xy: np.ndarray) -&gt; float:\n    \"\"\"Calculate the std_distance of xy-coords in a GeoDataFrame.\n\n    The std_distance is defined as the square root of the variance of the distances of\n    the xy-coords from their mean center.\n\n    Parameters:\n        xy (np.ndarray):\n            A numpy array of shape (n, 2) containing the x and y coordinates.\n            (centroids of the nuclei).\n\n    Returns:\n        float:\n            The std_distance of the xy-coords as a float value.\n    \"\"\"\n    n, p = xy.shape\n    m = mean_center(xy)\n    return np.sqrt(((xy * xy).sum(axis=0) / n - m * m).sum())\n</code></pre>"},{"location":"api/spatial_clust/weighted_mean_center/","title":"weighted_mean_center","text":"<p>Calculate the weighted mean center of a GeoDataFrame containing nuclei.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> <code>ndarray</code> <p>A numpy array of shape (n, 2) containing the x and y coordinates. (centroids of the nuclei).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The weighted mean center of the centroids as a numpy array with shape (2,).</p> Source code in <code>src/histolytics/spatial_clust/centrography.py</code> <pre><code>def weighted_mean_center(xy: np.ndarray, weights: Sequence) -&gt; np.ndarray:\n    \"\"\"Calculate the weighted mean center of a GeoDataFrame containing nuclei.\n\n    Parameters:\n        xy (np.ndarray):\n            A numpy array of shape (n, 2) containing the x and y coordinates.\n            (centroids of the nuclei).\n\n    Returns:\n        np.ndarray:\n            The weighted mean center of the centroids as a numpy array with shape (2,).\n    \"\"\"\n    points, weights = np.asarray(xy), np.asarray(weights)\n    w = weights * 1.0 / weights.sum()\n    w.shape = (1, len(points))\n    return np.dot(w, points)[0]\n</code></pre>"},{"location":"api/spatial_geom/average_turning_angle/","title":"average_turning_angle","text":"<p>Compute the average turning angle of a line.</p> <p>The average turning angle measures the mean absolute angular change between consecutive segments of a line. A straight line has an average turning angle of 0, while a line with many sharp turns has a higher value.</p> Note <p>This function calculates the absolute angle between consecutive segments using the arctan2 method, which gives angles in the range [0, 180] degrees.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>Union[LineString, MultiLineString]</code> <p>Input shapely LineString or MultiLineString object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>Optional[float]</code> <p>The average turning angle in degrees. Returns None if the line has fewer than 3 points (insufficient to calculate any angles).</p> Source code in <code>src/histolytics/spatial_geom/line_metrics.py</code> <pre><code>def average_turning_angle(line: Union[LineString, MultiLineString]) -&gt; Optional[float]:\n    \"\"\"Compute the average turning angle of a line.\n\n    The average turning angle measures the mean absolute angular change between\n    consecutive segments of a line. A straight line has an average turning angle\n    of 0, while a line with many sharp turns has a higher value.\n\n    Note:\n        This function calculates the absolute angle between consecutive segments\n        using the arctan2 method, which gives angles in the range [0, 180] degrees.\n\n    Parameters:\n        line (Union[LineString, MultiLineString]):\n            Input shapely LineString or MultiLineString object.\n\n    Returns:\n        float:\n            The average turning angle in degrees. Returns None if the line has\n            fewer than 3 points (insufficient to calculate any angles).\n    \"\"\"\n    if isinstance(line, LineString):\n        # Extract coordinates\n        coords = list(line.coords)\n\n        # Need at least 3 points to calculate angles\n        if len(coords) &lt; 3:\n            return None\n\n        # Calculate angles between consecutive segments\n        angles = []\n        for i in range(len(coords) - 2):\n            # Get three consecutive points\n            p1 = coords[i]\n            p2 = coords[i + 1]\n            p3 = coords[i + 2]\n\n            # Calculate direction angles of the two segments using arctan2\n            # arctan2 gives angle in radians with respect to the positive x-axis\n            angle1 = np.arctan2(p2[1] - p1[1], p2[0] - p1[0])\n            angle2 = np.arctan2(p3[1] - p2[1], p3[0] - p2[0])\n\n            # Calculate the absolute difference in angles (turning angle)\n            # This handles the circular nature of angles\n            diff = np.abs(angle2 - angle1)\n            if diff &gt; np.pi:\n                diff = 2 * np.pi - diff\n\n            # Convert to degrees\n            angle_deg = np.degrees(diff)\n            angles.append(angle_deg)\n\n        # Return the average turning angle\n        return np.mean(angles) if angles else None\n\n    elif isinstance(line, MultiLineString):\n        # Combine all angles from individual LineStrings\n        all_angles = []\n\n        for geom in line.geoms:\n            # Process each LineString\n            coords = list(geom.coords)\n\n            if len(coords) &lt; 3:\n                continue\n\n            for i in range(len(coords) - 2):\n                p1 = coords[i]\n                p2 = coords[i + 1]\n                p3 = coords[i + 2]\n\n                angle1 = np.arctan2(p2[1] - p1[1], p2[0] - p1[0])\n                angle2 = np.arctan2(p3[1] - p2[1], p3[0] - p2[0])\n\n                diff = np.abs(angle2 - angle1)\n                if diff &gt; np.pi:\n                    diff = 2 * np.pi - diff\n\n                angle_deg = np.degrees(diff)\n                all_angles.append(angle_deg)\n\n        # Return the average turning angle across all LineStrings\n        return np.mean(all_angles) if all_angles else None\n\n    else:\n        return None\n</code></pre>"},{"location":"api/spatial_geom/circularity/","title":"circularity","text":"<p>Compute the circularity of a polygon.</p> Note <p>Circularity (sometimes roundness) is the ratio of the area of an object to the area of a circle with the same convex perimeter. Circularity equals 1 for a circular object and less than 1 for non-circular objects. Note that circularity is insensitive to irregular boundaries. - Wirth</p> <p>Circularity: $$ \\frac{4 \\times \\pi A_{poly}}{P_{convex}^2} $$</p> <p>where \\(A_{poly}\\) is the area of the polygon and \\(P_{convex}\\) is the perimeter of the convex hull.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The circularity value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def circularity(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the circularity of a polygon.\n\n    Note:\n        Circularity (sometimes roundness) is the ratio of the area of\n        an object to the area of a circle with the same convex perimeter.\n        Circularity equals 1 for a circular object and less than 1 for\n        non-circular objects. Note that circularity is insensitive to\n        irregular boundaries.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    **Circularity:**\n    $$\n    \\\\frac{4 \\\\times \\\\pi A_{poly}}{P_{convex}^2}\n    $$\n\n    where $A_{poly}$ is the area of the polygon and $P_{convex}$ is the perimeter of\n    the convex hull.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The circularity value of a polygon between 0-1.\n    \"\"\"\n    convex_perimeter = polygon.convex_hull.length\n    area = polygon.area\n\n    circularity = (4 * np.pi * area) / convex_perimeter**2\n\n    return circularity\n</code></pre>"},{"location":"api/spatial_geom/compactness/","title":"compactness","text":"<p>Compute the compactness of a polygon.</p> Note <p>Compactness is defined as the ratio of the area of an object to the area of a circle with the same perimeter. A circle is the most compact shape. Objects that are elliptical or have complicated, irregular (not smooth) boundaries have larger compactness. - Wirth</p> <p>Compactness: $$ \\frac{4\\pi A_{poly}}{P_{poly}^2} $$</p> <p>where \\(A_{poly}\\) is the area of the polygon and \\(P_{poly}\\) is the perimeter of the polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The compactness value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def compactness(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the compactness of a polygon.\n\n    Note:\n        Compactness is defined as the ratio of the area of an object\n        to the area of a circle with the same perimeter. A circle is the\n        most compact shape. Objects that are elliptical or have complicated,\n        irregular (not smooth) boundaries have larger compactness.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    **Compactness:**\n    $$\n    \\\\frac{4\\\\pi A_{poly}}{P_{poly}^2}\n    $$\n\n    where $A_{poly}$ is the area of the polygon and $P_{poly}$ is the perimeter of\n    the polygon.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The compactness value of a polygon between 0-1.\n    \"\"\"\n    perimeter = polygon.length\n    area = polygon.area\n\n    compactness = (4 * np.pi * area) / perimeter**2\n\n    return compactness\n</code></pre>"},{"location":"api/spatial_geom/convexity/","title":"convexity","text":"<p>Compute the convexity of a polygon.</p> Note <p>Convexity is the relative amount that an object differs from a convex object. Convexity is defined by computing the ratio of the perimeter of an object's convex hull to the perimeter of the object itself. This will take the value of 1 for a convex object, and will be less than 1 if the object is not convex, such as one having an irregular boundary. - Wirth</p> <p>Convexity: $$ \\frac{P_{convex}}{P_{poly}} $$</p> <p>where \\(P_{convex}\\) is the perimeter of the convex hull and \\(P_{poly}\\) is the perimeter of the polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The convexity value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def convexity(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the convexity of a polygon.\n\n    Note:\n        Convexity is the relative amount that an object differs from a\n        convex object. Convexity is defined by computing the ratio of\n        the perimeter of an object's convex hull to the perimeter of\n        the object itself. This will take the value of 1 for a convex\n        object, and will be less than 1 if the object is not convex, such\n        as one having an irregular boundary.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    **Convexity:**\n    $$\n    \\\\frac{P_{convex}}{P_{poly}}\n    $$\n\n    where $P_{convex}$ is the perimeter of the convex hull and $P_{poly}$ is the\n    perimeter of the polygon.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The convexity value of a polygon between 0-1.\n    \"\"\"\n    convex_perimeter = polygon.convex_hull.length\n    perimeter = polygon.length\n\n    convexity = convex_perimeter / perimeter\n\n    return convexity\n</code></pre>"},{"location":"api/spatial_geom/eccentricity/","title":"eccentricity","text":"<p>Compute the eccentricity of a polygon.</p> Note <p>Eccentricity (sometimes ellipticity) measures how far the object is from an ellipse. It is defined as the ratio of the length of the minor axis to the length of the major axis of an object. The closer the object is to an ellipse, the closer the eccentricity is to 1 - Wirth</p> <p>Eccentricity: $$ \\sqrt{1 - \\frac{\\text{minor axis}^2}{\\text{major axis}^2}} $$</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The eccentricity value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def eccentricity(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the eccentricity of a polygon.\n\n    Note:\n        Eccentricity (sometimes ellipticity) measures how far the object is\n        from an ellipse. It is defined as the ratio of the length of the minor\n        axis to the length of the major axis of an object. The closer the\n        object is to an ellipse, the closer the eccentricity is to 1\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    **Eccentricity:**\n    $$\n    \\\\sqrt{1 - \\\\frac{\\\\text{minor axis}^2}{\\\\text{major axis}^2}}\n    $$\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The eccentricity value of a polygon between 0-1.\n    \"\"\"\n    major_ax = axis_len(polygon, \"major\")\n    minor_ax = axis_len(polygon, \"minor\")\n    eccentricity = np.sqrt(1 - (minor_ax**2 / major_ax**2))\n    return eccentricity\n</code></pre>"},{"location":"api/spatial_geom/elongation/","title":"elongation","text":"<p>Compute the elongation of a polygon.</p> Note <p>Elongation is the ratio between the length and width of the object bounding box. If the ratio is equal to 1, the object is roughly square or circularly shaped. As the ratio decreases from 1, the object becomes more elongated. - Wirth</p> <p>Elongation: $$ \\frac{\\text{bbox width}}{\\text{bbox height}} $$</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The elongation value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def elongation(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the elongation of a polygon.\n\n    Note:\n        Elongation is the ratio between the length and width of the\n        object bounding box. If the ratio is equal to 1, the object\n        is roughly square or circularly shaped. As the ratio decreases\n        from 1, the object becomes more elongated.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    **Elongation:**\n    $$\n    \\\\frac{\\\\text{bbox width}}{\\\\text{bbox height}}\n    $$\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The elongation value of a polygon between 0-1.\n    \"\"\"\n    minx, miny, maxx, maxy = polygon.bounds\n\n    width = maxx - minx\n    height = maxy - miny\n\n    if width &lt;= height:\n        elongation = width / height\n    else:\n        elongation = height / width\n\n    return elongation\n</code></pre>"},{"location":"api/spatial_geom/equivalent_rectangular_index/","title":"equivalent_rectangular_index","text":"<p>Compute the equivalent rectangular index.</p> Note <p>Equivalent rectangluar index is the deviation of a polygon from an equivalent rectangle.</p> <p>ERI: $$ \\frac{\\sqrt{A_{poly}}}{A_{MRR}} \\times \\frac{P_{MRR}}{P_{poly}} $$</p> <p>where \\(A_{poly}\\) is the area of the polygon, \\(A_{MRR}\\) is the area of the minimum rotated rectangle, \\(P_{MRR}\\) is the perimeter of the minimum rotated rectangle and \\(P_{poly}\\) is the perimeter of the polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The ERI value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def equivalent_rectangular_index(polygon: Polygon) -&gt; float:\n    \"\"\"Compute the equivalent rectangular index.\n\n    Note:\n        Equivalent rectangluar index is the deviation of a polygon from\n        an equivalent rectangle.\n\n    **ERI:**\n    $$\n    \\\\frac{\\\\sqrt{A_{poly}}}{A_{MRR}}\n    \\\\times\n    \\\\frac{P_{MRR}}{P_{poly}}\n    $$\n\n    where $A_{poly}$ is the area of the polygon, $A_{MRR}$ is the area of the\n    minimum rotated rectangle, $P_{MRR}$ is the perimeter of the minimum rotated\n    rectangle and $P_{poly}$ is the perimeter of the polygon.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The ERI value of a polygon between 0-1.\n    \"\"\"\n    mrr = polygon.minimum_rotated_rectangle\n\n    return np.sqrt(polygon.area / mrr.area) / (mrr.length / polygon.length)\n</code></pre>"},{"location":"api/spatial_geom/fractal_dimension/","title":"fractal_dimension","text":"<p>Compute the fractal dimension of a polygon.</p> Note <p>The fractal dimension is the rate at which the perimeter of an object increases as the measurement scale is reduced. The fractal dimension produces a single numeric value that summarizes the irregularity of \"roughness\" of the feature boundary. - Wirth</p> <p>Fractal dimension: $$ 2 \\times \\frac{\\log(\\frac{P_{poly}}{4})}{\\log(A_{poly})} $$</p> <p>where \\(P_{poly}\\) is the perimeter of the polygon and \\(A_{poly}\\) is the area of the polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The fractal dimension value of a polygon.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def fractal_dimension(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the fractal dimension of a polygon.\n\n    Note:\n        The fractal dimension is the rate at which the perimeter of an\n        object increases as the measurement scale is reduced. The fractal\n        dimension produces a single numeric value that summarizes the\n        irregularity of \"roughness\" of the feature boundary.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n\n    **Fractal dimension:**\n    $$\n    2 \\\\times \\\\frac{\\\\log(\\\\frac{P_{poly}}{4})}{\\\\log(A_{poly})}\n    $$\n\n    where $P_{poly}$ is the perimeter of the polygon and $A_{poly}$ is the area of the\n    polygon.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The fractal dimension value of a polygon.\n    \"\"\"\n    perimeter = polygon.length\n    area = polygon.area\n\n    return (2 * np.log(perimeter / 4)) / np.log(area)\n</code></pre>"},{"location":"api/spatial_geom/hull/","title":"hull","text":"<p>Compute a geometric hull around a set of 2D points.</p> <p>Parameters:</p> Name Type Description Default <code>xy</code> <code>ndarray</code> <p>An array of shape (N, 2) representing N 2D points.</p> required <code>hull_type</code> <code>str</code> <p>The type of hull to compute. Must be one of \"alpha_shape\", \"convex_hull\", \"bbox\", or \"ellipse\".</p> <code>'alpha_shape'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the underlying hull computation functions (e.g., parameters for alpha shape).</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid hull_type is provided.</p> <p>Returns:</p> Name Type Description <code>Polygon</code> <code>Polygon</code> <p>A shapely Polygon object representing the computed hull.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hull(points, hull_type=\"convex_hull\")\n&lt;shapely.geometry.polygon.Polygon object at ...&gt;\n</code></pre> Source code in <code>src/histolytics/spatial_geom/hull.py</code> <pre><code>def hull(xy: np.ndarray, hull_type: str = \"alpha_shape\", **kwargs: Any) -&gt; Polygon:\n    \"\"\"Compute a geometric hull around a set of 2D points.\n\n    Parameters:\n        xy (np.ndarray):\n            An array of shape (N, 2) representing N 2D points.\n        hull_type (str):\n            The type of hull to compute. Must be one of\n            \"alpha_shape\", \"convex_hull\", \"bbox\", or \"ellipse\".\n        **kwargs (Any):\n            Additional keyword arguments passed to the underlying hull computation\n            functions (e.g., parameters for alpha shape).\n\n    Raises:\n        ValueError: If an invalid hull_type is provided.\n\n    Returns:\n        Polygon: A shapely Polygon object representing the computed hull.\n\n    Examples:\n        &gt;&gt;&gt; hull(points, hull_type=\"convex_hull\")\n        &lt;shapely.geometry.polygon.Polygon object at ...&gt;\n    \"\"\"\n    allowed_hulls = [\"alpha_shape\", \"convex_hull\", \"ellipse\", \"bbox\"]\n    if hull_type not in allowed_hulls:\n        raise ValueError(f\"Invalid hull type. Allowed values are: {allowed_hulls}\")\n\n    if hull_type == \"alpha_shape\":\n        hull_poly = alpha_shape_auto(xy, **kwargs)\n    elif hull_type == \"convex_hull\":\n        hull = ConvexHull(xy)\n        hull_points = xy[hull.vertices]\n        hull_poly = Polygon(hull_points)\n    elif hull_type == \"ellipse\":\n        hull_poly = ellipse_poly(xy)\n    elif hull_type == \"bbox\":\n        poly = Polygon(xy)\n        hull_poly = poly.envelope\n\n    return hull_poly\n</code></pre>"},{"location":"api/spatial_geom/line_metrics/","title":"line_metric","text":"<p>Compute a set of line metrics for every row of the gdf.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The input GeoDataFrame.</p> required <code>metrics</code> <code>Tuple[str, ...]</code> <p>A Tuple/List of line metrics.</p> required <code>normalize</code> <code>bool</code> <p>Flag whether to column (quantile) normalize the computed metrics or not.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Flag whether to use parallel apply operations when computing the diversities.</p> <code>True</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use when parallel=True. If -1, this will use all available cores.</p> <code>1</code> <code>col_prefix</code> <code>str</code> <p>Prefix for the new column names.</p> <code>None</code> <code>create_copy</code> <code>bool</code> <p>Flag whether to create a copy of the input gdf or not.</p> <code>True</code> Note <p>Allowed shape metrics are:</p> <ul> <li><code>tortuosity</code></li> <li><code>average_turning_angle</code></li> <li><code>length</code></li> <li><code>major_axis_len</code></li> <li><code>minor_axis_len</code></li> <li><code>major_axis_angle</code></li> <li><code>minor_axis_angle</code></li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an illegal metric is given.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The input geodataframe with computed shape metric columns added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from shapely.geometry import LineString\n&gt;&gt;&gt; from histolytics.spatial_geom.line_metrics import line_metric\n&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; lines = [\n...     LineString([(i, i) for i in range(12)]),\n...     LineString([(i, 0 if i % 2 == 0 else 1) for i in range(12)]),\n...     LineString([(0, i) for i in range(12)]),\n&gt;&gt;&gt; ]\n&gt;&gt;&gt; gdf = gpd.GeoDataFrame(geometry=lines)\n&gt;&gt;&gt; gdf = line_metric(gdf, metrics=[\"tortuosity\", \"length\"], parallel=True)\n&gt;&gt;&gt; print(gdf.head(3))\n                                                geometry  tortuosity     length\n    0  LINESTRING (0 0, 1 1, 2 2, 3 3, 4 4, 5 5, 6 6,...    1.000000  15.556349\n    1  LINESTRING (0 0, 1 1, 2 0, 3 1, 4 0, 5 1, 6 0,...    1.408406  15.556349\n    2  LINESTRING (0 0, 0 1, 0 2, 0 3, 0 4, 0 5, 0 6,...    1.000000  11.000000\n</code></pre> Source code in <code>src/histolytics/spatial_geom/line_metrics.py</code> <pre><code>def line_metric(\n    gdf: gpd.GeoDataFrame,\n    metrics: Tuple[str, ...],\n    normalize: bool = False,\n    parallel: bool = True,\n    num_processes: int = 1,\n    col_prefix: str = None,\n    create_copy: bool = True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute a set of line metrics for every row of the gdf.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The input GeoDataFrame.\n        metrics (Tuple[str, ...]):\n            A Tuple/List of line metrics.\n        normalize (bool):\n            Flag whether to column (quantile) normalize the computed metrics or not.\n        parallel (bool):\n            Flag whether to use parallel apply operations when computing the diversities.\n        num_processes (int):\n            The number of processes to use when parallel=True. If -1,\n            this will use all available cores.\n        col_prefix (str):\n            Prefix for the new column names.\n        create_copy (bool):\n            Flag whether to create a copy of the input gdf or not.\n\n    Note:\n        Allowed shape metrics are:\n\n        - `tortuosity`\n        - `average_turning_angle`\n        - `length`\n        - `major_axis_len`\n        - `minor_axis_len`\n        - `major_axis_angle`\n        - `minor_axis_angle`\n\n    Raises:\n        ValueError:\n            If an illegal metric is given.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The input geodataframe with computed shape metric columns added.\n\n    Examples:\n        &gt;&gt;&gt; from shapely.geometry import LineString\n        &gt;&gt;&gt; from histolytics.spatial_geom.line_metrics import line_metric\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; lines = [\n        ...     LineString([(i, i) for i in range(12)]),\n        ...     LineString([(i, 0 if i % 2 == 0 else 1) for i in range(12)]),\n        ...     LineString([(0, i) for i in range(12)]),\n        &gt;&gt;&gt; ]\n        &gt;&gt;&gt; gdf = gpd.GeoDataFrame(geometry=lines)\n        &gt;&gt;&gt; gdf = line_metric(gdf, metrics=[\"tortuosity\", \"length\"], parallel=True)\n        &gt;&gt;&gt; print(gdf.head(3))\n                                                        geometry  tortuosity     length\n            0  LINESTRING (0 0, 1 1, 2 2, 3 3, 4 4, 5 5, 6 6,...    1.000000  15.556349\n            1  LINESTRING (0 0, 1 1, 2 0, 3 1, 4 0, 5 1, 6 0,...    1.408406  15.556349\n            2  LINESTRING (0 0, 0 1, 0 2, 0 3, 0 4, 0 5, 0 6,...    1.000000  11.000000\n\n    \"\"\"\n    if not isinstance(metrics, (list, tuple)):\n        raise ValueError(f\"`metrics` must be a list or tuple. Got: {type(metrics)}.\")\n\n    allowed = list(LINE_SHAPE_LOOKUP.keys())\n    if not all(m in allowed for m in metrics):\n        raise ValueError(\n            f\"Illegal metric in `metrics`. Got: {metrics}. Allowed metrics: {allowed}.\"\n        )\n\n    if create_copy:\n        gdf = gdf.copy()\n\n    if col_prefix is None:\n        col_prefix = \"\"\n    else:\n        col_prefix += \"_\"\n\n    met = list(metrics)\n    if \"length\" in metrics:\n        gdf[f\"{col_prefix}length\"] = gdf.length\n        if normalize:\n            gdf[f\"{col_prefix}length\"] = col_norm(gdf[f\"{col_prefix}length\"])\n        met.remove(\"length\")\n\n    for metric in met:\n        gdf[f\"{col_prefix}{metric}\"] = gdf_apply(\n            gdf,\n            LINE_SHAPE_LOOKUP[metric],\n            columns=[\"geometry\"],\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n        if normalize:\n            gdf[f\"{col_prefix}{metric}\"] = col_norm(gdf[f\"{col_prefix}{metric}\"])\n\n    return gdf\n</code></pre>"},{"location":"api/spatial_geom/major_axis_angle/","title":"major_axis_angle","text":"<p>Compute the major axis angle of a geometry.</p> Note <p>The major axis is the (x,y) endpoints of the longest line that can be drawn through the object. Major axis angle is the angle of the major axis with respect to the x-axis. - Wirth</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>BaseGeometry</code> <p>Input shapely geometry object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The angle of the major axis in degrees.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def major_axis_angle(geom: BaseGeometry) -&gt; float:\n    \"\"\"Compute the major axis angle of a geometry.\n\n    Note:\n        The major axis is the (x,y) endpoints of the longest line that\n        can be drawn through the object. Major axis angle is the angle of\n        the major axis with respect to the x-axis.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    Parameters:\n        geom (BaseGeometry):\n            Input shapely geometry object.\n\n    Returns:\n        float:\n            The angle of the major axis in degrees.\n    \"\"\"\n    return axis_angle(geom, \"major\")\n</code></pre>"},{"location":"api/spatial_geom/major_axis_len/","title":"major_axis_len","text":"<p>Compute the major axis length of a geometry.</p> Note <p>The major axis is the (x,y) endpoints of the longest line that can be drawn through the object. Major axis length is the pixel distance between the major-axis endpoints - Wirth</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The length of the major axis.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def major_axis_len(geom: Polygon) -&gt; float:\n    \"\"\"Compute the major axis length of a geometry.\n\n    Note:\n        The major axis is the (x,y) endpoints of the longest line that\n        can be drawn through the object. Major axis length is the pixel\n        distance between the major-axis endpoints\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    Parameters:\n        geom (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The length of the major axis.\n    \"\"\"\n    return axis_len(geom, \"major\")\n</code></pre>"},{"location":"api/spatial_geom/medial_lines/","title":"medial_lines","text":"<p>Compute medial lines for the input GeoDataFrame polygon geometries.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing polygons to compute medial lines for.</p> required <code>num_points</code> <code>int</code> <p>Number of resampled points in the input polygons.</p> <code>500</code> <code>delta</code> <code>float</code> <p>Distance between resampled polygon points. Ignored if <code>num_points</code> is not None.</p> <code>0.3</code> <code>simplify_level</code> <code>float</code> <p>Level of simplification to apply to the input geometries before computing medial lines. This helps to reduce noise from the voronoi triangulation.</p> <code>30.0</code> <code>parallel</code> <code>bool</code> <p>Whether to run the computation in parallel.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>Number of processes to use for parallel computation.</p> <code>1</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: GeoDataFrame containing the computed medial lines.</p> Note <p>Returns an empty GeoDataFrame if the input is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_geom.medial_lines import medial_lines\n&gt;&gt;&gt; from histolytics.data import cervix_tissue\n&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a simple polygon\n&gt;&gt;&gt; cervix_tis = cervix_tissue()\n&gt;&gt;&gt; lesion = cervix_tis[cervix_tis[\"class_name\"] == \"cin\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute medial lines for the largest lesion segmentation\n&gt;&gt;&gt; medials = medial_lines(lesion, num_points=500, simplify_level=50)\n&gt;&gt;&gt; ax = cervix_tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n&gt;&gt;&gt; medials.plot(ax=ax, color=\"red\", lw=1, alpha=0.5)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_geom/medial_lines.py</code> <pre><code>def medial_lines(\n    gdf: gpd.GeoDataFrame,\n    num_points: int = 500,\n    delta: float = 0.3,\n    simplify_level: float = 30.0,\n    parallel: bool = False,\n    num_processes: int = 1,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute medial lines for the input GeoDataFrame polygon geometries.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            GeoDataFrame containing polygons to compute medial lines for.\n        num_points (int):\n            Number of resampled points in the input polygons.\n        delta (float):\n            Distance between resampled polygon points. Ignored\n            if `num_points` is not None.\n        simplify_level (float):\n            Level of simplification to apply to the input geometries before computing\n            medial lines. This helps to reduce noise from the voronoi triangulation.\n        parallel (bool):\n            Whether to run the computation in parallel.\n        num_processes (int):\n            Number of processes to use for parallel computation.\n\n    Returns:\n        gpd.GeoDataFrame:\n            GeoDataFrame containing the computed medial lines.\n\n    Note:\n        Returns an empty GeoDataFrame if the input is empty.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_geom.medial_lines import medial_lines\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create a simple polygon\n        &gt;&gt;&gt; cervix_tis = cervix_tissue()\n        &gt;&gt;&gt; lesion = cervix_tis[cervix_tis[\"class_name\"] == \"cin\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute medial lines for the largest lesion segmentation\n        &gt;&gt;&gt; medials = medial_lines(lesion, num_points=500, simplify_level=50)\n        &gt;&gt;&gt; ax = cervix_tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n        &gt;&gt;&gt; medials.plot(ax=ax, color=\"red\", lw=1, alpha=0.5)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/medial_lines.png)\n    \"\"\"\n    if gdf.empty:\n        return gpd.GeoDataFrame(columns=[\"geometry\", \"class_name\"])\n\n    # Explode multipolygons if present\n    if \"MultiPolygon\" in gdf.geometry.geom_type.unique():\n        gdf = gdf.explode(index_parts=False, ignore_index=True)\n\n    gdf = gdf.assign(geometry=gdf[\"geometry\"].simplify(simplify_level))\n\n    medials = gdf_apply(\n        gdf,\n        partial(_compute_medial_line, num_points=num_points, delta=delta),\n        columns=[\"geometry\"],\n        parallel=parallel,\n        num_processes=num_processes,\n    )\n\n    ret = gpd.GeoDataFrame(geometry=medials)\n    ret.set_crs(gdf.crs, inplace=True)\n    ret[\"class_name\"] = \"medial\"\n\n    return ret\n</code></pre>"},{"location":"api/spatial_geom/minor_axis_angle/","title":"minor_axis_angle","text":"<p>Compute the minor axis angle of a geometry.</p> Note <p>The minor axis is the (x,y) endpoints of the longest line that can be drawn through the object whilst remaining perpendicular with the major-axis. Minor axis angle is the angle of the minor axis with respect to the x-axis. - Wirth</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>BaseGeometry</code> <p>Input shapely geometry object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The angle of the minor axis in degrees.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def minor_axis_angle(geom: BaseGeometry) -&gt; float:\n    \"\"\"Compute the minor axis angle of a geometry.\n\n    Note:\n        The minor axis is the (x,y) endpoints of the longest line that\n        can be drawn through the object whilst remaining perpendicular\n        with the major-axis. Minor axis angle is the angle of the minor\n        axis with respect to the x-axis.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    Parameters:\n        geom (BaseGeometry):\n            Input shapely geometry object.\n\n    Returns:\n        float:\n            The angle of the minor axis in **degrees**.\n    \"\"\"\n    return axis_angle(geom, \"minor\")\n</code></pre>"},{"location":"api/spatial_geom/minor_axis_len/","title":"minor_axis_len","text":"<p>Compute the minor axis length of a geometry.</p> Note <p>The minor axis is the (x,y) endpoints of the longest line that can be drawn through the object whilst remaining perpendicular with the major-axis. Minor axis length is the pixel distance between the minor-axis endpoints - Wirth</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Basegeometry</code> <p>Input shapely geometry object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The length of the minor axis.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def minor_axis_len(geom: BaseGeometry) -&gt; float:\n    \"\"\"Compute the minor axis length of a geometry.\n\n    Note:\n        The minor axis is the (x,y) endpoints of the longest line that\n        can be drawn through the object whilst remaining perpendicular\n        with the major-axis. Minor axis length is the pixel distance\n        between the minor-axis endpoints\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    Parameters:\n        geom (Basegeometry):\n            Input shapely geometry object.\n\n    Returns:\n        float:\n            The length of the minor axis.\n    \"\"\"\n    return axis_len(geom, \"minor\")\n</code></pre>"},{"location":"api/spatial_geom/rectangularity/","title":"rectangularity","text":"<p>Compute the rectangularity of a polygon.</p> Note <p>Rectangularity is the ratio of the object to the area of the minimum bounding rectangle. Rectangularity has a value of 1 for perfectly rectangular object.</p> <p>Rectangularity: $$ \\frac{A_{poly}}{A_{MRR}} $$</p> <p>where \\(A_{poly}\\) is the area of the polygon and \\(A_{MRR}\\) is the area of the minimum rotated rectangle.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The rectangularity value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def rectangularity(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the rectangularity of a polygon.\n\n    Note:\n        Rectangularity is the ratio of the object to the area of the\n        minimum bounding rectangle. Rectangularity has a value of 1\n        for perfectly rectangular object.\n\n    **Rectangularity:**\n    $$\n    \\\\frac{A_{poly}}{A_{MRR}}\n    $$\n\n    where $A_{poly}$ is the area of the polygon and $A_{MRR}$ is the area of the\n    minimum rotated rectangle.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The rectangularity value of a polygon between 0-1.\n    \"\"\"\n    mrr = polygon.minimum_rotated_rectangle\n\n    return polygon.area / mrr.area\n</code></pre>"},{"location":"api/spatial_geom/shape_index/","title":"shape_index","text":"<p>Compute the shape index of a polygon.</p> Note <p>Basically, the inverse of circularity.</p> <p>Shape Index: $$ \\frac{\\sqrt{\\frac{A_{poly}}{\\pi}}}{\\text{MBR}} $$</p> <p>where \\(A_{poly}\\) is the area of the polygon and \\(\\text{MBR}\\) is the radius of the minimum bounding radius.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The shape index value of a polygon.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def shape_index(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the shape index of a polygon.\n\n    Note:\n        Basically, the inverse of circularity.\n\n    **Shape Index:**\n    $$\n    \\\\frac{\\\\sqrt{\\\\frac{A_{poly}}{\\\\pi}}}{\\\\text{MBR}}\n    $$\n\n    where $A_{poly}$ is the area of the polygon and $\\\\text{MBR}$ is the radius of the\n    minimum bounding radius.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The shape index value of a polygon.\n    \"\"\"\n    r = shapely.minimum_bounding_radius(polygon)\n    area = polygon.area\n\n    return np.sqrt(area / np.pi) / r\n</code></pre>"},{"location":"api/spatial_geom/shape_metrics/","title":"shape_metric","text":"<p>Compute a set of shape metrics for every row of the gdf.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The input GeoDataFrame.</p> required <code>metrics</code> <code>Tuple[str, ...]</code> <p>A Tuple/List of shape metrics.</p> required <code>normalize</code> <code>bool</code> <p>Flag whether to column (quantile) normalize the computed metrics or not.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Flag whether to use parallel apply operations when computing the diversities.</p> <code>True</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use when parallel=True. If -1, this will use all available cores.</p> <code>1</code> <code>col_prefix</code> <code>str</code> <p>Prefix for the new column names.</p> <code>None</code> <code>create_copy</code> <code>bool</code> <p>Flag whether to create a copy of the input gdf or not.</p> <code>True</code> Note <p>Allowed shape metrics are:</p> <ul> <li><code>area</code></li> <li><code>perimeter</code></li> <li><code>major_axis_len</code></li> <li><code>minor_axis_len</code></li> <li><code>major_axis_angle</code></li> <li><code>minor_axis_angle</code></li> <li><code>compactness</code></li> <li><code>circularity</code></li> <li><code>convexity</code></li> <li><code>solidity</code></li> <li><code>elongation</code></li> <li><code>eccentricity</code></li> <li><code>fractal_dimension</code></li> <li><code>shape_index</code></li> <li><code>rectangularity</code></li> <li><code>squareness</code></li> <li><code>equivalent_rectangular_index</code></li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an illegal metric is given.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The input geodataframe with computed shape metric columns added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # input data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; nuc = set_uid(nuc)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Calculate shape metrics\n&gt;&gt;&gt; nuc = shape_metric(nuc, metrics=[\"eccentricity\", \"solidity\"])\n&gt;&gt;&gt; print(nuc.head(3))\n            geometry        class_name  uid              uid\n    0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n    1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n    2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n        eccentricity  solidity\n    uid\n    0        0.960195  0.989154\n    1        0.041712  1.000000\n    2        0.610266  0.996911\n</code></pre> Source code in <code>src/histolytics/spatial_geom/shape_metrics.py</code> <pre><code>def shape_metric(\n    gdf: gpd.GeoDataFrame,\n    metrics: Tuple[str, ...],\n    normalize: bool = False,\n    parallel: bool = True,\n    num_processes: int = 1,\n    col_prefix: str = None,\n    create_copy: bool = True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute a set of shape metrics for every row of the gdf.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The input GeoDataFrame.\n        metrics (Tuple[str, ...]):\n            A Tuple/List of shape metrics.\n        normalize (bool):\n            Flag whether to column (quantile) normalize the computed metrics or not.\n        parallel (bool):\n            Flag whether to use parallel apply operations when computing the diversities.\n        num_processes (int):\n            The number of processes to use when parallel=True. If -1,\n            this will use all available cores.\n        col_prefix (str):\n            Prefix for the new column names.\n        create_copy (bool):\n            Flag whether to create a copy of the input gdf or not.\n\n    Note:\n        Allowed shape metrics are:\n\n        - `area`\n        - `perimeter`\n        - `major_axis_len`\n        - `minor_axis_len`\n        - `major_axis_angle`\n        - `minor_axis_angle`\n        - `compactness`\n        - `circularity`\n        - `convexity`\n        - `solidity`\n        - `elongation`\n        - `eccentricity`\n        - `fractal_dimension`\n        - `shape_index`\n        - `rectangularity`\n        - `squareness`\n        - `equivalent_rectangular_index`\n\n    Raises:\n        ValueError:\n            If an illegal metric is given.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The input geodataframe with computed shape metric columns added.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # input data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; nuc = set_uid(nuc)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Calculate shape metrics\n        &gt;&gt;&gt; nuc = shape_metric(nuc, metrics=[\"eccentricity\", \"solidity\"])\n        &gt;&gt;&gt; print(nuc.head(3))\n                    geometry        class_name  uid  \\\n            uid\n            0    POLYGON ((940.01 5570.02, 939.01 5573, 939 559...        connective    0\n            1    POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...        connective    1\n            2    POLYGON ((866 5137.02, 862.77 5137.94, 860 513...  squamous_epithel    2\n                eccentricity  solidity\n            uid\n            0        0.960195  0.989154\n            1        0.041712  1.000000\n            2        0.610266  0.996911\n    \"\"\"\n    if not isinstance(metrics, (list, tuple)):\n        raise ValueError(f\"`metrics` must be a list or tuple. Got: {type(metrics)}.\")\n\n    allowed = list(SHAPE_LOOKUP.keys())\n    if not all(m in allowed for m in metrics):\n        raise ValueError(\n            f\"Illegal metric in `metrics`. Got: {metrics}. Allowed metrics: {allowed}.\"\n        )\n\n    if create_copy:\n        gdf = gdf.copy()\n\n    if col_prefix is None:\n        col_prefix = \"\"\n    else:\n        col_prefix += \"_\"\n\n    met = list(metrics)\n    if \"area\" in metrics:\n        gdf[f\"{col_prefix}area\"] = gdf.area\n        if normalize:\n            gdf[f\"{col_prefix}length\"] = col_norm(gdf[f\"{col_prefix}length\"])\n        met.remove(\"area\")\n\n    if \"perimeter\" in metrics:\n        gdf[f\"{col_prefix}perimeter\"] = gdf.length\n        if normalize:\n            gdf[f\"{col_prefix}perimeter\"] = col_norm(gdf[f\"{col_prefix}perimeter\"])\n        met.remove(\"perimeter\")\n\n    for metric in met:\n        gdf[f\"{col_prefix}{metric}\"] = gdf_apply(\n            gdf,\n            SHAPE_LOOKUP[metric],\n            columns=[\"geometry\"],\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n        if normalize:\n            gdf[f\"{col_prefix}{metric}\"] = col_norm(gdf[f\"{col_prefix}{metric}\"])\n\n    return gdf\n</code></pre>"},{"location":"api/spatial_geom/solidity/","title":"solidity","text":"<p>Compute the solidity of a polygon.</p> Note <p>Solidity measures the density of an object. It is defined as the ratio of the area of an object to the area of a convex hull of the object. A value of 1 signifies a solid object, and a value less than 1 will signify an object having an irregular boundary, or containing holes. - Wirth</p> <p>Solidity: $$ \\frac{A_{poly}}{A_{convex}} $$</p> <p>where \\(A_{poly}\\) is the area of the polygon and \\(A_{convex}\\) is the area of the convex hull.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The solidity value of a polygon between 0-1.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def solidity(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the solidity of a polygon.\n\n    Note:\n        Solidity measures the density of an object. It is defined as the\n        ratio of the area of an object to the area of a convex hull of the\n        object. A value of 1 signifies a solid object, and a value less than\n        1 will signify an object having an irregular boundary, or containing\n        holes.\n        - [Wirth](http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf)\n\n    **Solidity:**\n    $$\n    \\\\frac{A_{poly}}{A_{convex}}\n    $$\n\n    where $A_{poly}$ is the area of the polygon and $A_{convex}$ is the area of the\n    convex hull.\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The solidity value of a polygon between 0-1.\n    \"\"\"\n    convex_area = polygon.convex_hull.area\n    area = polygon.area\n\n    return area / convex_area\n</code></pre>"},{"location":"api/spatial_geom/squareness/","title":"squareness","text":"<p>Compute the squareness of a polygon.</p> Note <p>Squareness is a measure of how close an object is to a square.</p> <p>Squareness: $$ \\left(\\frac{4*\\sqrt{A_{poly}}}{P_{poly}}\\right)^2 $$</p> <p>where \\(A_{poly}\\) is the area of the polygon and \\(P_{poly}\\) is the perimeter of the polygon.</p> Note <p>For irregular shapes, squareness is close to zero and for circular shapes close to 1.3. For squares, equals 1</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>Input shapely polygon object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The squareness value of a polygon.</p> Source code in <code>src/histolytics/spatial_geom/morphometrics.py</code> <pre><code>def squareness(polygon: Polygon, **kwargs) -&gt; float:\n    \"\"\"Compute the squareness of a polygon.\n\n    Note:\n        Squareness is a measure of how close an object is to a square.\n\n    **Squareness:**\n    $$\n    \\\\left(\\\\frac{4*\\\\sqrt{A_{poly}}}{P_{poly}}\\\\right)^2\n    $$\n\n    where $A_{poly}$ is the area of the polygon and $P_{poly}$ is the perimeter of\n    the polygon.\n\n    Note:\n        For irregular shapes, squareness is close to zero and for circular shapes close\n        to 1.3. For squares, equals 1\n\n    Parameters:\n        polygon (Polygon):\n            Input shapely polygon object.\n\n    Returns:\n        float:\n            The squareness value of a polygon.\n    \"\"\"\n    area = polygon.area\n    perimeter = polygon.length\n\n    return ((np.sqrt(area) * 4) / perimeter) ** 2\n</code></pre>"},{"location":"api/spatial_geom/tortuosity/","title":"tortuosity","text":"<p>Compute the tortuosity of a line.</p> Note <p>Defined as the ratio of the actual path length to the straight-line (Euclidean) distance between its endpoints. Tortuosity is a measure of how convoluted or winding a line is. A perfectly straight line has a tortuosity of 1, while more winding lines have higher values.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>Union[LineString, MultiLineString]</code> <p>Input shapely LineString or MultiLineString object.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The tortuosity value of a line.</p> Source code in <code>src/histolytics/spatial_geom/line_metrics.py</code> <pre><code>def tortuosity(line: Union[LineString, MultiLineString]) -&gt; float:\n    \"\"\"Compute the tortuosity of a line.\n\n    Note:\n        Defined as the ratio of the actual path length to the straight-line (Euclidean)\n        distance between its endpoints. Tortuosity is a measure of how convoluted or\n        winding a line is. A perfectly straight line has a tortuosity of 1, while more\n        winding lines have higher values.\n\n    Parameters:\n        line (Union[LineString, MultiLineString]):\n            Input shapely LineString or MultiLineString object.\n\n    Returns:\n        float:\n            The tortuosity value of a line.\n    \"\"\"\n    if isinstance(line, LineString):\n        path_length = line.length\n        euclidean_distance = LineString([line.coords[0], line.coords[-1]]).length\n        return path_length / euclidean_distance if euclidean_distance &gt; 0 else None\n    elif isinstance(line, MultiLineString):\n        path_length = line.length\n        # Find the correct start and end points by sorting the coordinates\n        coords = [list(geom.coords) for geom in line.geoms]\n        coords = [item for sublist in coords for item in sublist]\n        sorted_coords = sorted(coords, key=lambda x: (x[0], x[1]))\n        start_point = sorted_coords[0]\n        end_point = sorted_coords[-1]\n        euclidean_distance = LineString([start_point, end_point]).length\n        return path_length / euclidean_distance if euclidean_distance &gt; 0 else None\n    else:\n        return None\n</code></pre>"},{"location":"api/spatial_graph/connected_components/","title":"connected_components","text":"<p>Get the connected components of a spatial weights object.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>W</code> <p>The spatial weights object.</p> required <code>ids</code> <code>ndarray</code> <p>The ids of the nodes in the weights object.</p> required <p>Returns:</p> Name Type Description <code>sub_graphs</code> <code>List[W]</code> <p>The connected components of the graph.</p> Source code in <code>src/histolytics/spatial_graph/utils.py</code> <pre><code>def get_connected_components(w: W, ids: np.ndarray) -&gt; List[W]:\n    \"\"\"Get the connected components of a spatial weights object.\n\n    Parameters:\n        w (W):\n            The spatial weights object.\n        ids (np.ndarray):\n            The ids of the nodes in the weights object.\n\n    Returns:\n        sub_graphs (List[W]):\n            The connected components of the graph.\n    \"\"\"\n    w_sub = w_subset(w, ids, silence_warnings=True)\n\n    G = w_sub.to_networkx()\n    sub_graphs = [\n        W(nx.to_dict_of_lists(G.subgraph(c).copy()), silence_warnings=True)\n        for c in nx.connected_components(G)\n    ]\n\n    return sub_graphs\n</code></pre>"},{"location":"api/spatial_graph/graph/","title":"fit_graph","text":"<p>Fit a spatial graph to a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The input GeoDataFrame with spatial data.</p> required <code>method</code> <code>str</code> <p>Type of spatial graph to fit. Options are: \"delaunay\", \"knn\", \"rel_nhood\", \"distband\", \"gabriel\", \"voronoi\".</p> required <code>id_col</code> <code>str</code> <p>Column name for unique identifiers in the GeoDataFrame.</p> <code>'uid'</code> <code>threshold</code> <code>int</code> <p>Distance threshold (in pixels) for distance-based graphs.</p> <code>100</code> <code>use_polars</code> <code>bool</code> <p>If True, use Polars for computations during gdf conversion. This can speed up the process for large datasets. Requires <code>polars</code> to be installed.</p> <code>False</code> <code>use_parallel</code> <code>bool</code> <p>If True, use parallel processing for computations during gdf conversion. If <code>use_polars</code> is True, this will be ignored.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>Number of processes to use for parallel processing. If -1, uses all available cores. Ignored if <code>use_polars</code> is True. If <code>use_parallel</code> is False, this will be ignored.</p> <code>1</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for specific graph fitting functions. For example, <code>k</code> for KNN etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>W | GeoDataFrame</code> <p>W and gpd.GeoDataFrame: returns a libpysal weights object and a GeoDataFrame containing the spatial graph edges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n&gt;&gt;&gt; # load the HGSC cancer nuclei dataset\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; # set unique identifiers if not present\n&gt;&gt;&gt; nuc = set_uid(nuc, id_col=\"uid\")\n&gt;&gt;&gt; # Fit a Delaunay triangulation graph\n&gt;&gt;&gt; w, w_gdf = fit_graph(\n...     nuc, \"delaunay\", id_col=\"uid\", threshold=100\n... )\n&gt;&gt;&gt; print(w_gdf.head(3))\nindex  ...                                     geometry\n0      0  ...  LINESTRING (1400.038 1.692, 1386.459 9.581)\n1      1  ...   LINESTRING (1400.038 1.692, 1306.06 2.528)\n2      6  ...   LINESTRING (1386.459 9.581, 1306.06 2.528)\n[3 rows x 12 columns]\n&gt;&gt;&gt; # Plot the spatial graph\n&gt;&gt;&gt; ax = nuc.plot(column=\"class_name\", figsize=(5, 5), aspect=1)\n&gt;&gt;&gt; w_gdf.plot(ax=ax, column=\"class_name\", aspect=1, lw=0.5)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_graph/graph.py</code> <pre><code>def fit_graph(\n    gdf: gpd.GeoDataFrame,\n    method: str,\n    id_col: str = \"uid\",\n    threshold: int = 100,\n    use_polars: bool = False,\n    use_parallel: bool = False,\n    num_processes: int = 1,\n    **kwargs,\n) -&gt; W | gpd.GeoDataFrame:\n    \"\"\"Fit a spatial graph to a GeoDataFrame.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The input GeoDataFrame with spatial data.\n        method (str):\n            Type of spatial graph to fit. Options are: \"delaunay\", \"knn\", \"rel_nhood\",\n            \"distband\", \"gabriel\", \"voronoi\".\n        id_col (str):\n            Column name for unique identifiers in the GeoDataFrame.\n        threshold (int):\n            Distance threshold (in pixels) for distance-based graphs.\n        use_polars (bool):\n            If True, use Polars for computations during gdf conversion. This can speed\n            up the process for large datasets. Requires `polars` to be installed.\n        use_parallel (bool):\n            If True, use parallel processing for computations during gdf conversion. If\n            `use_polars` is True, this will be ignored.\n        num_processes (int):\n            Number of processes to use for parallel processing. If -1, uses all\n            available cores. Ignored if `use_polars` is True. If `use_parallel` is\n            False, this will be ignored.\n        **kwargs (Any):\n            Additional keyword arguments for specific graph fitting functions.\n            For example, `k` for KNN etc.\n\n    Returns:\n        W and gpd.GeoDataFrame:\n            returns a libpysal weights object and a GeoDataFrame containing the spatial\n            graph edges.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.spatial_graph.graph import fit_graph\n        &gt;&gt;&gt; # load the HGSC cancer nuclei dataset\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; # set unique identifiers if not present\n        &gt;&gt;&gt; nuc = set_uid(nuc, id_col=\"uid\")\n        &gt;&gt;&gt; # Fit a Delaunay triangulation graph\n        &gt;&gt;&gt; w, w_gdf = fit_graph(\n        ...     nuc, \"delaunay\", id_col=\"uid\", threshold=100\n        ... )\n        &gt;&gt;&gt; print(w_gdf.head(3))\n        index  ...                                     geometry\n        0      0  ...  LINESTRING (1400.038 1.692, 1386.459 9.581)\n        1      1  ...   LINESTRING (1400.038 1.692, 1306.06 2.528)\n        2      6  ...   LINESTRING (1386.459 9.581, 1306.06 2.528)\n        [3 rows x 12 columns]\n        &gt;&gt;&gt; # Plot the spatial graph\n        &gt;&gt;&gt; ax = nuc.plot(column=\"class_name\", figsize=(5, 5), aspect=1)\n        &gt;&gt;&gt; w_gdf.plot(ax=ax, column=\"class_name\", aspect=1, lw=0.5)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/delaunay.png)\n    \"\"\"\n    allowed_types = [\"delaunay\", \"knn\", \"rel_nhood\", \"distband\", \"gabriel\", \"voronoi\"]\n    if method not in allowed_types:\n        raise ValueError(f\"Type must be one of {allowed_types}. Got {method}.\")\n\n    # ensure gdf has a unique identifier\n    if id_col not in gdf.columns:\n        if id_col is None:\n            id_col = \"uid\"\n        gdf = set_uid(gdf, id_col=id_col)\n        gdf = set_crs(gdf)  # ensure CRS is set to avoid warnings\n\n    # fit spatial weights\n    if method == \"delaunay\":\n        w = fit_delaunay(gdf, id_col=id_col, **kwargs)\n    elif method == \"knn\":\n        w = fit_knn(gdf, id_col=id_col, **kwargs)\n    elif method == \"rel_nhood\":\n        w = fit_rel_nhood(gdf, id_col=id_col, **kwargs)\n    elif method == \"distband\":\n        w = fit_distband(gdf, threshold=threshold, id_col=id_col, **kwargs)\n    elif method == \"gabriel\":\n        w = fit_gabriel(gdf, id_col=id_col, **kwargs)\n    elif method == \"voronoi\":\n        w = fit_voronoi(gdf, id_col=id_col, **kwargs)\n\n    # if islands are dropped, add them back to avoid errors\n    missing_keys = sorted(set(gdf[id_col]) - set(w.neighbors.keys()))\n    if missing_keys:\n        w = _set_missing_keys(w, missing_keys=missing_keys)\n\n    # convert to GeoDataFrame\n    w_gdf = weights2gdf(\n        gdf,\n        w,\n        parallel=use_parallel,\n        use_polars=use_polars,\n        num_processes=num_processes,\n    )\n\n    # drop geometries that are longer than the threshold\n    if method != \"distband\":\n        w_gdf = w_gdf[w_gdf.geometry.length &lt;= threshold]\n\n    return w, w_gdf.reset_index(drop=True)\n</code></pre>"},{"location":"api/spatial_graph/weights2gdf/","title":"weights2gdf","text":"<p>Convert a <code>libpysal</code> weights object to a <code>geopandas.GeoDataFrame</code>.</p> <p>Adds class names and node centroids to the dataframe.</p> Note <p>if <code>w.neighbors</code> is empty, this will return None.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of the nodes.</p> required <code>w</code> <code>W</code> <p>PySAL weights object.</p> required <code>use_polars</code> <code>bool</code> <p>Whether to use Polars for computations. For large datasets, this can significantly speed up the process. Note that this requires <code>polars</code> to be installed. If set to True, the <code>parallel</code> argument will be ignored.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to use parallel processing.</p> <code>False</code> <code>num_processes</code> <code>int</code> <p>Number of processes to use for parallel processing. If -1, uses all available cores. Ignored if <code>use_polars</code> is True. If <code>parallel</code> is False, this will be ignored.</p> <code>1</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: GeoDataFrame of the links.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_graph.utils import weights2gdf\n&gt;&gt;&gt; from histolytics.spatial_graph.spatial_weights import fit_delaunay\n&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; id_col = \"uid\"\n&gt;&gt;&gt; gdf = nuc.copy()\n&gt;&gt;&gt; gdf = set_uid(gdf, id_col=id_col)\n&gt;&gt;&gt; # use only neoplastic nuclei\n&gt;&gt;&gt; gdf = gdf[gdf[\"class_name\"] == \"neoplastic\"].copy()\n&gt;&gt;&gt; w = fit_delaunay(gdf, id_col=id_col)\n&gt;&gt;&gt; link_gdf = weights2gdf(gdf, w)\n&gt;&gt;&gt; print(link_gdf.iloc[:, :5].head(3))\nindex  focal  neighbor  weight                               focal_centroid\n0      0     23        26     1.0  POINT (942.1755496587866 4706.286605348464)\n1      1     23       168     1.0  POINT (942.1755496587866 4706.286605348464)\n2      2     23      1291     1.0  POINT (942.1755496587866 4706.286605348464)\n</code></pre> Source code in <code>src/histolytics/spatial_graph/utils.py</code> <pre><code>def weights2gdf(\n    gdf: gpd.GeoDataFrame,\n    w: W,\n    use_polars: bool = False,\n    parallel: bool = False,\n    num_processes: int = 1,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert a `libpysal` weights object to a `geopandas.GeoDataFrame`.\n\n    Adds class names and node centroids to the dataframe.\n\n    Note:\n        if `w.neighbors` is empty, this will return None.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            GeoDataFrame of the nodes.\n        w (W):\n            PySAL weights object.\n        use_polars (bool):\n            Whether to use Polars for computations. For large datasets, this can\n            significantly speed up the process. Note that this requires `polars`\n            to be installed. If set to True, the `parallel` argument will be ignored.\n        parallel (bool):\n            Whether to use parallel processing.\n        num_processes (int):\n            Number of processes to use for parallel processing. If -1, uses all\n            available cores. Ignored if `use_polars` is True. If `parallel` is\n            False, this will be ignored.\n\n    Returns:\n        gpd.GeoDataFrame:\n            GeoDataFrame of the links.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_graph.utils import weights2gdf\n        &gt;&gt;&gt; from histolytics.spatial_graph.spatial_weights import fit_delaunay\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; id_col = \"uid\"\n        &gt;&gt;&gt; gdf = nuc.copy()\n        &gt;&gt;&gt; gdf = set_uid(gdf, id_col=id_col)\n        &gt;&gt;&gt; # use only neoplastic nuclei\n        &gt;&gt;&gt; gdf = gdf[gdf[\"class_name\"] == \"neoplastic\"].copy()\n        &gt;&gt;&gt; w = fit_delaunay(gdf, id_col=id_col)\n        &gt;&gt;&gt; link_gdf = weights2gdf(gdf, w)\n        &gt;&gt;&gt; print(link_gdf.iloc[:, :5].head(3))\n        index  focal  neighbor  weight                               focal_centroid\n        0      0     23        26     1.0  POINT (942.1755496587866 4706.286605348464)\n        1      1     23       168     1.0  POINT (942.1755496587866 4706.286605348464)\n        2      2     23      1291     1.0  POINT (942.1755496587866 4706.286605348464)\n    \"\"\"\n    gdf = gdf.copy()\n\n    if not w.neighbors:\n        return\n\n    if \"class_name\" not in gdf.columns:\n        raise ValueError(\"GeoDataFrame must contain a 'class_name' column.\")\n\n    # Check for non-string values in class_name column (excluding NaN)\n    non_null_classes = gdf[\"class_name\"].dropna()\n    if len(non_null_classes) &gt; 0:\n        non_string_classes = non_null_classes[\n            ~non_null_classes.apply(lambda x: isinstance(x, str))\n        ]\n        if len(non_string_classes) &gt; 0:\n            non_string_values = non_string_classes.unique()\n            raise ValueError(\n                f\"All values in 'class_name' column must be strings. \"\n                f\"Found non-string values: {list(non_string_values)} \"\n                f\"with types: {[type(v).__name__ for v in non_string_values]}\"\n            )\n\n    # get all possible link class combinations\n    classes = sorted(gdf.class_name.unique().tolist())\n\n    link_combos = _get_link_combinations(classes)\n\n    # init link gdf\n    link_gdf = w.to_adjlist(remove_symmetric=True, drop_islands=True).reset_index()\n\n    # add centroids\n    gdf.loc[:, \"centroid\"] = gdf.centroid\n    gdf[\"centroid_x\"] = gdf[\"centroid\"].apply(lambda p: p.x)\n    gdf[\"centroid_y\"] = gdf[\"centroid\"].apply(lambda p: p.y)\n\n    # add focal and neighbor centroid coords and class names\n    # don't use shapely objs here to speed things up\n    link_gdf.loc[:, \"focal_centroid_x\"] = gdf.loc[link_gdf.focal][\n        \"centroid_x\"\n    ].to_list()\n    link_gdf.loc[:, \"focal_centroid_y\"] = gdf.loc[link_gdf.focal][\n        \"centroid_y\"\n    ].to_list()\n    link_gdf.loc[:, \"neighbor_centroid_x\"] = gdf.loc[link_gdf.neighbor][\n        \"centroid_x\"\n    ].to_list()\n    link_gdf.loc[:, \"neighbor_centroid_y\"] = gdf.loc[link_gdf.neighbor][\n        \"centroid_y\"\n    ].to_list()\n    link_gdf.loc[:, \"focal_class_name\"] = gdf.loc[link_gdf.focal][\n        \"class_name\"\n    ].to_list()\n    link_gdf.loc[:, \"neighbor_class_name\"] = gdf.loc[link_gdf.neighbor][\n        \"class_name\"\n    ].to_list()\n\n    if use_polars:\n        try:\n            import polars as pl\n        except ImportError:\n            raise ImportError(\n                \"polars is not installed. Please install it with `pip install polars`.\"\n            )\n\n        # get link classses\n        link_gdf = gdf_to_polars(link_gdf)\n\n        func = partial(_get_link_class, link_combos=link_combos)\n        link_gdf = link_gdf.with_columns(\n            pl.struct(\n                [\n                    \"focal_class_name\",\n                    \"neighbor_class_name\",\n                ]\n            )\n            .map_elements(\n                lambda x: func(\n                    x[\"focal_class_name\"],\n                    x[\"neighbor_class_name\"],\n                ),\n                return_dtype=pl.String,\n            )\n            .alias(\"class_name\")\n        )\n\n        # create links between centroids\n        link_gdf = link_gdf.with_columns(\n            pl.struct(\n                [\n                    \"focal_centroid_x\",\n                    \"focal_centroid_y\",\n                    \"neighbor_centroid_x\",\n                    \"neighbor_centroid_y\",\n                ]\n            )\n            .map_elements(\n                lambda x: _create_link(\n                    x[\"focal_centroid_x\"],\n                    x[\"focal_centroid_y\"],\n                    x[\"neighbor_centroid_x\"],\n                    x[\"neighbor_centroid_y\"],\n                ),\n                return_dtype=pl.Object,\n            )\n            .alias(\"geometry\")\n        )\n        # Convert to pandas DataFrame\n        link_gdf = link_gdf.to_pandas()\n\n        # Convert to GeoDataFrame and set geometry\n        link_gdf = gpd.GeoDataFrame(link_gdf, geometry=\"geometry\")\n    else:\n        #  get link class names based on focal and neighbor class names\n        func = partial(_get_link_class, link_combos=link_combos)\n        link_gdf[\"class_name\"] = gdf_apply(\n            link_gdf,\n            func=func,\n            columns=[\"focal_class_name\", \"neighbor_class_name\"],\n            axis=1,\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n\n        link_gdf[\"geometry\"] = gdf_apply(\n            link_gdf,\n            func=_create_link,\n            columns=[\n                \"focal_centroid_x\",\n                \"focal_centroid_y\",\n                \"neighbor_centroid_x\",\n                \"neighbor_centroid_y\",\n            ],\n            axis=1,\n            parallel=parallel,\n            num_processes=num_processes,\n        )\n        link_gdf = link_gdf.set_geometry(\"geometry\")\n\n    return link_gdf\n</code></pre>"},{"location":"api/spatial_ops/get_interfaces/","title":"get_interfaces","text":"<p>Get the interfaces b/w the polygons defined in a <code>areas</code> and <code>buffer_area</code>.</p> Note <p>Identifies the interface regions between polygons in <code>area1</code> and in <code>area2</code> by buffering the <code>area2</code> polygons and finding their intersections with <code>area1</code>. The width of the interface is controlled by the <code>buffer_dist</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>area1</code> <code>GeoDataFrame</code> <p>The area or region of interest that is buffered on top of polygons in area2.</p> required <code>area2</code> <code>GeoDataFrame</code> <p>A geodataframe containing polygons (tissue areas) that might intersect. with the <code>buffer_area</code>.</p> required <code>buffer_dist</code> <code>int</code> <p>The radius (in pixels) of the buffer.</p> <code>200</code> <code>symmetric_buffer</code> <code>bool</code> <p>Whether to use a symmetric buffering to both directions. This doubles the buffer size.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A geodataframe containing the intersecting polygons including the buffer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_ops import get_interfaces\n&gt;&gt;&gt; from histolytics.data import cervix_tissue\n&gt;&gt;&gt; # load the tissue data\n&gt;&gt;&gt; tis = cervix_tissue()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # get the stromal and CIN tissue\n&gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n&gt;&gt;&gt; cin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # get the interface between cin lesion and stroma\n&gt;&gt;&gt; interface = get_interfaces(cin_tissue, stroma, buffer_dist=300)\n&gt;&gt;&gt; print(interface.head(3))\n    class_name                                           geometry\n0     stroma  POLYGON ((1588.71 4829.03, 1591.18 4828.83, 15...\n1     stroma  POLYGON ((743.07 5587.48, 743.63 5589, 744.07 ...\n2     stroma  POLYGON ((1151 7566, 1150 7567, 1148.2 7568.2,...\n&gt;&gt;&gt; # plot the tissue and the interface\n&gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n&gt;&gt;&gt; interface.plot(ax=ax, color=\"blue\", lw=1, alpha=0.3)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_ops/ops.py</code> <pre><code>def get_interfaces(\n    area1: gpd.GeoDataFrame,\n    area2: gpd.GeoDataFrame,\n    buffer_dist: int = 200,\n    symmetric_buffer: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get the interfaces b/w the polygons defined in a `areas` and `buffer_area`.\n\n    Note:\n        Identifies the interface regions between polygons in `area1` and in `area2`\n        by buffering the `area2` polygons and finding their intersections with `area1`.\n        The width of the interface is controlled by the `buffer_dist` parameter.\n\n    Parameters:\n        area1 (gpd.GeoDataFrame):\n            The area or region of interest that is buffered on top of polygons in area2.\n        area2 (gpd.GeoDataFrame):\n            A geodataframe containing polygons (tissue areas) that might intersect.\n            with the `buffer_area`.\n        buffer_dist (int):\n            The radius (in pixels) of the buffer.\n        symmetric_buffer (bool):\n            Whether to use a symmetric buffering to both directions. This doubles the\n            buffer size.\n\n    Returns:\n        gpd.GeoDataFrame:\n            A geodataframe containing the intersecting polygons including the buffer.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_ops import get_interfaces\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue\n        &gt;&gt;&gt; # load the tissue data\n        &gt;&gt;&gt; tis = cervix_tissue()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # get the stromal and CIN tissue\n        &gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n        &gt;&gt;&gt; cin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # get the interface between cin lesion and stroma\n        &gt;&gt;&gt; interface = get_interfaces(cin_tissue, stroma, buffer_dist=300)\n        &gt;&gt;&gt; print(interface.head(3))\n            class_name                                           geometry\n        0     stroma  POLYGON ((1588.71 4829.03, 1591.18 4828.83, 15...\n        1     stroma  POLYGON ((743.07 5587.48, 743.63 5589, 744.07 ...\n        2     stroma  POLYGON ((1151 7566, 1150 7567, 1148.2 7568.2,...\n        &gt;&gt;&gt; # plot the tissue and the interface\n        &gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n        &gt;&gt;&gt; interface.plot(ax=ax, color=\"blue\", lw=1, alpha=0.3)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/interfaces.png)\n    \"\"\"\n    area1 = set_crs(area1)\n    area2 = set_crs(area2)\n\n    buffer_zone = gpd.GeoDataFrame(\n        {\"geometry\": list(area1.buffer(buffer_dist))},\n        crs=area1.crs,\n    )\n    inter = area2.overlay(buffer_zone, how=\"intersection\")\n\n    if symmetric_buffer:\n        buffer_zone2 = gpd.GeoDataFrame(\n            {\"geometry\": list(area2.buffer(buffer_dist))},\n            crs=area2.crs,\n        )\n        inter = buffer_zone.overlay(buffer_zone2, how=\"intersection\")\n\n    # if the intersecting area is covered totally by any polygon in the `areas` gdf\n    # take the difference of the intresecting area and the orig roi to discard\n    # the roi from the interface 'sheet'\n    if not inter.empty:\n        if area2.covers(inter.geometry.loc[0]).any():  # len(inter) == 1\n            inter = inter.overlay(area1, how=\"difference\", keep_geom_type=True)\n\n    return inter.dissolve().explode().reset_index(drop=True)\n</code></pre>"},{"location":"api/spatial_ops/get_objs/","title":"get_objs","text":"<p>Query objects in relation to the given <code>area</code> GeoDataFrame (tissue segmentations).</p> <p>Parameters:</p> Name Type Description Default <code>area</code> <code>GeoDataFrame</code> <p>Area of interest. The objects that intersect with this area will be returned.</p> required <code>objects</code> <code>GeoDataFrame</code> <p>Objects to check for intersection with the area.</p> required <code>predicate</code> <code>str</code> <p>Predicate for the spatial query. One of contains\", \"contains_properly\", \"covered_by\", \"covers\", \"crosses\", \"intersects\", \"overlaps\", \"touches\", \"within\", \"dwithin\"</p> <code>'intersects'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the spatial query.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, None]</code> <p>Union[gpd.GeoDataFrame, None]: Objects that intersect with the given area.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import cervix_nuclei, cervix_tissue\n&gt;&gt;&gt; from histolytics.spatial_ops import get_objs\n&gt;&gt;&gt; # load the data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; tis = cervix_tissue()\n&gt;&gt;&gt; # select the CIN tissue\n&gt;&gt;&gt; cin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # select all the nuclei contained within CIN tissue\n&gt;&gt;&gt; nuc_within_cin = get_objs(cin_tissue, nuc, predicate=\"contains\")\n&gt;&gt;&gt; print(nuc_within_cin.head(3))\n                                            geometry         class_name\n1  POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...         connective\n2  POLYGON ((866 5137.02, 862.77 5137.94, 860 513...   squamous_epithel\n3  POLYGON ((932 4777.02, 928 4778.02, 922.81 478...  glandular_epithel\n&gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n&gt;&gt;&gt; nuc_within_cin.plot(ax=ax, color=\"blue\")\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_ops/ops.py</code> <pre><code>def get_objs(\n    area: gpd.GeoDataFrame,\n    objects: gpd.GeoDataFrame,\n    predicate: str = \"intersects\",\n    **kwargs,\n) -&gt; Union[gpd.GeoDataFrame, None]:\n    \"\"\"Query objects in relation to the given `area` GeoDataFrame (tissue segmentations).\n\n    Parameters:\n        area (gpd.GeoDataFrame):\n            Area of interest. The objects that intersect with this area will be returned.\n        objects (gpd.GeoDataFrame):\n            Objects to check for intersection with the area.\n        predicate (str):\n            Predicate for the spatial query. One of contains\", \"contains_properly\",\n            \"covered_by\", \"covers\", \"crosses\", \"intersects\", \"overlaps\", \"touches\",\n            \"within\", \"dwithin\"\n        **kwargs (Any):\n            Additional keyword arguments to pass to the spatial query.\n\n    Returns:\n        Union[gpd.GeoDataFrame, None]:\n            Objects that intersect with the given area.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import cervix_nuclei, cervix_tissue\n        &gt;&gt;&gt; from histolytics.spatial_ops import get_objs\n        &gt;&gt;&gt; # load the data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; tis = cervix_tissue()\n        &gt;&gt;&gt; # select the CIN tissue\n        &gt;&gt;&gt; cin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # select all the nuclei contained within CIN tissue\n        &gt;&gt;&gt; nuc_within_cin = get_objs(cin_tissue, nuc, predicate=\"contains\")\n        &gt;&gt;&gt; print(nuc_within_cin.head(3))\n                                                    geometry         class_name\n        1  POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ...         connective\n        2  POLYGON ((866 5137.02, 862.77 5137.94, 860 513...   squamous_epithel\n        3  POLYGON ((932 4777.02, 928 4778.02, 922.81 478...  glandular_epithel\n        &gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n        &gt;&gt;&gt; nuc_within_cin.plot(ax=ax, color=\"blue\")\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/get_objs.png)\n    \"\"\"\n    if isinstance(area, shapely.geometry.Polygon):\n        area = gpd.GeoSeries([area], crs=objects.crs)\n\n    # NOTE, gdfs need to have same crs, otherwise warning flood.\n    inds = objects.geometry.sindex.query(area.geometry, predicate=predicate, **kwargs)\n\n    # filter indices that are out of bounds\n    obj_pos_index = np.arange(len(objects))\n    inds = np.intersect1d(np.unique(inds), obj_pos_index)\n    objs: gpd.GeoDataFrame = objects.iloc[inds]\n\n    return objs.drop_duplicates(\"geometry\")\n</code></pre>"},{"location":"api/spatial_ops/h3_grid/","title":"h3_grid","text":"<p>Fit a <code>h3</code> hexagonal grid on top of a <code>geopandas.GeoDataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame to fit grid to.</p> required <code>resolution</code> <code>int</code> <p>H3 resolution, by default 9.</p> <code>9</code> <code>to_lonlat</code> <code>bool</code> <p>Whether to convert to lonlat coordinates, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Fitted h3 hex grid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_ops.h3 import h3_grid\n&gt;&gt;&gt; from histolytics.data import cervix_tissue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # get the stromal tissue\n&gt;&gt;&gt; tis = cervix_tissue()\n&gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n&gt;&gt;&gt; # Fit an H3 grid to the stromal tissue\n&gt;&gt;&gt; h3_gr = h3_grid(stroma, resolution=9)\n&gt;&gt;&gt; print(h3_gr.head(3))\n                                                          geometry\n8982a939503ffff  POLYGON ((6672.79721 859.08743, 6647.90711 661...\n8982a939877ffff  POLYGON ((2556.61731 5658.46273, 2581.53692 58...\n8982a939c4bffff  POLYGON ((4546.44516 4059.58249, 4366.53531 39...\n&gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n&gt;&gt;&gt; h3_gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=1)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_ops/h3.py</code> <pre><code>def h3_grid(\n    gdf: gpd.GeoDataFrame, resolution: int = 9, to_lonlat: bool = True\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Fit a `h3` hexagonal grid on top of a `geopandas.GeoDataFrame`.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            GeoDataFrame to fit grid to.\n        resolution (int):\n            H3 resolution, by default 9.\n        to_lonlat (bool):\n            Whether to convert to lonlat coordinates, by default True.\n\n    Returns:\n        gpd.GeoDataFrame:\n            Fitted h3 hex grid.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_ops.h3 import h3_grid\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # get the stromal tissue\n        &gt;&gt;&gt; tis = cervix_tissue()\n        &gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n        &gt;&gt;&gt; # Fit an H3 grid to the stromal tissue\n        &gt;&gt;&gt; h3_gr = h3_grid(stroma, resolution=9)\n        &gt;&gt;&gt; print(h3_gr.head(3))\n                                                                  geometry\n        8982a939503ffff  POLYGON ((6672.79721 859.08743, 6647.90711 661...\n        8982a939877ffff  POLYGON ((2556.61731 5658.46273, 2581.53692 58...\n        8982a939c4bffff  POLYGON ((4546.44516 4059.58249, 4366.53531 39...\n        &gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n        &gt;&gt;&gt; h3_gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=1)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/h3_grid.png)\n    \"\"\"\n    if gdf.empty or gdf is None:\n        return\n\n    # drop invalid geometries if there are any after buffer\n    gdf.loc[:, \"geometry\"] = gdf.make_valid()\n    orig_crs = gdf.crs\n\n    poly = shapely.force_2d(gdf.union_all())\n    if isinstance(poly, Polygon):\n        hexagons = _poly2hexgrid(poly, resolution=resolution, to_lonlat=to_lonlat)\n    else:\n        output = []\n        for geom in poly.geoms:\n            hexes = _poly2hexgrid(geom, resolution=resolution, to_lonlat=to_lonlat)\n            output.append(hexes)\n        hexagons = pd.concat(output)\n\n    return hexagons.set_crs(orig_crs, allow_override=True).drop_duplicates(\"geometry\")\n</code></pre>"},{"location":"api/spatial_ops/quadbin_grid/","title":"quadbin_grid","text":"<p>Fit a <code>quadbin</code> rectangular grid on top of a <code>geopandas.GeoDataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame to fit grid to.</p> required <code>resolution</code> <code>int</code> <p>Quadbin resolution, by default 17.</p> <code>17</code> <code>to_lonlat</code> <code>bool</code> <p>Whether to convert to lonlat coordinates, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Fitted Quadbin quad grid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_ops.quadbin import quadbin_grid\n&gt;&gt;&gt; from histolytics.data import cervix_tissue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # get the stromal tissue\n&gt;&gt;&gt; tis = cervix_tissue()\n&gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit a quadbin grid to the stromal tissue\n&gt;&gt;&gt; quad_grid = quadbin_grid(stroma, resolution=17)\n&gt;&gt;&gt; print(quad_grid.head(3))\n                                                            geometry\n5271089524171866111  POLYGON ((6581.37043 761.23896, 6581.36916 608...\n5271089524172062719  POLYGON ((6734.64415 761.23754, 6734.64288 608...\n5271089524171931647  POLYGON ((6734.64571 913.48504, 6734.64415 761...\n&gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n&gt;&gt;&gt; quad_grid.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=1)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_ops/quadbin.py</code> <pre><code>def quadbin_grid(\n    gdf: gpd.GeoDataFrame, resolution: int = 17, to_lonlat: bool = True\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Fit a `quadbin` rectangular grid on top of a `geopandas.GeoDataFrame`.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            GeoDataFrame to fit grid to.\n        resolution (int):\n            Quadbin resolution, by default 17.\n        to_lonlat (bool):\n            Whether to convert to lonlat coordinates, by default True.\n\n    Returns:\n        gpd.GeoDataFrame:\n            Fitted Quadbin quad grid.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_ops.quadbin import quadbin_grid\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # get the stromal tissue\n        &gt;&gt;&gt; tis = cervix_tissue()\n        &gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit a quadbin grid to the stromal tissue\n        &gt;&gt;&gt; quad_grid = quadbin_grid(stroma, resolution=17)\n        &gt;&gt;&gt; print(quad_grid.head(3))\n                                                                    geometry\n        5271089524171866111  POLYGON ((6581.37043 761.23896, 6581.36916 608...\n        5271089524172062719  POLYGON ((6734.64415 761.23754, 6734.64288 608...\n        5271089524171931647  POLYGON ((6734.64571 913.48504, 6734.64415 761...\n        &gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n        &gt;&gt;&gt; quad_grid.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=1)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/quadbin_grid.png)\n    \"\"\"\n    if gdf.empty or gdf is None:\n        return\n\n    # drop invalid geometries if there are any after buffer\n    gdf.loc[:, \"geometry\"] = gdf.make_valid()\n    orig_crs = gdf.crs\n\n    poly = shapely.force_2d(gdf.union_all())\n    if isinstance(poly, Polygon):\n        quads = _poly2hexgrid(poly, resolution=resolution, to_lonlat=to_lonlat)\n    else:\n        output = []\n        for geom in poly.geoms:\n            hexes = _poly2hexgrid(geom, resolution=resolution, to_lonlat=to_lonlat)\n            output.append(hexes)\n        quads = pd.concat(output)\n\n    return quads.set_crs(orig_crs, allow_override=True).drop_duplicates(\"geometry\")\n</code></pre>"},{"location":"api/spatial_ops/rect_grid/","title":"rect_grid","text":"<p>Overlay a square grid to the given areas of a <code>gpd.GeoDataFrame</code>.</p> Note <p>This function fits a rectangular grid with user defined resolution and optional overlap. The spatial predicates can be used to filter the grid cells that intersect, or are contained strictly within the given input GeoDataFrame.</p> Note <p>Returns None if the gdf is empty.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame to fit the grid to. Uses the bounding box of the GeoDataFrame to fit the grid.</p> required <code>resolution</code> <code>Tuple[int, int]</code> <p>Patch size/resolution of the grid (in pixels).</p> <code>(256, 256)</code> <code>overlap</code> <code>int</code> <p>overlap of the cells in the grid (in percentages).</p> <code>0</code> <code>predicate</code> <code>str</code> <p>Predicate to use for the spatial join, by default \"intersects\". Allowed values are \"intersects\", \"within\", \"contains\", \"contains_properly\".</p> <code>'intersects'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: GeoDataFrame with the grid fitted to the given GeoDataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If predicate is not one of \"intersects\" or \"within\".</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.spatial_ops.rect_grid import rect_grid\n&gt;&gt;&gt; from histolytics.data import cervix_tissue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # get the stromal tissue\n&gt;&gt;&gt; tis = cervix_tissue()\n&gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # fit a rectangular grid strictly within the stromal tissue\n&gt;&gt;&gt; grid = rect_grid(stroma, resolution=(256, 256), overlap=0, predicate=\"contains\")\n&gt;&gt;&gt; print(grid.head(3))\n                                            geometry\n0  POLYGON ((5443 626, 5699 626, 5699 882, 5443 8...\n1  POLYGON ((4419 882, 4675 882, 4675 1138, 4419 ...\n2  POLYGON ((4675 882, 4931 882, 4931 1138, 4675 ...\n&gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n&gt;&gt;&gt; grid.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=1)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/spatial_ops/rect_grid.py</code> <pre><code>def rect_grid(\n    gdf: gpd.GeoDataFrame,\n    resolution: Tuple[int, int] = (256, 256),\n    overlap: int = 0,\n    predicate: str = \"intersects\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Overlay a square grid to the given areas of a `gpd.GeoDataFrame`.\n\n    Note:\n        This function fits a rectangular grid with user defined resolution and optional\n        overlap. The spatial predicates can be used to filter the grid cells that\n        intersect, or are contained strictly within the given input GeoDataFrame.\n\n    Note:\n        Returns None if the gdf is empty.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            GeoDataFrame to fit the grid to. Uses the bounding box of the GeoDataFrame\n            to fit the grid.\n        resolution (Tuple[int, int]):\n            Patch size/resolution of the grid (in pixels).\n        overlap (int):\n            overlap of the cells in the grid (in percentages).\n        predicate (str):\n            Predicate to use for the spatial join, by default \"intersects\".\n            Allowed values are \"intersects\", \"within\", \"contains\", \"contains_properly\".\n\n    Returns:\n        gpd.GeoDataFrame:\n            GeoDataFrame with the grid fitted to the given GeoDataFrame.\n\n    Raises:\n        ValueError: If predicate is not one of \"intersects\" or \"within\".\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.spatial_ops.rect_grid import rect_grid\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # get the stromal tissue\n        &gt;&gt;&gt; tis = cervix_tissue()\n        &gt;&gt;&gt; stroma = tis[tis[\"class_name\"] == \"stroma\"]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # fit a rectangular grid strictly within the stromal tissue\n        &gt;&gt;&gt; grid = rect_grid(stroma, resolution=(256, 256), overlap=0, predicate=\"contains\")\n        &gt;&gt;&gt; print(grid.head(3))\n                                                    geometry\n        0  POLYGON ((5443 626, 5699 626, 5699 882, 5443 8...\n        1  POLYGON ((4419 882, 4675 882, 4675 1138, 4419 ...\n        2  POLYGON ((4675 882, 4931 882, 4931 1138, 4675 ...\n        &gt;&gt;&gt; ax = tis.plot(column=\"class_name\", figsize=(5, 5), aspect=1, alpha=0.5)\n        &gt;&gt;&gt; grid.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=1)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/rect_grid.png)\n    \"\"\"\n    if gdf.empty or gdf is None:\n        return\n\n    allowed = [\"intersects\", \"within\", \"contains\", \"contains_properly\"]\n    if predicate not in allowed:\n        raise ValueError(f\"predicate must be one of {allowed}. Got {predicate}\")\n\n    if not (0 &lt;= overlap &lt; 100):\n        raise ValueError(\"overlap must be in the range [0, 100)\")\n\n    stride = (\n        int(resolution[0] * (1 - overlap / 100)),\n        int(resolution[1] * (1 - overlap / 100)),\n    )\n\n    grid = _full_rect_grid(gdf, resolution, stride, pad=20)\n    grid = grid.set_crs(gdf.crs, allow_override=True)\n    _, grid_inds = grid.sindex.query(gdf.geometry, predicate=predicate)\n    grid = grid.iloc[np.unique(grid_inds)]\n\n    return grid.drop_duplicates(\"geometry\").reset_index(drop=True)\n</code></pre>"},{"location":"api/stroma_feats/collagen/","title":"extract_collagen_fibers","text":"<p>Extract collagen fibers from a H&amp;E image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The input image. Shape (H, W, 3).</p> required <code>label</code> <code>ndarray</code> <p>Nuclei binary or label mask. Shape (H, W). This is used to mask out the nuclei when extracting collagen fibers. If None, the entire image is used.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>The sigma parameter for the Canny edge detector.</p> <code>2.5</code> <code>min_size</code> <code>float</code> <p>Minimum size of the edges to keep.</p> <code>25</code> <code>rm_bg</code> <code>bool</code> <p>Whether to remove the background component from the edges.</p> <code>False</code> <code>rm_fg</code> <code>bool</code> <p>Whether to remove the foreground component from the edges.</p> <code>False</code> <code>mask</code> <code>ndarray</code> <p>Binary mask to restrict the region of interest. Shape (H, W). For example, it can be used to mask out tissues that are not of interest.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda', CuPy and cucim will be used for GPU acceleration.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The collagen fibers binary mask. Shape (H, W).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_stroma_he\n&gt;&gt;&gt; from histolytics.stroma_feats.collagen import extract_collagen_fibers\n&gt;&gt;&gt; from skimage.measure import label\n&gt;&gt;&gt; from skimage.color import label2rgb\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt;\n&gt;&gt;&gt; im = hgsc_stroma_he()\n&gt;&gt;&gt; collagen = extract_collagen_fibers(im, label=None, rm_bg=False, rm_fg=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n&gt;&gt;&gt; ax[0].imshow(label2rgb(label(collagen), bg_label=0))\n&gt;&gt;&gt; ax[0].set_axis_off()\n&gt;&gt;&gt; ax[1].imshow(im)\n&gt;&gt;&gt; ax[1].set_axis_off()\n&gt;&gt;&gt; fig.tight_layout()\n</code></pre> <p></p> Source code in <code>src/histolytics/stroma_feats/collagen.py</code> <pre><code>def extract_collagen_fibers(\n    img: np.ndarray,\n    label: np.ndarray = None,\n    sigma: float = 2.5,\n    min_size: int = 25,\n    rm_bg: bool = False,\n    rm_fg: bool = False,\n    mask: np.ndarray = None,\n    device: str = \"cpu\",\n) -&gt; np.ndarray:\n    \"\"\"Extract collagen fibers from a H&amp;E image.\n\n    Parameters:\n        img (np.ndarray):\n            The input image. Shape (H, W, 3).\n        label (np.ndarray):\n            Nuclei binary or label mask. Shape (H, W). This is used to mask out the\n            nuclei when extracting collagen fibers. If None, the entire image is used.\n        sigma (float):\n            The sigma parameter for the Canny edge detector.\n        min_size (float):\n            Minimum size of the edges to keep.\n        rm_bg (bool):\n            Whether to remove the background component from the edges.\n        rm_fg (bool):\n            Whether to remove the foreground component from the edges.\n        mask (np.ndarray):\n            Binary mask to restrict the region of interest. Shape (H, W). For example,\n            it can be used to mask out tissues that are not of interest.\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda',\n            CuPy and cucim will be used for GPU acceleration.\n\n    Returns:\n        np.ndarray: The collagen fibers binary mask. Shape (H, W).\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_stroma_he\n        &gt;&gt;&gt; from histolytics.stroma_feats.collagen import extract_collagen_fibers\n        &gt;&gt;&gt; from skimage.measure import label\n        &gt;&gt;&gt; from skimage.color import label2rgb\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; im = hgsc_stroma_he()\n        &gt;&gt;&gt; collagen = extract_collagen_fibers(im, label=None, rm_bg=False, rm_fg=False)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n        &gt;&gt;&gt; ax[0].imshow(label2rgb(label(collagen), bg_label=0))\n        &gt;&gt;&gt; ax[0].set_axis_off()\n        &gt;&gt;&gt; ax[1].imshow(im)\n        &gt;&gt;&gt; ax[1].set_axis_off()\n        &gt;&gt;&gt; fig.tight_layout()\n    ![out](../../img/collagen_fiber.png)\n    \"\"\"\n    if label is not None and img.shape[:2] != label.shape:\n        raise ValueError(\n            f\"Shape mismatch: img has shape {img.shape}, but label has shape {label.shape}.\"\n        )\n\n    if mask is not None:\n        if label is not None and mask.shape != label.shape:\n            raise ValueError(\n                f\"Shape mismatch: mask has shape {mask.shape}, but label has shape {label.shape}.\"\n            )\n        elif label is None and mask.shape != img.shape[:2]:\n            raise ValueError(\n                f\"Shape mismatch: mask has shape {mask.shape}, but img has shape {img.shape[:2]}.\"\n            )\n\n    if _has_cp and device == \"cuda\":\n        edges = canny(rgb2gray_cp(cp.array(img)).get(), sigma=sigma, mode=\"nearest\")\n    else:\n        edges = canny(rgb2gray(img), sigma=sigma, mode=\"nearest\")\n\n    if rm_bg or rm_fg:\n        if label is not None:\n            label = dilation(label, footprint_rectangle((5, 5)))\n            edges[label &gt; 0] = 0\n\n        bg_mask, dark_mask = tissue_components(img, label, device=device)\n        if rm_bg and rm_fg:\n            edges[bg_mask | dark_mask] = 0\n        elif rm_bg:\n            edges[bg_mask] = 0\n        elif rm_fg:\n            edges[dark_mask] = 0\n    else:\n        if label is not None:\n            edges[label &gt; 0] = 0\n\n    if _has_cp and device == \"cuda\":\n        edges = remove_small_objects_cp(\n            cp.array(edges), min_size=min_size, connectivity=2\n        ).get()\n    else:\n        edges = remove_small_objects(edges, min_size=min_size, connectivity=2)\n\n    if mask is not None:\n        edges = edges &amp; mask\n\n    return edges\n</code></pre>"},{"location":"api/stroma_feats/fiber_feats/","title":"fiber_feats","text":"<p>Extract collagen fiber features from an H&amp;E image.</p> Note <p>This function extracts collagen fibers from the image and computes various metrics on the extracted fibers. Allowed metrics are:</p> <pre><code>- tortuosity\n- average_turning_angle\n- length\n- major_axis_len\n- minor_axis_len\n- major_axis_angle\n- minor_axis_angle\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The input H&amp;E image. Shape (H, W, 3).</p> required <code>metrics</code> <code>Tuple[str]</code> <p>The metrics to compute. Options are:     - \"tortuosity\"     - \"average_turning_angle\"     - \"length\"     - \"major_axis_len\"     - \"minor_axis_len\"     - \"major_axis_angle\"     - \"minor_axis_angle\"</p> required <code>label</code> <code>ndarray</code> <p>The nuclei binary or label mask. Shape (H, W). This is used to mask out the nuclei when extracting collagen fibers. If None, the entire image is used.</p> <code>None</code> <code>mask</code> <code>ndarray</code> <p>Binary mask to restrict the region of interest. Shape (H, W). For example, it can be used to mask out tissues that are not of interest.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Flag whether to column (quantile) normalize the computed metrics or not.</p> <code>False</code> <code>rm_bg</code> <code>bool</code> <p>Whether to remove the background component from the edges.</p> <code>False</code> <code>rm_fg</code> <code>bool</code> <p>Whether to remove the foreground component from the edges.</p> <code>False</code> <code>device</code> <code>str</code> <p>Device to use for collagen extraction. Options are 'cpu' or 'cuda'. If set to 'cuda', CuPy and cucim will be used for GPU acceleration. This affects only the collagen extraction step, not the metric computation.</p> <code>'cpu'</code> <code>num_processes</code> <code>int</code> <p>The number of processes when converting to GeoDataFrame. If -1, all available processes will be used. Default is 1. Ignored if return_edges is False.</p> <code>1</code> <code>reset_uid</code> <code>bool</code> <p>Whether to reset the UID of the extracted fibers. Default is True. If False, the original UIDs will be preserved.</p> required <code>return_edges</code> <code>bool</code> <p>Whether to return the extracted edges as a GeoDataFrame. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame containing the extracted collagen fibers as LineString geometries and the computed metrics as columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_stroma_he, hgsc_stroma_nuclei\n&gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load example image and nuclei annotation\n&gt;&gt;&gt; img = hgsc_stroma_he()\n&gt;&gt;&gt; label = gdf2inst(hgsc_stroma_nuclei(), width=1500, height=1500)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Extract fiber features\n&gt;&gt;&gt; edge_gdf = fiber_feats(\n...    img,\n...    label=label,\n...    metrics=(\"length\", \"tortuosity\", \"average_turning_angle\"),\n...    device=\"cpu\",\n...    num_processes=4,\n...    normalize=True,\n...    return_edges=True,\n... )\n&gt;&gt;&gt; print(edge_gdf.head(3))\n        uid  class_name                                           geometry              0    1           1  LINESTRING (29.06525 26.95506, 29.03764 26.844...\n    1    2           1  MULTILINESTRING ((69.19964 89.83999, 69.01369 ...\n    2    3           1  MULTILINESTRING ((51.54728 1.36606, 51.67797 1...\n        length  tortuosity  average_turning_angle\n    0  0.450252    0.372026               0.294881\n    1  0.977289    0.643115               0.605263\n    2  0.700793    0.661500               0.560562\n</code></pre> Source code in <code>src/histolytics/stroma_feats/collagen.py</code> <pre><code>def fiber_feats(\n    img: np.ndarray,\n    metrics: Tuple[str],\n    label: np.ndarray = None,\n    mask: np.ndarray = None,\n    normalize: bool = False,\n    rm_bg: bool = False,\n    rm_fg: bool = False,\n    device: str = \"cpu\",\n    num_processes: int = 1,\n    return_edges: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Extract collagen fiber features from an H&amp;E image.\n\n    Note:\n        This function extracts collagen fibers from the image and computes various metrics\n        on the extracted fibers. Allowed metrics are:\n\n            - tortuosity\n            - average_turning_angle\n            - length\n            - major_axis_len\n            - minor_axis_len\n            - major_axis_angle\n            - minor_axis_angle\n\n    Parameters:\n        img (np.ndarray):\n            The input H&amp;E image. Shape (H, W, 3).\n        metrics (Tuple[str]):\n            The metrics to compute. Options are:\n                - \"tortuosity\"\n                - \"average_turning_angle\"\n                - \"length\"\n                - \"major_axis_len\"\n                - \"minor_axis_len\"\n                - \"major_axis_angle\"\n                - \"minor_axis_angle\"\n        label (np.ndarray):\n            The nuclei binary or label mask. Shape (H, W). This is used to mask out the\n            nuclei when extracting collagen fibers. If None, the entire image is used.\n        mask (np.ndarray):\n            Binary mask to restrict the region of interest. Shape (H, W). For example,\n            it can be used to mask out tissues that are not of interest.\n        normalize (bool):\n            Flag whether to column (quantile) normalize the computed metrics or not.\n        rm_bg (bool):\n            Whether to remove the background component from the edges.\n        rm_fg (bool):\n            Whether to remove the foreground component from the edges.\n        device (str):\n            Device to use for collagen extraction. Options are 'cpu' or 'cuda'. If set to\n            'cuda', CuPy and cucim will be used for GPU acceleration. This affects only\n            the collagen extraction step, not the metric computation.\n        num_processes (int):\n            The number of processes when converting to GeoDataFrame. If -1, all\n            available processes will be used. Default is 1. Ignored if return_edges is False.\n        reset_uid (bool):\n            Whether to reset the UID of the extracted fibers. Default is True. If False,\n            the original UIDs will be preserved.\n        return_edges (bool):\n            Whether to return the extracted edges as a GeoDataFrame. Default is False.\n\n    Returns:\n        gpd.GeoDataFrame:\n            A GeoDataFrame containing the extracted collagen fibers as LineString\n            geometries and the computed metrics as columns.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_stroma_he, hgsc_stroma_nuclei\n        &gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load example image and nuclei annotation\n        &gt;&gt;&gt; img = hgsc_stroma_he()\n        &gt;&gt;&gt; label = gdf2inst(hgsc_stroma_nuclei(), width=1500, height=1500)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Extract fiber features\n        &gt;&gt;&gt; edge_gdf = fiber_feats(\n        ...    img,\n        ...    label=label,\n        ...    metrics=(\"length\", \"tortuosity\", \"average_turning_angle\"),\n        ...    device=\"cpu\",\n        ...    num_processes=4,\n        ...    normalize=True,\n        ...    return_edges=True,\n        ... )\n        &gt;&gt;&gt; print(edge_gdf.head(3))\n                uid  class_name                                           geometry  \\\n            0    1           1  LINESTRING (29.06525 26.95506, 29.03764 26.844...\n            1    2           1  MULTILINESTRING ((69.19964 89.83999, 69.01369 ...\n            2    3           1  MULTILINESTRING ((51.54728 1.36606, 51.67797 1...\n                length  tortuosity  average_turning_angle\n            0  0.450252    0.372026               0.294881\n            1  0.977289    0.643115               0.605263\n            2  0.700793    0.661500               0.560562\n    \"\"\"\n    edges = extract_collagen_fibers(\n        img, label=label, mask=mask, device=device, rm_bg=rm_bg, rm_fg=rm_fg\n    )\n    labeled_edges = sklabel(edges)\n\n    if len(np.unique(labeled_edges)) &lt;= 1:\n        return gpd.GeoDataFrame(columns=[\"uid\", \"class_name\", \"geometry\", *metrics])\n\n    feat_df = _compute_fiber_feats(labeled_edges, metrics)\n\n    if normalize:\n        feat_df = feat_df.apply(col_norm)\n\n    # Convert labeled edges to GeoDataFrame\n    if return_edges:\n        edge_gdf = inst2gdf(dilation(labeled_edges))\n        edge_gdf = edge_gdf.merge(feat_df, left_on=\"uid\", right_index=True)\n        edge_gdf[\"geometry\"] = gdf_apply(\n            edge_gdf,\n            _get_medial_smooth,\n            columns=[\"geometry\"],\n            parallel=num_processes &gt; 1,\n            num_processes=num_processes,\n        )\n        edge_gdf = edge_gdf.assign(class_name=\"collagen\")\n        return (\n            edge_gdf.sort_values(by=\"uid\")\n            .set_index(\"uid\", verify_integrity=True, drop=True)\n            .reset_index(drop=True)\n        )\n\n    return feat_df\n</code></pre>"},{"location":"api/stroma_feats/stroma_feats/","title":"stromal_intensity_feats","text":"<p>Computes intensity features of stromal components of an input H&amp;E image.</p> Note <p>This functions decomposes the input image into its H&amp;E components and computes various intensity features for the corresponding hematoxylin and eosin components along with the area occupied by both stain components.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The input image. Shape (H, W, 3).</p> required <code>label</code> <code>ndarray</code> <p>The nuclei mask. Shape (H, W). This is used to mask out the nuclei when computing stromal features. If None, the entire image is used.</p> <code>None</code> <code>metrics</code> <code>Tuple[str, ...]</code> <p>The intensity stats to compute.</p> <pre><code>- \"max\"\n- \"min\"\n- \"mean\"\n- \"median\"\n- \"std\"\n- \"quantiles\"\n- \"meanmediandiff\"\n- \"mad\"\n- \"iqr\"\n- \"skewness\"\n- \"kurtosis\"\n- \"histenergy\"\n- \"histentropy\"\n</code></pre> <code>('mean', 'std', 'quantiles')</code> <code>quantiles</code> <code>tuple or list</code> <p>The quantiles to compute. Default is (0.25, 0.5, 0.75).</p> <code>(0.25, 0.5, 0.75)</code> <code>n_bins</code> <code>int</code> <p>Number of bins for histogram-based features.</p> <code>32</code> <code>hist_range</code> <code>Tuple[float, float]</code> <p>Range for histogram computation. If None, uses data range.</p> <code>None</code> <code>mask</code> <code>ndarray</code> <p>Binary mask to restrict the region of interest. Shape (H, W). For example, it can be used to mask out tissues that are not of interest.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use (\"cpu\" or \"cuda\"). Gpu-acceleration can be enabled for hed- decomposition.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The computed features. Keys include features for both hematoxylin and eosin components with prefixes like \"hematoxylin_area\", \"eosin_mean_red\", etc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_stroma_he, hgsc_stroma_nuclei\n&gt;&gt;&gt; from histolytics.stroma_feats.intensity import stromal_intensity_feats\n&gt;&gt;&gt;\n&gt;&gt;&gt; img = hgsc_stroma_he()\n&gt;&gt;&gt; label = gdf2inst(hgsc_stroma_nuclei(), width=1500, height=1500)\n&gt;&gt;&gt; feats = stromal_intensity_feats(\n...     img=img,\n...     label=label,\n...     metrics=metrics,\n...     quantiles=quantiles,\n...     n_bins=n_bins,\n...     hist_range=hist_range,\n...     device=\"cuda\",\n... )\n&gt;&gt;&gt; print(feats.head(5).round(4))\nhematoxylin_area        545588.0000\nhematoxylin_R_min            0.0000\nhematoxylin_R_max            0.9068\nhematoxylin_R_mean           0.7751\nhematoxylin_R_median         0.8070\ndtype: float64\n</code></pre> Source code in <code>src/histolytics/stroma_feats/intensity.py</code> <pre><code>def stromal_intensity_feats(\n    img: np.ndarray,\n    label: np.ndarray = None,\n    metrics: Tuple[str, ...] = (\"mean\", \"std\", \"quantiles\"),\n    quantiles: Union[tuple, list] = (0.25, 0.5, 0.75),\n    n_bins: int = 32,\n    hist_range: Tuple[float, float] = None,\n    mask: np.ndarray = None,\n    device: str = \"cpu\",\n) -&gt; pd.Series:\n    \"\"\"Computes intensity features of stromal components of an input H&amp;E image.\n\n    Note:\n        This functions decomposes the input image into its H&amp;E components and computes\n        various intensity features for the corresponding hematoxylin and eosin components\n        along with the area occupied by both stain components.\n\n    Parameters:\n        img (np.ndarray):\n            The input image. Shape (H, W, 3).\n        label (np.ndarray):\n            The nuclei mask. Shape (H, W). This is used to mask out the nuclei when\n            computing stromal features. If None, the entire image is used.\n        metrics (Tuple[str, ...]):\n            The intensity stats to compute.\n\n                - \"max\"\n                - \"min\"\n                - \"mean\"\n                - \"median\"\n                - \"std\"\n                - \"quantiles\"\n                - \"meanmediandiff\"\n                - \"mad\"\n                - \"iqr\"\n                - \"skewness\"\n                - \"kurtosis\"\n                - \"histenergy\"\n                - \"histentropy\"\n        quantiles (tuple or list):\n            The quantiles to compute. Default is (0.25, 0.5, 0.75).\n        n_bins (int):\n            Number of bins for histogram-based features.\n        hist_range (Tuple[float, float]):\n            Range for histogram computation. If None, uses data range.\n        mask (np.ndarray):\n            Binary mask to restrict the region of interest. Shape (H, W). For example,\n            it can be used to mask out tissues that are not of interest.\n        device (str):\n            Device to use (\"cpu\" or \"cuda\"). Gpu-acceleration can be enabled for hed-\n            decomposition.\n\n    Returns:\n        pd.Series:\n            The computed features. Keys include features for both hematoxylin and eosin\n            components with prefixes like \"hematoxylin_area\", \"eosin_mean_red\", etc.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_stroma_he, hgsc_stroma_nuclei\n        &gt;&gt;&gt; from histolytics.stroma_feats.intensity import stromal_intensity_feats\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; img = hgsc_stroma_he()\n        &gt;&gt;&gt; label = gdf2inst(hgsc_stroma_nuclei(), width=1500, height=1500)\n        &gt;&gt;&gt; feats = stromal_intensity_feats(\n        ...     img=img,\n        ...     label=label,\n        ...     metrics=metrics,\n        ...     quantiles=quantiles,\n        ...     n_bins=n_bins,\n        ...     hist_range=hist_range,\n        ...     device=\"cuda\",\n        ... )\n        &gt;&gt;&gt; print(feats.head(5).round(4))\n        hematoxylin_area        545588.0000\n        hematoxylin_R_min            0.0000\n        hematoxylin_R_max            0.9068\n        hematoxylin_R_mean           0.7751\n        hematoxylin_R_median         0.8070\n        dtype: float64\n    \"\"\"\n    allowed = (\n        \"max\",\n        \"min\",\n        \"mean\",\n        \"median\",\n        \"std\",\n        \"quantiles\",\n        \"meanmediandiff\",\n        \"mad\",\n        \"iqr\",\n        \"skewness\",\n        \"kurtosis\",\n        \"histenergy\",\n        \"histentropy\",\n    )\n    if not all(m in allowed for m in metrics):\n        raise ValueError(f\"Invalid metrics: {metrics}. Allowed metrics are: {allowed}\")\n\n    img_hematoxylin, img_eosin, _ = hed_decompose(img, device=device)\n    eosin_mask = get_eosin_mask(img_eosin, device=device)\n    hematoxylin_mask = get_hematoxylin_mask(img_hematoxylin, eosin_mask, device=device)\n\n    # Mask out the cell objects\n    if label is not None:\n        if mask is not None:\n            label = label * (mask &gt; 0)\n        eosin_mask[label &gt; 0] = 0\n        hematoxylin_mask[label &gt; 0] = 0\n\n    # Compute features for each stain and channel\n    features = {}\n\n    for stain, mask, img_stain in [\n        (\"hematoxylin\", hematoxylin_mask, img_hematoxylin),\n        (\"eosin\", eosin_mask, img_eosin),\n    ]:\n        area = np.sum(mask)\n        features[f\"{stain}_area\"] = area\n\n        # Compute features for each RGB channel\n        for i, color in enumerate([\"R\", \"G\", \"B\"]):\n            if area &gt; 0:\n                pixels = img_stain[mask]\n                channel_vals = pixels[:, i]\n            else:\n                channel_vals = np.array([])  # Empty array for consistent handling\n\n            # Compute all requested features for this stain-channel combination\n            channel_features = _compute_intensity_feats(\n                channel_vals, metrics, quantiles, n_bins, hist_range\n            )\n\n            for i, met in enumerate(metrics):\n                features[f\"{stain}_{color}_{met}\"] = channel_features[i]\n\n    return pd.Series(features, dtype=np.float32)\n</code></pre>"},{"location":"api/transforms/apply_each/","title":"ApplyEach","text":"<p>               Bases: <code>BaseCompose</code></p>"},{"location":"api/transforms/apply_each/#histolytics.transforms.ApplyEach.__init__","title":"__init__","text":"<pre><code>__init__(transforms: List[OnlyInstMapTransform], p: float = 1.0, as_list: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Apply each transform to the input non-sequentially.</p> <p>Returns outputs for each transform.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>List[Any]</code> <p>List of transforms to apply.</p> required <code>p</code> <code>float, default=1.0</code> <p>Probability of applying the transform.</p> <code>1.0</code> <code>as_list</code> <code>bool, default=False</code> <p>Return the outputs as list with shapes (H, W, C).</p> <code>False</code>"},{"location":"api/transforms/apply_each/#histolytics.transforms.ApplyEach.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return a string representation of the class.</p>"},{"location":"api/transforms/binarize/","title":"BinarizeTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/binarize/#histolytics.transforms.BinarizeTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/binarize/#histolytics.transforms.BinarizeTransform.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Binarize instance labelled mask.</p>"},{"location":"api/transforms/binarize/#histolytics.transforms.BinarizeTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Generate a binary mask from instance labelled mask.</p> <p>Parameters:</p> Name Type Description Default <code>inst</code> <code>ndarray</code> <p>Instance labelled mask. Shape (H, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Binary mask. Shape: (H, W). Dtype: uint8</p>"},{"location":"api/transforms/cellpose/","title":"CellposeTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/cellpose/#histolytics.transforms.CellposeTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/cellpose/#histolytics.transforms.CellposeTransform.__init__","title":"__init__","text":"<pre><code>__init__(deduplicate: bool = True) -&gt; None\n</code></pre> <p>Generate flows from a heat diffused label mask.</p> <p>https://www.nature.com/articles/s41592-020-01018-x</p> <p>Parameters:</p> Name Type Description Default <code>deduplicate</code> <code>bool, default=True</code> <p>Whether to fix duplicate values in the mask before transforming. This adds overhead, so use only if you know that the mask has duplicates.</p> <code>True</code>"},{"location":"api/transforms/cellpose/#histolytics.transforms.CellposeTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Fix duplicate values and generate flows.</p> <p>Parameters:</p> Name Type Description Default <code>inst</code> <code>ndarray</code> <p>Instance labelled mask. Shape (H, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Horizontal and vertical flows of objects. Shape: (2, H, W). Dtype: float64.</p>"},{"location":"api/transforms/contour/","title":"ContourTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/contour/#histolytics.transforms.ContourTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/contour/#histolytics.transforms.ContourTransform.__init__","title":"__init__","text":"<pre><code>__init__(deduplicate: bool = True)\n</code></pre> <p>Generate contour map from a label mask.</p> <p>Parameters:</p> Name Type Description Default <code>deduplicate</code> <code>bool, default=True</code> <p>Whether to fix duplicate values in the mask before transforming. This adds overhead, so use only if you know that the mask has duplicates.</p> <code>True</code>"},{"location":"api/transforms/contour/#histolytics.transforms.ContourTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Generate contour transforms.</p> <p>Parameters     inst (np.ndarray):         Instance labelled mask. Shape (H, W).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Contour of objects. Shape: (H, W). Dtype: float64</p>"},{"location":"api/transforms/dist/","title":"DistTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/dist/#histolytics.transforms.DistTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/dist/#histolytics.transforms.DistTransform.__init__","title":"__init__","text":"<pre><code>__init__(deduplicate: bool = True) -&gt; None\n</code></pre> <p>Generate distance transforms from a label mask.</p> <p>Parameters:</p> Name Type Description Default <code>deduplicate</code> <code>bool, default=True</code> <p>Whether to fix duplicate values in the mask before transforming. This adds overhead, so use only if you know that the mask has duplicates.</p> <code>True</code>"},{"location":"api/transforms/dist/#histolytics.transforms.DistTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Generate distance transforms.</p> <p>Parameters     inst (np.ndarray):         Instance labelled mask. Shape: (H, W).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Distance transforms of objects. Shape: (H, W). Dtype: float64.</p>"},{"location":"api/transforms/edge_weight/","title":"EdgeWeightTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/edge_weight/#histolytics.transforms.EdgeWeightTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/edge_weight/#histolytics.transforms.EdgeWeightTransform.__init__","title":"__init__","text":"<pre><code>__init__(deduplicate: bool = True) -&gt; None\n</code></pre> <p>Generate weight maps for object boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>deduplicate</code> <code>bool, default=True</code> <p>Whether to fix duplicate values in the mask before transforming. This adds overhead, so use only if you know that the mask has duplicates.</p> <code>True</code>"},{"location":"api/transforms/edge_weight/#histolytics.transforms.EdgeWeightTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Generate edge weight transforms.</p> <p>Parameters     inst (np.ndarray):         Instance labelled mask. Shape (H, W).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Contour of objects. Shape: (H, W). Dtype: float64</p>"},{"location":"api/transforms/hovernet/","title":"HoverNetTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/hovernet/#histolytics.transforms.HoverNetTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/hovernet/#histolytics.transforms.HoverNetTransform.__init__","title":"__init__","text":"<pre><code>__init__(deduplicate: bool = True) -&gt; None\n</code></pre> <p>Generate horizontal and vertical gradients from a label mask.</p> <p>https://www.sciencedirect.com/science/article/pii/S1361841519301045</p> <p>Parameters:</p> Name Type Description Default <code>deduplicate</code> <code>bool, default=True</code> <p>Whether to fix duplicate values in the mask before transforming. This adds overhead, so use only if you know that the mask has duplicates.</p> <code>True</code>"},{"location":"api/transforms/hovernet/#histolytics.transforms.HoverNetTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Fix duplicate values and generate gradients.</p> <p>Parameters:</p> Name Type Description Default <code>inst</code> <code>ndarray</code> <p>Instance labelled mask. Shape (H, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Horizontal and vertical gradients of objects. Shape: (2, H, W). Dtype: float64.</p>"},{"location":"api/transforms/minmax/","title":"MinMaxNormalization","text":"<p>               Bases: <code>ImageOnlyTransform</code></p>"},{"location":"api/transforms/minmax/#histolytics.transforms.MinMaxNormalization.__init__","title":"__init__","text":"<pre><code>__init__(amin: float = 0.0, amax: float = 1.0, p: float = 1.0, copy: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Min-max normalization. Normalizes to range [amin, amax].</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float, default=1.0</code> <p>Probability of applying the transformation.</p> <code>1.0</code> <code>copy</code> <code>bool, default=False</code> <p>If True, normalize the copy of the input.</p> <code>False</code>"},{"location":"api/transforms/minmax/#histolytics.transforms.MinMaxNormalization.apply","title":"apply","text":"<pre><code>apply(image: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Apply min-max normalization.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image to be normalized. Shape (H, W, C)|(H, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized image. Same shape as input. dtype: float32.</p>"},{"location":"api/transforms/minmax/#histolytics.transforms.MinMaxNormalization.get_transform_init_args_names","title":"get_transform_init_args_names","text":"<pre><code>get_transform_init_args_names()\n</code></pre> <p>Get the names of the transformation arguments.</p>"},{"location":"api/transforms/norm/","title":"Normalization","text":"<p>               Bases: <code>ImageOnlyTransform</code></p>"},{"location":"api/transforms/norm/#histolytics.transforms.Normalization.__init__","title":"__init__","text":"<pre><code>__init__(mean: ndarray, std: ndarray, p: float = 1.0, copy: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Image level normalization transformation.</p> <p>NOTE: this is not dataset-level normalization but image-level.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>ndarray</code> <p>Mean values for each channel. Shape (C,)</p> required <code>std</code> <code>ndarray</code> <p>Standard deviation values for each channel. Shape (C,)</p> required <code>p</code> <code>float, default=1.0</code> <p>Probability of applying the transformation.</p> <code>1.0</code> <code>copy</code> <code>bool, default=False</code> <p>If True, normalize the copy of the input.</p> <code>False</code>"},{"location":"api/transforms/norm/#histolytics.transforms.Normalization.apply","title":"apply","text":"<pre><code>apply(image: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Apply image-level normalization to input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image to be normalized. Shape (H, W, C)|(H, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized image. Same shape as input. dtype: float32.</p>"},{"location":"api/transforms/norm/#histolytics.transforms.Normalization.get_transform_init_args_names","title":"get_transform_init_args_names","text":"<pre><code>get_transform_init_args_names()\n</code></pre> <p>Get the names of the transformation arguments.</p>"},{"location":"api/transforms/percentile/","title":"PercentileNormalization","text":"<p>               Bases: <code>ImageOnlyTransform</code></p>"},{"location":"api/transforms/percentile/#histolytics.transforms.PercentileNormalization.__init__","title":"__init__","text":"<pre><code>__init__(lower: float = 0.01, upper: float = 99.99, p: float = 1.0, copy: bool = False, **kwargs) -&gt; None\n</code></pre> <p>Percentile normalization. Normalizes to percentile range [lower, upper].</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>float, default=0.01</code> <p>Lower percentile.</p> <code>0.01</code> <code>upper</code> <code>float, default=99.99</code> <p>Clamp max value. No clamping performed if None.</p> <code>99.99</code> <code>p</code> <code>float, default=1.0</code> <p>Probability of applying the transformation.</p> <code>1.0</code> <code>copy</code> <code>bool, default=False</code> <p>If True, normalize the copy of the input.</p> <code>False</code>"},{"location":"api/transforms/percentile/#histolytics.transforms.PercentileNormalization.apply","title":"apply","text":"<pre><code>apply(image: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Apply percentile normalization to input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image to be normalized. Shape (H, W, C)|(H, W).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized image. Same shape as input. dtype: float32.</p>"},{"location":"api/transforms/percentile/#histolytics.transforms.PercentileNormalization.get_transform_init_args_names","title":"get_transform_init_args_names","text":"<pre><code>get_transform_init_args_names()\n</code></pre> <p>Get the names of the transformation arguments.</p>"},{"location":"api/transforms/smooth_dist/","title":"SmoothDistTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/smooth_dist/#histolytics.transforms.SmoothDistTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/smooth_dist/#histolytics.transforms.SmoothDistTransform.__init__","title":"__init__","text":"<pre><code>__init__(deduplicate: bool = True)\n</code></pre> <p>Generate FIM distance transforms from a label mask.</p> <p>https://www.biorxiv.org/content/10.1101/2021.11.03.467199v2</p> <p>Parameters:</p> Name Type Description Default <code>deduplicate</code> <code>bool, default=True</code> <p>Whether to fix duplicate values in the mask before transforming. This adds overhead, so use only if you know that the mask has duplicates.</p> <code>True</code>"},{"location":"api/transforms/smooth_dist/#histolytics.transforms.SmoothDistTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Generate smooth distance transforms.</p> <p>Parameters     inst (np.ndarray):         Instance labelled mask. Shape (H, W).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Smooth distance transforms of objects. Shape: (H, W). Dtype: float64.</p>"},{"location":"api/transforms/stardist/","title":"StardistTransform","text":"<p>               Bases: <code>OnlyInstMapTransform</code></p>"},{"location":"api/transforms/stardist/#histolytics.transforms.StardistTransform.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: set[str]\n</code></pre> <p>Returns set of available keys.</p>"},{"location":"api/transforms/stardist/#histolytics.transforms.StardistTransform.__init__","title":"__init__","text":"<pre><code>__init__(n_rays: int = 32, deduplicate: bool = True, **kwargs)\n</code></pre> <p>Generate radial distance maps from a label mask.</p> <p>https://arxiv.org/abs/1806.03535</p> <p>Parameters:</p> Name Type Description Default <code>n_rays</code> <code>int, default=32</code> <p>Number of rays used for computing distance maps.</p> <code>32</code> <code>deduplicate</code> <code>bool, default=True</code> <p>Whether to fix duplicate values in the mask before transforming. This adds overhead, so use only if you know that the mask has duplicates.</p> <code>True</code>"},{"location":"api/transforms/stardist/#histolytics.transforms.StardistTransform.__call__","title":"__call__","text":"<pre><code>__call__(inst: ndarray, **kwargs) -&gt; np.ndarray\n</code></pre> <p>Fix duplicate values and generate radial distance maps.</p> <p>Parameters     inst (np.ndarray):         Instance labelled mask. Shape (H, W).</p> <p>Returns     np.ndarray:         Pixelwise radial distance maps.         Shape: (n_rays, H, W). Dtype: float64.</p>"},{"location":"api/transforms/strong_augment/","title":"AlbuStrongAugment","text":"<p>               Bases: <code>BaseCompose</code></p>"},{"location":"api/transforms/strong_augment/#histolytics.transforms.AlbuStrongAugment.__init__","title":"__init__","text":"<pre><code>__init__(augment_space: Dict[str, tuple] = AUGMENT_SPACE, operations: Tuple[int] = (3, 4, 5), probabilites: Tuple[float] = (0.2, 0.3, 0.5), seed: Optional[int] = None, p=1.0) -&gt; None\n</code></pre> <p>Strong augment augmentation policy albumentations wrapper.</p> <p>Augment like there's no tomorrow: Consistently performing neural networks for medical imaging: https://arxiv.org/abs/2206.15274</p>"},{"location":"api/transforms/strong_augment/#histolytics.transforms.AlbuStrongAugment.__init__--parameters","title":"Parameters","text":"<pre><code>augment_space : Dict[str, tuple], default: AUGMENT_SPACE\n    Augmentation space to sample operations from.\noperations : Tuple[int], default: [3, 4, 5].\n    Number of operations to apply. If None, sample from\n    [1, len(augment_space)].\nprobabilites : Tuple[float], default: [0.2, 0.3, 0.5]\n    Probabilities of sampling operations. If None, sample from\n    the uniform distribution.\nseed : Optional[int], default: None\n    Random seed.\np : float, default: 1.0\n    Probability of applying the transform.\n</code></pre>"},{"location":"api/transforms/strong_augment/#histolytics.transforms.AlbuStrongAugment.__call__","title":"__call__","text":"<pre><code>__call__(*args, force_apply: bool = False, **data) -&gt; Dict[str, Any]\n</code></pre> <p>Apply the StrongAugment transformation pipeline.</p>"},{"location":"api/transforms/strong_augment/#histolytics.transforms.AlbuStrongAugment.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return the string representation of the StrongAugment object.</p>"},{"location":"api/utils/draw_thing_contours/","title":"draw_thing_contours","text":"<p>Overlay coloured contours on a background image from an instance labelled raster mask.</p> Note <p>If a semantic <code>type_map</code> is provided, the contours will be coloured according to the type.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Original image. Shape (H, W, 3).</p> required <code>inst_map</code> <code>ndarray</code> <p>Instance segmentation map. Shape (H, W).</p> required <code>type_map</code> <code>ndarray</code> <p>Semantic segmentation map. Shape (H, W).</p> required <code>thickness</code> <code>int</code> <p>Thickness of the contour lines</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The contours overlaid on top of original image. Shape: (H, W, 3).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from histolytics.utils.plot import draw_thing_contours\n&gt;&gt;&gt; from histolytics.data import (\n...     hgsc_cancer_he,\n...     hgsc_cancer_inst_mask,\n...     hgsc_cancer_type_mask,\n... )\n&gt;&gt;&gt; # Load the HE image, instance mask and type mask\n&gt;&gt;&gt; he_image = hgsc_cancer_he()\n&gt;&gt;&gt; inst_mask = hgsc_cancer_inst_mask()\n&gt;&gt;&gt; type_mask = hgsc_cancer_type_mask()\n&gt;&gt;&gt; # Draw contours of the instance segmentation mask\n&gt;&gt;&gt; overlay = draw_thing_contours(\n...     he_image,\n...     inst_mask,\n...     type_mask,\n...     thickness=2,\n... )\n&gt;&gt;&gt; # Display the overlay\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(5, 5))\n&gt;&gt;&gt; ax.imshow(overlay)\n&gt;&gt;&gt; ax.set_axis_off()\n</code></pre> <p></p> Source code in <code>src/histolytics/utils/plot.py</code> <pre><code>def draw_thing_contours(\n    image: np.ndarray,\n    inst_map: np.ndarray,\n    type_map: np.ndarray,\n    thickness: int = 2,\n) -&gt; np.ndarray:\n    \"\"\"Overlay coloured contours on a background image from an instance labelled raster mask.\n\n    Note:\n        If a semantic `type_map` is provided, the contours will be coloured according to the type.\n\n    Parameters:\n        image (np.ndarray):\n            Original image. Shape (H, W, 3).\n        inst_map (np.ndarray):\n            Instance segmentation map. Shape (H, W).\n        type_map (np.ndarray):\n            Semantic segmentation map. Shape (H, W).\n        thickness (int):\n            Thickness of the contour lines\n\n    Returns:\n        np.ndarray:\n            The contours overlaid on top of original image. Shape: (H, W, 3).\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from histolytics.utils.plot import draw_thing_contours\n        &gt;&gt;&gt; from histolytics.data import (\n        ...     hgsc_cancer_he,\n        ...     hgsc_cancer_inst_mask,\n        ...     hgsc_cancer_type_mask,\n        ... )\n        &gt;&gt;&gt; # Load the HE image, instance mask and type mask\n        &gt;&gt;&gt; he_image = hgsc_cancer_he()\n        &gt;&gt;&gt; inst_mask = hgsc_cancer_inst_mask()\n        &gt;&gt;&gt; type_mask = hgsc_cancer_type_mask()\n        &gt;&gt;&gt; # Draw contours of the instance segmentation mask\n        &gt;&gt;&gt; overlay = draw_thing_contours(\n        ...     he_image,\n        ...     inst_mask,\n        ...     type_mask,\n        ...     thickness=2,\n        ... )\n        &gt;&gt;&gt; # Display the overlay\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(5, 5))\n        &gt;&gt;&gt; ax.imshow(overlay)\n        &gt;&gt;&gt; ax.set_axis_off()\n    ![out](../../img/overlay.png)\n    \"\"\"\n    bg = np.copy(image)\n\n    shape = inst_map.shape[:2]\n    nuc_list = list(np.unique(inst_map))\n\n    if 0 in nuc_list:\n        nuc_list.remove(0)  # 0 is background\n\n    for _, nuc_id in enumerate(nuc_list):\n        inst = np.array(inst_map == nuc_id, np.uint8)\n\n        y1, y2, x1, x2 = bounding_box(inst)\n        y1 = y1 - 2 if y1 - 2 &gt;= 0 else y1\n        x1 = x1 - 2 if x1 - 2 &gt;= 0 else x1\n        x2 = x2 + 2 if x2 + 2 &lt;= shape[1] - 1 else x2\n        y2 = y2 + 2 if y2 + 2 &lt;= shape[0] - 1 else y2\n\n        inst_crop = inst[y1:y2, x1:x2]\n        inst_bg_crop = bg[y1:y2, x1:x2]\n        contours = cv2.findContours(inst_crop, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[\n            0\n        ]\n\n        type_crop = type_map[y1:y2, x1:x2]\n        type = np.unique(type_crop[inst_crop &gt; 0])[0]\n        inst_color = NUM_COLORS[type]\n\n        cv2.drawContours(\n            inst_bg_crop,\n            contours,\n            contourIdx=-1,\n            color=inst_color,\n            thickness=thickness,\n        )\n\n        bg[y1:y2, x1:x2] = inst_bg_crop\n\n    return bg\n</code></pre>"},{"location":"api/utils/gdf2inst/","title":"gdf2inst","text":"<p>Converts a GeoDataFrame to an instance segmentation raster mask.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame to convert to an instance segmentation mask.</p> required <code>xoff</code> <code>int</code> <p>X offset. This is used to translate the geometries in the GeoDataFrame to burn the geometries in correctly to the raster mask.</p> <code>0</code> <code>yoff</code> <code>int</code> <p>Y offset. This is used to translate the geometries in the GeoDataFrame to burn the geometries in correctly to the raster mask.</p> <code>0</code> <code>width</code> <code>int</code> <p>Width of the output. This should match with the underlying image width. If None, the width will be calculated from the input gdf.</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the output. This should match with the underlying image height. If None, the height will be calculated from the input gdf.</p> <code>None</code> <code>reset_index</code> <code>bool</code> <p>Whether to reset the index of the output GeoDataFrame.</p> <code>False</code> <code>id_col</code> <code>str</code> <p>If provided, the column name to use for the instance IDs. If None, the index of the GeoDataFrame will be used as instance IDs. Ignored if <code>reset_index</code> is set to True.</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>np.ndarray: Instance segmentation mask of the input gdf. Shape (height, width).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n&gt;&gt;&gt; from skimage.measure import label\n&gt;&gt;&gt; from skimage.color import label2rgb\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt;\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; # Convert the GeoDataFrame to an instance segmentation raster\n&gt;&gt;&gt; nuc_raster = gdf2inst(nuc, xoff=0, yoff=0, width=1500, height=1500)\n&gt;&gt;&gt; # Visualize the instance segmentation raster and the GeoDataFrame\n&gt;&gt;&gt; fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n&gt;&gt;&gt; ax[0].imshow(label2rgb(label(nuc_raster), bg_label=0))\n&gt;&gt;&gt; ax[0].set_axis_off()\n&gt;&gt;&gt; nuc.plot(column=\"class_name\", ax=ax[1])\n&gt;&gt;&gt; ax[1].set_axis_off()\n&gt;&gt;&gt; fig.tight_layout()\n</code></pre> <p></p> Source code in <code>src/histolytics/utils/raster.py</code> <pre><code>def gdf2inst(\n    gdf: gpd.GeoDataFrame,\n    xoff: int = 0,\n    yoff: int = 0,\n    width: int = None,\n    height: int = None,\n    reset_index: bool = False,\n    id_col: str = None,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Converts a GeoDataFrame to an instance segmentation raster mask.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            GeoDataFrame to convert to an instance segmentation mask.\n        xoff (int):\n            X offset. This is used to translate the geometries in the GeoDataFrame to\n            burn the geometries in correctly to the raster mask.\n        yoff (int):\n            Y offset. This is used to translate the geometries in the GeoDataFrame to\n            burn the geometries in correctly to the raster mask.\n        width (int):\n            Width of the output. This should match with the underlying image width.\n            If None, the width will be calculated from the input gdf.\n        height (int):\n            Height of the output. This should match with the underlying image height.\n            If None, the height will be calculated from the input gdf.\n        reset_index (bool):\n            Whether to reset the index of the output GeoDataFrame.\n        id_col (str):\n            If provided, the column name to use for the instance IDs. If None, the index\n            of the GeoDataFrame will be used as instance IDs. Ignored if `reset_index`\n            is set to True.\n\n    Returns:\n        np.ndarray:\n            Instance segmentation mask of the input gdf. Shape (height, width).\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.raster import gdf2inst\n        &gt;&gt;&gt; from skimage.measure import label\n        &gt;&gt;&gt; from skimage.color import label2rgb\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; # Convert the GeoDataFrame to an instance segmentation raster\n        &gt;&gt;&gt; nuc_raster = gdf2inst(nuc, xoff=0, yoff=0, width=1500, height=1500)\n        &gt;&gt;&gt; # Visualize the instance segmentation raster and the GeoDataFrame\n        &gt;&gt;&gt; fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n        &gt;&gt;&gt; ax[0].imshow(label2rgb(label(nuc_raster), bg_label=0))\n        &gt;&gt;&gt; ax[0].set_axis_off()\n        &gt;&gt;&gt; nuc.plot(column=\"class_name\", ax=ax[1])\n        &gt;&gt;&gt; ax[1].set_axis_off()\n        &gt;&gt;&gt; fig.tight_layout()\n    ![out](../../img/gdf2inst.png)\n    \"\"\"\n    if gdf.empty:\n        try:\n            return np.zeros((int(height), int(width)), dtype=np.int32)\n        except TypeError:\n            raise TypeError(\n                \"Input gdf is empty, trying to return an empty mask but height and width\"\n                \" are not provided. Cannot infer the output shape.\"\n            )\n\n    xmin, ymin, xmax, ymax = gdf.total_bounds\n    xoff = xoff - xmin\n    yoff = yoff - ymin\n\n    if width is None:\n        width = int(xmax - xmin)\n    if height is None:\n        height = int(ymax - ymin)\n\n    geoms = gdf.geometry.translate(xoff=-xmin - xoff, yoff=-ymin - yoff)\n\n    if reset_index:\n        labels = range(len(gdf))\n    else:\n        if id_col is not None:\n            labels = gdf[id_col].values\n        else:\n            labels = gdf.index.values\n\n    image_shape = (int(height), int(width))\n    shapes = list(zip(geoms, labels))\n    out_mask = rasterize(\n        shapes,\n        out_shape=image_shape,\n        fill=0,\n        dtype=np.int32,\n    )\n\n    return out_mask\n</code></pre>"},{"location":"api/utils/gdf2sem/","title":"gdf2sem","text":"<p>Converts a GeoDataFrame to a semantic segmentation raster mask.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with a \"class_name\" column.</p> required <code>xoff</code> <code>int</code> <p>X offset. This is used to translate the geometries in the GeoDataFrame to burn the geometries in correctly to the raster mask.</p> <code>0</code> <code>yoff</code> <code>int</code> <p>Y offset. This is used to translate the geometries in the GeoDataFrame to burn the geometries in correctly to the raster mask.</p> <code>0</code> <code>class_dict</code> <code>Dict[str, int], default=None</code> <p>Dictionary mapping class names to integers. e.g. {\"neoplastic\":1, \"immune\":2} If None, the classes will be mapped to integers in the order they appear in the GeoDataFrame.</p> <code>None</code> <code>width</code> <code>int</code> <p>Width of the output. This should match with the underlying image width. If None, the width will be calculated from the input gdf.</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the output. This should match with the underlying image height. If None, the height will be calculated from the input gdf.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Semantic segmentation mask of the input gdf.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.raster import gdf2sem\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from skimage.measure import label\n&gt;&gt;&gt; from skimage.color import label2rgb\n&gt;&gt;&gt;\n&gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n&gt;&gt;&gt; # Convert the GeoDataFrame to an instance segmentation raster\n&gt;&gt;&gt; nuc_raster = gdf2sem(nuc, xoff=0, yoff=0, width=1500, height=1500)\n&gt;&gt;&gt; # Visualize the semantic segmentation raster and the GeoDataFrame\n&gt;&gt;&gt; fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n&gt;&gt;&gt; ax[0].imshow(label2rgb(nuc_raster, bg_label=0))\n&gt;&gt;&gt; ax[0].set_axis_off()\n&gt;&gt;&gt; nuc.plot(column=\"class_name\", ax=ax[1])\n&gt;&gt;&gt; ax[1].set_axis_off()\n&gt;&gt;&gt; fig.tight_layout()\n</code></pre> <p></p> Source code in <code>src/histolytics/utils/raster.py</code> <pre><code>def gdf2sem(\n    gdf: gpd.GeoDataFrame,\n    xoff: int = 0,\n    yoff: int = 0,\n    class_dict: Dict[str, int] = None,\n    width: int = None,\n    height: int = None,\n) -&gt; np.ndarray:\n    \"\"\"Converts a GeoDataFrame to a semantic segmentation raster mask.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            GeoDataFrame with a \"class_name\" column.\n        xoff (int):\n            X offset. This is used to translate the geometries in the GeoDataFrame to\n            burn the geometries in correctly to the raster mask.\n        yoff (int):\n            Y offset. This is used to translate the geometries in the GeoDataFrame to\n            burn the geometries in correctly to the raster mask.\n        class_dict (Dict[str, int], default=None):\n            Dictionary mapping class names to integers. e.g. {\"neoplastic\":1, \"immune\":2}\n            If None, the classes will be mapped to integers in the order they appear in\n            the GeoDataFrame.\n        width (int):\n            Width of the output. This should match with the underlying image width.\n            If None, the width will be calculated from the input gdf.\n        height (int):\n            Height of the output. This should match with the underlying image height.\n            If None, the height will be calculated from the input gdf.\n\n    Returns:\n        np.ndarray:\n            Semantic segmentation mask of the input gdf.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.raster import gdf2sem\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; from skimage.measure import label\n        &gt;&gt;&gt; from skimage.color import label2rgb\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; nuc = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; # Convert the GeoDataFrame to an instance segmentation raster\n        &gt;&gt;&gt; nuc_raster = gdf2sem(nuc, xoff=0, yoff=0, width=1500, height=1500)\n        &gt;&gt;&gt; # Visualize the semantic segmentation raster and the GeoDataFrame\n        &gt;&gt;&gt; fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n        &gt;&gt;&gt; ax[0].imshow(label2rgb(nuc_raster, bg_label=0))\n        &gt;&gt;&gt; ax[0].set_axis_off()\n        &gt;&gt;&gt; nuc.plot(column=\"class_name\", ax=ax[1])\n        &gt;&gt;&gt; ax[1].set_axis_off()\n        &gt;&gt;&gt; fig.tight_layout()\n    ![out](../../img/gdf2sem.png)\n    \"\"\"\n    if gdf.empty:\n        try:\n            return np.zeros((int(height), int(width)), dtype=np.int32)\n        except TypeError:\n            raise TypeError(\n                \"Input gdf is empty, trying to return an empty mask but height and width\"\n                \" are not provided. Cannot infer the output shape.\"\n            )\n\n    xmin, ymin, xmax, ymax = gdf.total_bounds\n    xoff = xoff - xmin\n    yoff = yoff - ymin\n\n    if width is None:\n        width = int(xmax - xmin)\n    if height is None:\n        height = int(ymax - ymin)\n\n    # Translate geometries to the correct position\n    geoms = gdf.geometry.translate(xoff=-xmin - xoff, yoff=-ymin - yoff)\n\n    # Map class names to integer labels\n    if class_dict is None:\n        labels = gdf[\"class_name\"].astype(\"category\").cat.codes + 1\n    else:\n        labels = gdf[\"class_name\"].map(class_dict).astype(np.int32)\n\n    image_shape = (int(height), int(width))\n    shapes = list(zip(geoms, labels))\n    out_mask = rasterize(\n        shapes,\n        out_shape=image_shape,\n        fill=0,\n        dtype=np.int32,\n    )\n\n    return out_mask\n</code></pre>"},{"location":"api/utils/gdf_apply/","title":"gdf_apply","text":"<p>Apply or parallel apply a function to any col or row of a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>gpd.GeoDataFramt an semantic e</code> <p>Input GeoDataFrame.</p> required <code>func</code> <code>Callable</code> <p>A callable function.</p> required <code>axis</code> <code>int</code> <p>The gdf axis to apply the function on.axis=1 means rowise. axis=0 means columnwise.</p> <code>1</code> <code>parallel</code> <code>bool</code> <p>Flag, whether to parallelize the operation with <code>pandarallel</code>.</p> <code>True</code> <code>num_processes</code> <code>int</code> <p>The number of processes to use when parallel=True. If -1, this will use all available cores.</p> <code>-1</code> <code>pbar</code> <code>bool</code> <p>Show progress bar when executing in parallel mode. Ignored if <code>parallel=False</code>.</p> <code>False</code> <code>columns</code> <code>Optional[Tuple[str, ...]]</code> <p>A tuple of column names to apply the function on. If None, this will apply the function to all columns.</p> <code>None</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Arbitrary keyword args for the <code>func</code> callable.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoSeries</code> <p>gpd.GeoSeries: A GeoSeries object containing the computed values for each row or col in the input gdf.</p> <p>Examples:</p> <p>Get the compactness of the polygons in a gdf</p> <pre><code>&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from histolytics.utils.gdf import gdf_apply\n&gt;&gt;&gt; from histolytics.spatial_geom.morphometrics import compactness\n&gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n&gt;&gt;&gt; gdf[\"compactness\"] = gdf_apply(\n...     gdf, compactness, columns=[\"geometry\"], parallel=True, num_processes=3\n... )\n                                                geometry  class_name  compactness\n    0  POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ...  connective     0.578699\n    1  POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ...  connective     0.947018\n    2  POLYGON ((1382.99 156.01, 1380 156.01, 1376.01...  connective     0.604828\n</code></pre> Source code in <code>src/histolytics/utils/gdf.py</code> <pre><code>def gdf_apply(\n    gdf: gpd.GeoDataFrame,\n    func: Callable,\n    axis: int = 1,\n    parallel: bool = True,\n    num_processes: Optional[int] = -1,\n    pbar: bool = False,\n    columns: Optional[Tuple[str, ...]] = None,\n    **kwargs,\n) -&gt; gpd.GeoSeries:\n    \"\"\"Apply or parallel apply a function to any col or row of a GeoDataFrame.\n\n    Parameters:\n        gdf (gpd.GeoDataFramt an semantic e):\n            Input GeoDataFrame.\n        func (Callable):\n            A callable function.\n        axis (int):\n            The gdf axis to apply the function on.axis=1 means rowise. axis=0\n            means columnwise.\n        parallel (bool):\n            Flag, whether to parallelize the operation with `pandarallel`.\n        num_processes (int):\n            The number of processes to use when parallel=True. If -1,\n            this will use all available cores.\n        pbar (bool):\n            Show progress bar when executing in parallel mode. Ignored if\n            `parallel=False`.\n        columns (Optional[Tuple[str, ...]]):\n            A tuple of column names to apply the function on. If None,\n            this will apply the function to all columns.\n        **kwargs (Dict[str, Any]): Arbitrary keyword args for the `func` callable.\n\n    Returns:\n        gpd.GeoSeries:\n            A GeoSeries object containing the computed values for each\n            row or col in the input gdf.\n\n    Examples:\n        Get the compactness of the polygons in a gdf\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from histolytics.utils.gdf import gdf_apply\n        &gt;&gt;&gt; from histolytics.spatial_geom.morphometrics import compactness\n        &gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; gdf[\"compactness\"] = gdf_apply(\n        ...     gdf, compactness, columns=[\"geometry\"], parallel=True, num_processes=3\n        ... )\n                                                        geometry  class_name  compactness\n            0  POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ...  connective     0.578699\n            1  POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ...  connective     0.947018\n            2  POLYGON ((1382.99 156.01, 1380 156.01, 1376.01...  connective     0.604828\n    \"\"\"\n    if columns is not None:\n        if not isinstance(columns, (tuple, list)):\n            raise ValueError(f\"columns must be a tuple or list, got {type(columns)}\")\n        gdf = gdf[columns]\n\n    if not parallel:\n        res = gdf.apply(lambda x: func(*x, **kwargs), axis=axis)\n    else:\n        cpus = psutil.cpu_count(logical=False) if num_processes == -1 else num_processes\n        pandarallel.initialize(verbose=1, progress_bar=pbar, nb_workers=cpus)\n        res = gdf.parallel_apply(lambda x: func(*x, **kwargs), axis=axis)\n\n    return res\n</code></pre>"},{"location":"api/utils/gdf_to_polars/","title":"gdf_to_polars","text":"<p>Convert a GeoDataFrame to a polars DataFrame while preserving Shapely geometries.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame The input GeoDataFrame</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If polars is not installed.</p> <p>Returns:</p> Type Description <p>pl.DataFrame: with Shapely objects preserved as Python objects</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import gdf_to_polars\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n&gt;&gt;&gt; gdf_pl = gdf_to_polars(gdf)\n&gt;&gt;&gt; print(gdf_pl.head(3))\n    shape: (3, 2)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 class_name \u2506 geometry                        \u2502\n    \u2502 ---        \u2506 ---                             \u2502\n    \u2502 str        \u2506 object                          \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 connective \u2506 POLYGON ((1394.01 0, 1395.01 1\u2026 \u2502\n    \u2502 connective \u2506 POLYGON ((1391 2.01, 1387 2.01\u2026 \u2502\n    \u2502 connective \u2506 POLYGON ((1382.99 156.01, 1380\u2026 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/histolytics/utils/gdf.py</code> <pre><code>def gdf_to_polars(gdf: gpd.GeoDataFrame):\n    \"\"\"Convert a GeoDataFrame to a polars DataFrame while preserving Shapely geometries.\n\n    Parameters:\n        gdf: geopandas.GeoDataFrame\n            The input GeoDataFrame\n\n    Raises:\n        ImportError: If polars is not installed.\n\n    Returns:\n        pl.DataFrame: with Shapely objects preserved as Python objects\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.gdf import gdf_to_polars\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; gdf_pl = gdf_to_polars(gdf)\n        &gt;&gt;&gt; print(gdf_pl.head(3))\n            shape: (3, 2)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 class_name \u2506 geometry                        \u2502\n            \u2502 ---        \u2506 ---                             \u2502\n            \u2502 str        \u2506 object                          \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 connective \u2506 POLYGON ((1394.01 0, 1395.01 1\u2026 \u2502\n            \u2502 connective \u2506 POLYGON ((1391 2.01, 1387 2.01\u2026 \u2502\n            \u2502 connective \u2506 POLYGON ((1382.99 156.01, 1380\u2026 \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    try:\n        import polars as pl\n    except ImportError:\n        raise ImportError(\n            \"polars is not installed. Please install it with `pip install polars`.\"\n        )\n\n    # First convert to pandas\n    pdf = pd.DataFrame(gdf)\n\n    # Identify columns containing Shapely objects\n    geometry_cols = []\n    for col in pdf.columns:\n        if len(pdf) &gt; 0:\n            shapely_modules = (\n                \"shapely.geometry.point\",\n                \"shapely.geometry.polygon\",\n                \"shapely.geometry.linestring\",\n                \"shapely.geometry.multipoint\",\n                \"shapely.geometry.multipolygon\",\n                \"shapely.geometry.multilinestring\",\n                \"shapely.geometry.collection\",\n            )\n            if (\n                getattr(pdf[col].iloc[0], \"__class__\", None)\n                and getattr(pdf[col].iloc[0].__class__, \"__module__\", None)\n                in shapely_modules\n            ):\n                # If the column contains Shapely objects, we will treat it as a geometry column\n                # and store it as a Python object in polars\n                geometry_cols.append(col)\n\n    # Convert to polars with all columns as objects initially\n    pl_df = pl.from_pandas(\n        pdf[[col for col in pdf.columns if col not in geometry_cols]]\n    )\n\n    # For geometry columns, ensure they're stored as Python objects\n    # Add geometry columns as Python objects to the polars DataFrame\n    for col in geometry_cols:\n        pl_df = pl_df.with_columns(pl.Series(col, pdf[col].tolist(), dtype=pl.Object))\n    return pl_df\n</code></pre>"},{"location":"api/utils/get_centroid_numpy/","title":"get_centroid_numpy","text":"<p>Get the centroid coordinates of a GeoDataFrame as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of shape (n, 2) containing the centroid coordinates of each geometry in the GeoDataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import get_centroid_numpy\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n&gt;&gt;&gt; centroids = get_centroid_numpy(gdf)\n&gt;&gt;&gt; print(centroids)\n    [[1400.03798043    1.69248393]\n    [1386.45857876    9.58076168]\n    [1378.29668867  170.69547823]\n    ...\n    [ 847.54653982  425.80712554]\n    [ 954.08683652  520.35605096]\n    [ 784.46362434  483.4973545 ]]\n</code></pre> Source code in <code>src/histolytics/utils/gdf.py</code> <pre><code>def get_centroid_numpy(gdf: gpd.GeoDataFrame) -&gt; np.ndarray:\n    \"\"\"Get the centroid coordinates of a GeoDataFrame as a numpy array.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            Input GeoDataFrame.\n\n    Returns:\n        np.ndarray:\n            A numpy array of shape (n, 2) containing the centroid coordinates\n            of each geometry in the GeoDataFrame.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.gdf import get_centroid_numpy\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; centroids = get_centroid_numpy(gdf)\n        &gt;&gt;&gt; print(centroids)\n            [[1400.03798043    1.69248393]\n            [1386.45857876    9.58076168]\n            [1378.29668867  170.69547823]\n            ...\n            [ 847.54653982  425.80712554]\n            [ 954.08683652  520.35605096]\n            [ 784.46362434  483.4973545 ]]\n    \"\"\"\n    return shapely.get_coordinates(gdf.centroid)\n</code></pre>"},{"location":"api/utils/get_eosin_mask/","title":"get_eosin_mask","text":"<p>Get the binary eosin mask from the eosin channel.</p> <p>Parameters:</p> Name Type Description Default <code>img_eosin</code> <code>ndarray</code> <p>The eosin channel. Shape (H, W, 3).</p> required <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The binary eosin mask. Shape (H, W).</p> Source code in <code>src/histolytics/utils/im.py</code> <pre><code>def get_eosin_mask(img_eosin: np.ndarray, device: str = \"cpu\") -&gt; np.ndarray:\n    \"\"\"Get the binary eosin mask from the eosin channel.\n\n    Parameters:\n        img_eosin (np.ndarray):\n            The eosin channel. Shape (H, W, 3).\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'.\n\n    Returns:\n        np.ndarray:\n            The binary eosin mask. Shape (H, W).\n    \"\"\"\n    if device == \"cuda\" and _has_cp:\n        return _get_eosin_mask_cp(img_eosin)\n    else:\n        return _get_eosin_mask_np(img_eosin)\n</code></pre>"},{"location":"api/utils/get_hematoxylin_mask/","title":"get_hematoxylin_mask","text":"<p>Get the binary hematoxylin mask from the hematoxylin channel.</p> <p>Parameters:</p> Name Type Description Default <code>img_hematoxylin</code> <code>ndarray</code> <p>The hematoxylin channel. Shape (H, W, 3).</p> required <code>eosin_mask</code> <code>ndarray</code> <p>The eosin mask. Shape (H, W).</p> required <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The binary hematoxylin mask. Shape (H, W).</p> Source code in <code>src/histolytics/utils/im.py</code> <pre><code>def get_hematoxylin_mask(\n    img_hematoxylin: np.ndarray, eosin_mask: np.ndarray, device: str = \"cpu\"\n) -&gt; np.ndarray:\n    \"\"\"Get the binary hematoxylin mask from the hematoxylin channel.\n\n    Parameters:\n        img_hematoxylin (np.ndarray):\n            The hematoxylin channel. Shape (H, W, 3).\n        eosin_mask (np.ndarray):\n            The eosin mask. Shape (H, W).\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'.\n\n    Returns:\n        np.ndarray:\n            The binary hematoxylin mask. Shape (H, W).\n    \"\"\"\n    if device == \"cuda\" and _has_cp:\n        return _get_hematoxylin_mask_cp(img_hematoxylin, eosin_mask)\n    else:\n        return _get_hematoxylin_mask_np(img_hematoxylin, eosin_mask)\n</code></pre>"},{"location":"api/utils/hed_decompose/","title":"hed_decompose","text":"<p>Transform an image to HED space and return the 3 channels.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The input image. Shape (H, W, 3).</p> required <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: The H, E, D channels.</p> Source code in <code>src/histolytics/utils/im.py</code> <pre><code>def hed_decompose(\n    img: np.ndarray, device: str = \"cpu\"\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Transform an image to HED space and return the 3 channels.\n\n    Parameters:\n        img (np.ndarray):\n            The input image. Shape (H, W, 3).\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: The H, E, D channels.\n    \"\"\"\n    if device == \"cuda\" and _has_cp:\n        return _hed_decompose_cp(img)\n    else:\n        return _hed_decompose_np(img)\n</code></pre>"},{"location":"api/utils/inst2gdf/","title":"inst2gdf","text":"<p>Convert an instance segmentation raster mask to a GeoDataFrame.</p> Note <p>This function should be applied to nuclei instance segmentation masks. Nuclei types can be provided with the <code>type_map</code> and <code>class_dict</code> arguments if needed.</p> <p>Parameters:</p> Name Type Description Default <code>inst_map</code> <code>ndarray</code> <p>An instance segmentation mask. Shape (H, W).</p> required <code>type_map</code> <code>ndarray</code> <p>A type segmentation mask. Shape (H, W). If provided, the types will be included in the resulting GeoDataFrame in column 'class_name'.</p> <code>None</code> <code>xoff</code> <code>int</code> <p>The x offset. Optional. The offset is used to translate the geometries in the GeoDataFrame. If None, no translation is applied.</p> <code>None</code> <code>yoff</code> <code>int</code> <p>The y offset. Optional. The offset is used to translate the geometries in the GeoDataFrame. If None, no translation is applied.</p> <code>None</code> <code>class_dict</code> <code>Dict[int, str]</code> <p>A dictionary mapping class indices to class names. e.g. {1: 'neoplastic', 2: 'immune'}. If None, the class indices will be used.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>The minimum size (in pixels) of the polygons to include in the GeoDataFrame.</p> <code>15</code> <code>smooth_func</code> <code>Callable</code> <p>A function to smooth the polygons. The function should take a shapely Polygon as input and return a shapely Polygon. Defaults to <code>uniform_smooth</code>, which applies a uniform filter. <code>histolytics.utils._filters</code> also provides <code>gaussian_smooth</code> and <code>median_smooth</code> for smoothing.</p> <code>uniform_smooth</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame of the raster instance mask. Contains columns:</p> <pre><code>- 'id' - the numeric pixel value of the instance mask,\n- 'class_name' - the name or index of the instance class (requires `type_map` and `class_dict`),\n- 'geometry' - the geometry of the polygon.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.raster import inst2gdf\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_inst_mask, hgsc_cancer_type_mask\n&gt;&gt;&gt; # load raster masks\n&gt;&gt;&gt; inst_mask = hgsc_cancer_inst_mask()\n&gt;&gt;&gt; type_mask = hgsc_cancer_type_mask()\n&gt;&gt;&gt; # convert to GeoDataFrame\n&gt;&gt;&gt; gdf = inst2gdf(inst_mask, type_mask)\n&gt;&gt;&gt; print(gdf.head(3))\n        uid  class_name                                           geometry\n    0  135           1  POLYGON ((405.019 0.45, 405.43 1.58, 406.589 2...\n    1  200           1  POLYGON ((817.01 0.225, 817.215 0.804, 817.795...\n    2    0           1  POLYGON ((1394.01 0.45, 1394.215 1.58, 1394.79...\n</code></pre> Source code in <code>src/histolytics/utils/raster.py</code> <pre><code>def inst2gdf(\n    inst_map: np.ndarray,\n    type_map: np.ndarray = None,\n    xoff: int = None,\n    yoff: int = None,\n    class_dict: Dict[int, str] = None,\n    min_size: int = 15,\n    smooth_func: Callable = uniform_smooth,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert an instance segmentation raster mask to a GeoDataFrame.\n\n    Note:\n        This function should be applied to nuclei instance segmentation masks. Nuclei\n        types can be provided with the `type_map` and `class_dict` arguments if needed.\n\n    Parameters:\n        inst_map (np.ndarray):\n            An instance segmentation mask. Shape (H, W).\n        type_map (np.ndarray):\n            A type segmentation mask. Shape (H, W). If provided, the types will be\n            included in the resulting GeoDataFrame in column 'class_name'.\n        xoff (int):\n            The x offset. Optional. The offset is used to translate the geometries\n            in the GeoDataFrame. If None, no translation is applied.\n        yoff (int):\n            The y offset. Optional. The offset is used to translate the geometries\n            in the GeoDataFrame. If None, no translation is applied.\n        class_dict (Dict[int, str]):\n            A dictionary mapping class indices to class names.\n            e.g. {1: 'neoplastic', 2: 'immune'}. If None, the class indices will be used.\n        min_size (int):\n            The minimum size (in pixels) of the polygons to include in the GeoDataFrame.\n        smooth_func (Callable):\n            A function to smooth the polygons. The function should take a shapely Polygon\n            as input and return a shapely Polygon. Defaults to `uniform_smooth`, which\n            applies a uniform filter. `histolytics.utils._filters` also provides\n            `gaussian_smooth` and `median_smooth` for smoothing.\n\n    returns:\n        gpd.GeoDataFrame:\n            A GeoDataFrame of the raster instance mask. Contains columns:\n\n                - 'id' - the numeric pixel value of the instance mask,\n                - 'class_name' - the name or index of the instance class (requires `type_map` and `class_dict`),\n                - 'geometry' - the geometry of the polygon.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.raster import inst2gdf\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_inst_mask, hgsc_cancer_type_mask\n        &gt;&gt;&gt; # load raster masks\n        &gt;&gt;&gt; inst_mask = hgsc_cancer_inst_mask()\n        &gt;&gt;&gt; type_mask = hgsc_cancer_type_mask()\n        &gt;&gt;&gt; # convert to GeoDataFrame\n        &gt;&gt;&gt; gdf = inst2gdf(inst_mask, type_mask)\n        &gt;&gt;&gt; print(gdf.head(3))\n                uid  class_name                                           geometry\n            0  135           1  POLYGON ((405.019 0.45, 405.43 1.58, 406.589 2...\n            1  200           1  POLYGON ((817.01 0.225, 817.215 0.804, 817.795...\n            2    0           1  POLYGON ((1394.01 0.45, 1394.215 1.58, 1394.79...\n    \"\"\"\n    # handle empty masks\n    if inst_map.size == 0 or np.max(inst_map) == 0:\n        return gpd.GeoDataFrame(columns=[\"uid\", \"class_name\", \"geometry\"])\n\n    if type_map is None:\n        type_map = inst_map &gt; 0\n\n    types = np.unique(type_map)[1:]\n\n    if class_dict is None:\n        class_dict = {int(i): int(i) for i in types}\n\n    inst_maps_per_type = []\n    for t in types:\n        mask = type_map == t\n        vectorized_data = (\n            (value, class_dict[int(t)], shape(polygon))\n            for polygon, value in shapes(inst_map, mask=mask)\n        )\n\n        res = gpd.GeoDataFrame(\n            vectorized_data,\n            columns=[\"uid\", \"class_name\", \"geometry\"],\n        )\n        res[\"uid\"] = res[\"uid\"].astype(int)\n        inst_maps_per_type.append(res)\n\n    res = pd.concat(inst_maps_per_type)\n\n    # filter out small geometries\n    res = res.loc[res.area &gt; min_size].reset_index(drop=True)\n\n    # translate geometries if offsets are provided\n    if xoff is not None or yoff is not None:\n        res[\"geometry\"] = res[\"geometry\"].translate(\n            xoff if xoff is not None else 0, yoff if yoff is not None else 0\n        )\n\n    # smooth geometries if a smoothing function is provided\n    if smooth_func is not None:\n        res[\"geometry\"] = res[\"geometry\"].apply(smooth_func)\n\n    return res\n</code></pre>"},{"location":"api/utils/kmeans_img/","title":"kmeans_img","text":"<p>Performs KMeans clustering on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Image to cluster. Shape (H, W, 3).</p> required <code>n_clust</code> <code>int</code> <p>Number of clusters.</p> <code>3</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda', Cuml will be used for GPU acceleration.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Label image. Shape (H, W).</p> Source code in <code>src/histolytics/utils/im.py</code> <pre><code>def kmeans_img(\n    img: np.ndarray, n_clust: int = 3, seed: int = 42, device: str = \"cpu\"\n) -&gt; np.ndarray:\n    \"\"\"Performs KMeans clustering on the input image.\n\n    Parameters:\n        img (np.ndarray):\n            Image to cluster. Shape (H, W, 3).\n        n_clust (int):\n            Number of clusters.\n        seed (int):\n            Random seed.\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda',\n            Cuml will be used for GPU acceleration.\n\n    Returns:\n        np.ndarray:\n            Label image. Shape (H, W).\n    \"\"\"\n    if device == \"cuda\" and not _has_cp:\n        raise RuntimeError(\n            \"CuPy and cucim are required for GPU acceleration (device='cuda'). \"\n            \"Please install them with:\\n\"\n            \"  pip install cupy-cuda12x cucim-cu12\\n\"\n            \"or set device='cpu'.\"\n        )\n\n    # Check for sufficient color variation\n    pixels = img.reshape(-1, 3)\n    unique_colors = np.unique(pixels, axis=0)\n\n    # If we have fewer unique colors than requested clusters, reduce n_clust\n    if len(unique_colors) &lt; n_clust:\n        n_clust = max(1, len(unique_colors))\n        return np.zeros((img.shape[0], img.shape[1]), dtype=np.int32)\n\n    if device == \"cuda\":\n        return _kmeans_cp(img, n_clust=n_clust, seed=seed)\n    elif device == \"cpu\":\n        return _kmeans_np(img, n_clust=n_clust, seed=seed)\n    else:\n        raise ValueError(f\"Invalid device '{device}'. Use 'cpu' or 'cuda'.\")\n</code></pre>"},{"location":"api/utils/legendgram/","title":"legendgram","text":"<p>Create a histogram legend for a specified column in a GeoDataFrame.</p> Note <p>\"Legendgrams are map legends that visualize the distribution of observations by color in a given map.\"</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame containing segmented objects.</p> required <code>column</code> <code>str</code> <p>The column/feature name to create the legend for. This needs to be numeric.</p> required <code>n_bins</code> <code>int</code> <p>The number of bins to use for the histogram.</p> <code>100</code> <code>cmap</code> <code>str</code> <p>The name of the matplotlib colormap to use for the legend.</p> <code>'viridis'</code> <code>breaks</code> <code>ndarray</code> <p>Custom breaks for the histogram. If None, breaks will be calculated based on the data in the specified column. If provided, should be a 1D array of numeric values that define the bin edges.</p> <code>None</code> <code>frame_on</code> <code>bool</code> <p>Whether to draw a frame around the legend.</p> <code>False</code> <code>add_mean</code> <code>bool</code> <p>Whether to add a vertical line for the mean of the specified column.</p> <code>True</code> <code>add_median</code> <code>bool</code> <p>Whether to add a vertical line for the median of the specified column.</p> <code>False</code> <code>lw</code> <code>float</code> <p>Line width for the mean/median line. Ignored if both <code>add_mean</code> and <code>add_median</code> are False.</p> <code>2</code> <code>lc</code> <code>str</code> <p>Line color for the mean/median line. Ignored if both <code>add_mean</code> and <code>add_median</code> are False.</p> <code>'black'</code> <code>ticks</code> <code>int | List[float]</code> <p>Number of x-ticks or an array of explicit tick locations for the x-axis.</p> <code>None</code> <code>tick_params</code> <code>dict</code> <p>Extra parameters for the tick labels.</p> <code>{'labelsize': 10}</code> <code>ax</code> <code>Axes</code> <p>The axes to draw the legend on. If None, a new axes will be created and the legend will be returned as standalone plt.Axes.</p> <code>None</code> <code>loc</code> <code>str</code> <p>The location of the legend. One of: \"upper left\", \"upper center\", \"upper right\", \"center left\", \"center\", \"center right\", \"lower left\", \"lower center\", \"lower right\". Ignored if <code>ax</code> is not provided.</p> <code>'lower left'</code> <code>legend_size</code> <code>Tuple[str, str] | Tuple[float, float]</code> <p>The size (width, height) of the legend. If the values are floats, the size is given in inches, e.g. (1.3, 1.0). If the values are strings, the size is in relative units to the given input axes, e.g. (\"40%\", \"25%\") means 40% of the width and 25% of the height of the input axes. Ignored if <code>ax</code> is not provided.</p> <code>('40%', '25%')</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: The axes containing the histogram legend. If <code>ax</code> is provided, it will be the same axes; otherwise, a new axes will be created and returned.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.data import cervix_tissue, cervix_nuclei\n&gt;&gt;&gt; from histolytics.spatial_ops.ops import get_objs\n&gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get the cervix nuclei and tissue data\n&gt;&gt;&gt; nuc = cervix_nuclei()\n&gt;&gt;&gt; tis = cervix_tissue()\n&gt;&gt;&gt; # Filter the tissue data for CIN lesions and get the neoplastic nuclei\n&gt;&gt;&gt; lesion = tis[tis[\"class_name\"] == \"cin\"]\n&gt;&gt;&gt; neo = get_objs(lesion, nuc)\n&gt;&gt;&gt; neo = neo[neo[\"class_name\"] == \"neoplastic\"]\n&gt;&gt;&gt; # Calculate the eccentricity for the neoplastic nuclei\n&gt;&gt;&gt; neo = shape_metric(neo, [\"eccentricity\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot the neoplastic nuclei with eccentricity as a color scale\n&gt;&gt;&gt; col = \"eccentricity\"\n&gt;&gt;&gt; ax = nuc.plot(\n...     column=\"class_name\",\n...     figsize=(6, 6),\n...     aspect=1,\n...     alpha=0.5,\n... )\n&gt;&gt;&gt; ax = neo.plot(\n...     ax=ax,\n...     column=col,\n...     legend=False,\n...     cmap=\"turbo\",\n... )\n&gt;&gt;&gt; ax.set_axis_off()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add a legendgram to the plot\n&gt;&gt;&gt; legendgram(\n...     neo,\n...     column=col,\n...     ax=ax,\n...     n_bins=50,\n...     cmap=\"turbo\",\n...     frame_on=False,\n...     lw=2,\n...     lc=\"black\",\n...     ticks=3,\n...     legend_size=(\"30%\", \"20%\"),\n... )\n</code></pre> <p></p> Source code in <code>src/histolytics/utils/plot.py</code> <pre><code>def legendgram(\n    gdf: gpd.GeoDataFrame,\n    column: str,\n    n_bins: int = 100,\n    cmap: str = \"viridis\",\n    breaks: np.ndarray = None,\n    frame_on: bool = False,\n    add_mean: bool = True,\n    add_median: bool = False,\n    lw: float = 2,\n    lc: str = \"black\",\n    ticks: int | List[float] = None,\n    tick_params: dict = {\"labelsize\": 10},\n    ax: plt.Axes = None,\n    loc: str = \"lower left\",\n    legend_size: Tuple[str, str] | Tuple[float, float] = (\"40%\", \"25%\"),\n) -&gt; plt.Axes:\n    \"\"\"Create a histogram legend for a specified column in a GeoDataFrame.\n\n    Note:\n        \"Legendgrams are map legends that visualize the distribution of observations by\n        color in a given map.\"\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            The GeoDataFrame containing segmented objects.\n        column (str):\n            The column/feature name to create the legend for. This needs to be numeric.\n        n_bins (int):\n            The number of bins to use for the histogram.\n        cmap (str):\n            The name of the matplotlib colormap to use for the legend.\n        breaks (np.ndarray):\n            Custom breaks for the histogram. If None, breaks will be calculated\n            based on the data in the specified column. If provided, should be a\n            1D array of numeric values that define the bin edges.\n        frame_on (bool):\n            Whether to draw a frame around the legend.\n        add_mean (bool):\n            Whether to add a vertical line for the mean of the specified column.\n        add_median (bool):\n            Whether to add a vertical line for the median of the specified column.\n        lw (float):\n            Line width for the mean/median line. Ignored if both `add_mean` and\n            `add_median` are False.\n        lc (str):\n            Line color for the mean/median line. Ignored if both `add_mean` and\n            `add_median` are False.\n        ticks (int | List[float]):\n            Number of x-ticks or an array of explicit tick locations for the x-axis.\n        tick_params (dict):\n            Extra parameters for the tick labels.\n        ax (plt.Axes):\n            The axes to draw the legend on. If None, a new axes will be created and the\n            legend will be returned as standalone plt.Axes.\n        loc (str):\n            The location of the legend. One of: \"upper left\", \"upper center\", \"upper right\",\n            \"center left\", \"center\", \"center right\", \"lower left\", \"lower center\", \"lower right\".\n            Ignored if `ax` is not provided.\n        legend_size (Tuple[str, str] | Tuple[float, float]):\n            The size (width, height) of the legend. If the values are floats, the size is\n            given in inches, e.g. (1.3, 1.0). If the values are strings, the size is in relative\n            units to the given input axes, e.g. (\"40%\", \"25%\") means 40% of the width and 25%\n            of the height of the input axes. Ignored if `ax` is not provided.\n\n    Returns:\n        plt.Axes:\n            The axes containing the histogram legend. If `ax` is provided, it will be the\n            same axes; otherwise, a new axes will be created and returned.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.data import cervix_tissue, cervix_nuclei\n        &gt;&gt;&gt; from histolytics.spatial_ops.ops import get_objs\n        &gt;&gt;&gt; from histolytics.spatial_geom.shape_metrics import shape_metric\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get the cervix nuclei and tissue data\n        &gt;&gt;&gt; nuc = cervix_nuclei()\n        &gt;&gt;&gt; tis = cervix_tissue()\n        &gt;&gt;&gt; # Filter the tissue data for CIN lesions and get the neoplastic nuclei\n        &gt;&gt;&gt; lesion = tis[tis[\"class_name\"] == \"cin\"]\n        &gt;&gt;&gt; neo = get_objs(lesion, nuc)\n        &gt;&gt;&gt; neo = neo[neo[\"class_name\"] == \"neoplastic\"]\n        &gt;&gt;&gt; # Calculate the eccentricity for the neoplastic nuclei\n        &gt;&gt;&gt; neo = shape_metric(neo, [\"eccentricity\"])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Plot the neoplastic nuclei with eccentricity as a color scale\n        &gt;&gt;&gt; col = \"eccentricity\"\n        &gt;&gt;&gt; ax = nuc.plot(\n        ...     column=\"class_name\",\n        ...     figsize=(6, 6),\n        ...     aspect=1,\n        ...     alpha=0.5,\n        ... )\n        &gt;&gt;&gt; ax = neo.plot(\n        ...     ax=ax,\n        ...     column=col,\n        ...     legend=False,\n        ...     cmap=\"turbo\",\n        ... )\n        &gt;&gt;&gt; ax.set_axis_off()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Add a legendgram to the plot\n        &gt;&gt;&gt; legendgram(\n        ...     neo,\n        ...     column=col,\n        ...     ax=ax,\n        ...     n_bins=50,\n        ...     cmap=\"turbo\",\n        ...     frame_on=False,\n        ...     lw=2,\n        ...     lc=\"black\",\n        ...     ticks=3,\n        ...     legend_size=(\"30%\", \"20%\"),\n        ... )\n    ![legendgram](../../img/legendgram.png)\n    \"\"\"\n    y = gdf[column].values\n\n    # Check if breaks are provided, if not, calculate them\n    if breaks is None:\n        min_val = np.round((y.min()), 1)\n        max_val = np.round((y.max()), 1)\n        step = np.round(((max_val - min_val) / n_bins), 3)\n        breaks = np.arange(min_val, max_val, step)\n\n    # Create a colormap with the specified number of breaks\n    pal = colormaps.get_cmap(cmap).resampled(len(breaks))\n\n    if ax is None:\n        _, histax = plt.subplots()\n    else:\n        histax = inset_axes(\n            ax,\n            width=legend_size[0],\n            height=legend_size[1],\n            loc=loc,\n        )\n\n    _, bins, patches = histax.hist(y, bins=n_bins, color=\"0.0\")\n\n    bucket_breaks = [0] + [np.searchsorted(bins, i) for i in breaks]\n    for c in range(len(breaks)):\n        for b in range(bucket_breaks[c], bucket_breaks[c + 1]):\n            try:\n                patches[b].set_facecolor(pal(c / len(breaks)))\n            except Exception:\n                continue\n\n    if add_mean:\n        plt.axvline(y.mean(), linestyle=\"dashed\", linewidth=lw, c=lc)\n\n    if add_median:\n        plt.axvline(np.median(y), linestyle=\"dashed\", linewidth=lw, c=lc)\n\n    histax.set_frame_on(frame_on)\n    histax.set_xlabel(column.title())\n    histax.get_yaxis().set_visible(False)\n    histax.tick_params(**tick_params)\n\n    # Set x-axis major tick frequency\n    if ticks is not None:\n        if isinstance(ticks, int):\n            tick_spacing = (max_val - min_val) / (ticks - 1)\n            histax.xaxis.set_major_locator(MultipleLocator(tick_spacing))\n        elif isinstance(ticks, (list, np.ndarray)):\n            histax.set_xticks(ticks)\n\n    return histax\n</code></pre>"},{"location":"api/utils/sem2gdf/","title":"sem2gdf","text":"<p>Convert an semantic segmentation raster mask to a GeoDataFrame.</p> Note <p>This function should be applied to semantic tissue segmentation masks.</p> <p>Parameters:</p> Name Type Description Default <code>sem_map</code> <code>ndarray</code> <p>A semantic segmentation mask. Shape (H, W).</p> required <code>xoff</code> <code>int</code> <p>The x offset. Optional. The offset is used to translate the geometries in the GeoDataFrame. If None, no translation is applied.</p> <code>None</code> <code>yoff</code> <code>int</code> <p>The y offset. Optional. The offset is used to translate the geometries in the GeoDataFrame. If None, no translation is applied.</p> <code>None</code> <code>class_dict</code> <code>Dict[int, str]</code> <p>A dictionary mapping class indices to class names. e.g. {1: 'neoplastic', 2: 'immune'}. If None, the class indices will be used.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>The minimum size (in pixels) of the polygons to include in the GeoDataFrame.</p> <code>15</code> <code>smooth_func</code> <code>Callable</code> <p>A function to smooth the polygons. The function should take a shapely Polygon as input and return a shapely Polygon. Defaults to <code>uniform_smooth</code>, which applies a uniform filter. <code>histolytics.utils._filters</code> also provides <code>gaussian_smooth</code> and <code>median_smooth</code> for smoothing.</p> <code>uniform_smooth</code> <p>returns:     gpd.GeoDataFrame:         A GeoDataFrame of the raster semantic mask. Contains columns:</p> <pre><code>        - 'id' - the numeric pixel value of the semantic mask,\n        - 'class_name' - the name of the class (same as id if class_dict is None),\n        - 'geometry' - the geometry of the polygon.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.raster import sem2gdf\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_type_mask\n&gt;&gt;&gt; # load semantic mask\n&gt;&gt;&gt; type_mask = hgsc_cancer_type_mask()\n&gt;&gt;&gt; # convert to GeoDataFrame\n&gt;&gt;&gt; gdf = sem2gdf(type_mask)\n&gt;&gt;&gt; print(gdf.head(3))\n        uid  class_name                                           geometry\n    0   2           2  POLYGON ((850.019 0.45, 850.431 1.58, 851.657 ...\n    1   2           2  POLYGON ((1194.01 0.225, 1194.215 0.795, 1194....\n    2   1           1  POLYGON ((405.019 0.45, 405.43 1.58, 406.589 2...\n</code></pre> Source code in <code>src/histolytics/utils/raster.py</code> <pre><code>def sem2gdf(\n    sem_map: np.ndarray,\n    xoff: int = None,\n    yoff: int = None,\n    class_dict: Dict[int, str] = None,\n    min_size: int = 15,\n    smooth_func: Callable = uniform_smooth,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert an semantic segmentation raster mask to a GeoDataFrame.\n\n    Note:\n        This function should be applied to semantic tissue segmentation masks.\n\n    Parameters:\n        sem_map (np.ndarray):\n            A semantic segmentation mask. Shape (H, W).\n        xoff (int):\n            The x offset. Optional. The offset is used to translate the geometries\n            in the GeoDataFrame. If None, no translation is applied.\n        yoff (int):\n            The y offset. Optional. The offset is used to translate the geometries\n            in the GeoDataFrame. If None, no translation is applied.\n        class_dict (Dict[int, str]):\n            A dictionary mapping class indices to class names.\n            e.g. {1: 'neoplastic', 2: 'immune'}. If None, the class indices will be used.\n        min_size (int):\n            The minimum size (in pixels) of the polygons to include in the GeoDataFrame.\n        smooth_func (Callable):\n            A function to smooth the polygons. The function should take a shapely Polygon\n            as input and return a shapely Polygon. Defaults to `uniform_smooth`, which\n            applies a uniform filter. `histolytics.utils._filters` also provides\n            `gaussian_smooth` and `median_smooth` for smoothing.\n    returns:\n        gpd.GeoDataFrame:\n            A GeoDataFrame of the raster semantic mask. Contains columns:\n\n                - 'id' - the numeric pixel value of the semantic mask,\n                - 'class_name' - the name of the class (same as id if class_dict is None),\n                - 'geometry' - the geometry of the polygon.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.raster import sem2gdf\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_type_mask\n        &gt;&gt;&gt; # load semantic mask\n        &gt;&gt;&gt; type_mask = hgsc_cancer_type_mask()\n        &gt;&gt;&gt; # convert to GeoDataFrame\n        &gt;&gt;&gt; gdf = sem2gdf(type_mask)\n        &gt;&gt;&gt; print(gdf.head(3))\n                uid  class_name                                           geometry\n            0   2           2  POLYGON ((850.019 0.45, 850.431 1.58, 851.657 ...\n            1   2           2  POLYGON ((1194.01 0.225, 1194.215 0.795, 1194....\n            2   1           1  POLYGON ((405.019 0.45, 405.43 1.58, 406.589 2...\n    \"\"\"\n    # Handle empty semantic mask\n    if sem_map.size == 0 or np.max(sem_map) == 0:\n        return gpd.GeoDataFrame(columns=[\"uid\", \"class_name\", \"geometry\"])\n\n    if class_dict is None:\n        class_dict = {int(i): int(i) for i in np.unique(sem_map)[1:]}\n\n    vectorized_data = (\n        (value, shape(polygon))\n        for polygon, value in shapes(\n            sem_map,\n            mask=sem_map &gt; 0,\n        )\n    )\n\n    res = gpd.GeoDataFrame(\n        vectorized_data,\n        columns=[\"uid\", \"geometry\"],\n    )\n    res[\"uid\"] = res[\"uid\"].astype(int)\n    res = res.loc[res.area &gt; min_size].reset_index(drop=True)\n    res[\"class_name\"] = res[\"uid\"].map(class_dict)\n    res = res[[\"uid\", \"class_name\", \"geometry\"]]  # reorder columns\n\n    if xoff is not None or yoff is not None:\n        res[\"geometry\"] = res[\"geometry\"].translate(\n            xoff if xoff is not None else 0, yoff if yoff is not None else 0\n        )\n\n    if smooth_func is not None:\n        res[\"geometry\"] = res[\"geometry\"].apply(smooth_func)\n\n    return res\n</code></pre>"},{"location":"api/utils/set_geom_precision/","title":"set_geom_precision","text":"<p>Set the precision of a Shapely geometry.</p> Note <p>Typically six decimals is sufficient for most applications.</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>BaseGeometry</code> <p>Input Shapely geometry.</p> required <code>precision</code> <code>int</code> <p>The number of decimal places to round the coordinates to.</p> <code>6</code> <p>Returns:</p> Name Type Description <code>BaseGeometry</code> <code>BaseGeometry</code> <p>The input geometry with coordinates rounded to the specified precision.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import gdf_apply, set_geom_precision\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; # Set precision to 3 decimal places\n&gt;&gt;&gt; prec = partial(set_geom_precision, precision=3)\n&gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n&gt;&gt;&gt; gdf = gdf_apply(gdf, prec, columns=[\"geometry\"])\n&gt;&gt;&gt; print(gdf.head(3))\n    0    POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ...\n    1    POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ...\n    2    POLYGON ((1382.99 156.01, 1380 156.01, 1376.01...\n    dtype: geometry\n</code></pre> Source code in <code>src/histolytics/utils/gdf.py</code> <pre><code>def set_geom_precision(geom: BaseGeometry, precision: int = 6) -&gt; BaseGeometry:\n    \"\"\"Set the precision of a Shapely geometry.\n\n    Note:\n        Typically six decimals is sufficient for most applications.\n\n    Parameters:\n        geom (BaseGeometry):\n            Input Shapely geometry.\n        precision (int):\n            The number of decimal places to round the coordinates to.\n\n    Returns:\n        BaseGeometry:\n            The input geometry with coordinates rounded to the specified precision.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.gdf import gdf_apply, set_geom_precision\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; from functools import partial\n        &gt;&gt;&gt; # Set precision to 3 decimal places\n        &gt;&gt;&gt; prec = partial(set_geom_precision, precision=3)\n        &gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; gdf = gdf_apply(gdf, prec, columns=[\"geometry\"])\n        &gt;&gt;&gt; print(gdf.head(3))\n            0    POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ...\n            1    POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ...\n            2    POLYGON ((1382.99 156.01, 1380 156.01, 1376.01...\n            dtype: geometry\n    \"\"\"\n    wkt_str = dumps(geom, rounding_precision=precision, trim=True)\n    return wkt.loads(wkt_str)\n</code></pre>"},{"location":"api/utils/set_uid/","title":"set_uid","text":"<p>Set a unique identifier column to gdf.</p> Note <p>by default sets a running index column to gdf as the uid.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input Geodataframe.</p> required <code>start_ix</code> <code>int</code> <p>The starting index of the id column.</p> <code>0</code> <code>id_col</code> <code>str</code> <p>The name of the column that will be used or set to the id.</p> <code>'uid'</code> <code>drop</code> <code>bool</code> <p>Drop the column after it is added to index.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The input gdf with a \"uid\" column added to it.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n&gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n&gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n&gt;&gt;&gt; gdf = set_uid(gdf, drop=False)\n&gt;&gt;&gt; print(gdf.head(3))\n                                                geometry  class_name  uid\n    uid\n    0    POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ...  connective    0\n    1    POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ...  connective    1\n    2    POLYGON ((1382.99 156.01, 1380 156.01, 1376.01...  connective    2\n</code></pre> Source code in <code>src/histolytics/utils/gdf.py</code> <pre><code>def set_uid(\n    gdf: gpd.GeoDataFrame, start_ix: int = 0, id_col: str = \"uid\", drop: bool = False\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Set a unique identifier column to gdf.\n\n    Note:\n        by default sets a running index column to gdf as the uid.\n\n    Parameters:\n        gdf (gpd.GeoDataFrame):\n            Input Geodataframe.\n        start_ix (int):\n            The starting index of the id column.\n        id_col (str):\n            The name of the column that will be used or set to the id.\n        drop (bool):\n            Drop the column after it is added to index.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The input gdf with a \"uid\" column added to it.\n\n    Examples:\n        &gt;&gt;&gt; from histolytics.utils.gdf import set_uid\n        &gt;&gt;&gt; from histolytics.data import hgsc_cancer_nuclei\n        &gt;&gt;&gt; gdf = hgsc_cancer_nuclei()\n        &gt;&gt;&gt; gdf = set_uid(gdf, drop=False)\n        &gt;&gt;&gt; print(gdf.head(3))\n                                                        geometry  class_name  uid\n            uid\n            0    POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ...  connective    0\n            1    POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ...  connective    1\n            2    POLYGON ((1382.99 156.01, 1380 156.01, 1376.01...  connective    2\n    \"\"\"\n    if id_col is None:\n        id_col = \"uid\"\n    gdf = gdf.assign(**{id_col: range(start_ix, len(gdf) + start_ix)})\n    gdf = gdf.set_index(id_col, drop=drop)\n\n    return gdf\n</code></pre>"},{"location":"api/utils/tissue_components/","title":"tissue_components","text":"<p>Segment background and foreground masks from H&amp;E image. Uses k-means clustering.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The input H&amp;E image. Shape (H, W, 3).</p> required <code>label</code> <code>ndarray</code> <p>The nuclei label mask. Shape (H, W). This is used to mask out the nuclei when extracting tissue components. If None, the entire image is used.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda', Cupy will be used for GPU acceleration.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The background and foreground masks. Shapes (H, W).</p> Source code in <code>src/histolytics/utils/im.py</code> <pre><code>def tissue_components(\n    img: np.ndarray, label: np.ndarray = None, device: str = \"cpu\"\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Segment background and foreground masks from H&amp;E image. Uses k-means clustering.\n\n    Parameters:\n        img (np.ndarray):\n            The input H&amp;E image. Shape (H, W, 3).\n        label (np.ndarray):\n            The nuclei label mask. Shape (H, W). This is used to mask out the nuclei when\n            extracting tissue components. If None, the entire image is used.\n        device (str):\n            Device to use for computation. Options are 'cpu' or 'cuda'. If set to 'cuda',\n            Cupy will be used for GPU acceleration.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]:\n            The background and foreground masks. Shapes (H, W).\n    \"\"\"\n    # mask out dark pixels\n    kmasks = kmeans_img(img, n_clust=3, device=device)\n\n    if not np.any(kmasks):\n        bg_mask = np.zeros(kmasks.shape[:2], dtype=bool)\n        dark_mask = np.zeros(kmasks.shape[:2], dtype=bool)\n        return bg_mask, dark_mask\n\n    if _has_cp and device == \"cuda\":\n        bg_mask, dark_mask = _get_tissue_bg_fg_cp(img, kmasks, label)\n    else:\n        bg_mask, dark_mask = _get_tissue_bg_fg_np(img, kmasks, label)\n\n    bg_mask = rm_objects_mask(\n        erosion(bg_mask, footprint_rectangle((3, 3))), min_size=1000, device=device\n    )\n    dark_mask = rm_objects_mask(\n        dilation(dark_mask, footprint_rectangle((3, 3))), min_size=200, device=device\n    )\n\n    # couldn't get this work with cupyx.ndimage..\n    bg_mask = ndimage.binary_fill_holes(bg_mask)\n    dark_mask = ndimage.binary_fill_holes(dark_mask)\n\n    return bg_mask, dark_mask\n</code></pre>"},{"location":"api/wsi/get_sub_grids/","title":"get_sub_grids","text":""},{"location":"api/wsi/get_sub_grids/#histolytics.wsi.utils.get_sub_grids","title":"histolytics.wsi.utils.get_sub_grids","text":"<pre><code>get_sub_grids(coordinates: List[Tuple[int, int, int, int]], inds: Tuple[int, ...] = None, min_size: int = 1, return_gdf: bool = False) -&gt; List[List[Tuple[int, int, int, int]]]\n</code></pre> <p>Get sub-grids based on connected components of the grid.</p> Note <p>The order of the sub-grids is from ymin to ymax.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>List[Tuple[int, int, int, int]]</code> <p>List of grid bbox coordinates in (x, y, w, h) format.</p> required <code>inds</code> <code>Tuple[int, ...], default=None</code> <p>Indices of the connected components to extract.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>Minimum size of the sub grid.</p> <code>1</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return a GeoDataFrame instead of a list of sub-grids.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[List[Tuple[int, int, int, int]]]</code> <p>List[List[Tuple[int, int, int, int]]]: Nested list of sub-grids in (x, y, w, h) format.</p> Source code in <code>src/histolytics/wsi/utils.py</code> <pre><code>def get_sub_grids(\n    coordinates: List[Tuple[int, int, int, int]],\n    inds: Tuple[int, ...] = None,\n    min_size: int = 1,\n    return_gdf: bool = False,\n) -&gt; List[List[Tuple[int, int, int, int]]]:\n    \"\"\"Get sub-grids based on connected components of the grid.\n\n    Note:\n        The order of the sub-grids is from ymin to ymax.\n\n    Parameters:\n        coordinates (List[Tuple[int, int, int, int]]):\n            List of grid bbox coordinates in (x, y, w, h) format.\n        inds (Tuple[int, ...], default=None):\n            Indices of the connected components to extract.\n        min_size (int):\n            Minimum size of the sub grid.\n        return_gdf (bool):\n            Whether to return a GeoDataFrame instead of a list of sub-grids.\n\n    Returns:\n        List[List[Tuple[int, int, int, int]]]:\n            Nested list of sub-grids in (x, y, w, h) format.\n    \"\"\"\n    if isinstance(coordinates, gpd.GeoDataFrame):\n        coordinates = coordinates.geometry.apply(_polygon_to_xywh).tolist()\n\n    # convert to (xmin, ymin, xmax, ymax) format\n    bbox_coords = [(x, y, x + w, y + h) for x, y, w, h in coordinates]\n\n    # convert to shapely boxes\n    box_polys = [box(xmin, ymin, xmax, ymax) for xmin, ymin, xmax, ymax in bbox_coords]\n\n    # Create GeoDataFrame from the grid\n    grid = gpd.GeoDataFrame({\"geometry\": box_polys}, crs=\"+proj=cea\")\n\n    # get queen contiguity of the grid\n    w = fuzzy_contiguity(\n        grid,\n        buffering=False,\n        predicate=\"intersects\",\n        silence_warnings=True,\n    )\n\n    # get connected components of the grid\n    G = w.to_networkx()\n    sub_graphs = [\n        W(nx.to_dict_of_lists(G.subgraph(c).copy())) for c in nx.connected_components(G)\n    ]\n\n    sub_graphs = [g for g in sub_graphs if len(g.neighbors) &gt; min_size]\n\n    if inds is not None:\n        sub_graphs = [sub_graphs[i] for i in inds]\n\n    sub_grids = []\n    if return_gdf:\n        for g in sub_graphs:\n            indices = list(g.neighbors.keys())\n            sub_grids.append(grid.loc[indices])\n    else:\n        for g in sub_graphs:\n            indices = list(g.neighbors.keys())\n            sub_grids.append([coordinates[i] for i in indices])\n\n    return sub_grids\n</code></pre>"},{"location":"api/wsi/slide_reader/","title":"SlideReader","text":""},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader","title":"histolytics.wsi.slide_reader.SlideReader","text":"Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>class SlideReader:\n    def __init__(\n        self,\n        path: Union[str, Path],\n        backend: str = \"OPENSLIDE\",\n    ) -&gt; None:\n        \"\"\"Reader class for histological whole slide images.\n\n        Parameters:\n            path (str, Path):\n                Path to slide image.\n            backend (str):\n                Backend to use for reading slide images. One of:\n\n                - \"OPENSLIDE\": Uses OpenSlideReader.\n                - \"CUCIM\": Uses CucimReader.\n                - \"BIOIO\": Uses BioIOReader.\n\n        Raises:\n            FileNotFoundError: Path does not exist.\n            ValueError: Backend name not recognised.\n        \"\"\"\n        super().__init__()\n        if backend not in AVAILABLE_BACKENDS:\n            raise ValueError(f\"Backend {backend} not recognised or not supported.\")\n\n        if backend == \"OPENSLIDE\":\n            self._reader = OpenSlideReader(path=path)\n        elif backend == \"CUCIM\":\n            self._reader = CucimReader(path=path)\n        elif backend == \"BIOIO\":\n            self._reader = BioIOReader(path=path)\n\n    @property\n    def path(self) -&gt; str:\n        \"\"\"Full slide filepath.\"\"\"\n        return self._reader.path\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Slide filename without an extension.\"\"\"\n        return self._reader.name\n\n    @property\n    def suffix(self) -&gt; str:\n        \"\"\"Slide file-extension.\"\"\"\n        return self._reader.suffix\n\n    @property\n    def backend_name(self) -&gt; str:\n        \"\"\"Name of the slide reader backend.\"\"\"\n        return self._reader.BACKEND_NAME\n\n    @property\n    def data_bounds(self) -&gt; tuple[int, int, int, int]:\n        \"\"\"Data bounds defined by `xywh`-coordinates at `level=0`.\n\n        Some image formats (eg. `.mrxs`) define a bounding box where image data resides,\n        which may differ from the actual image dimensions. `HistoPrep` always uses the\n        full image dimensions, but other software (such as `QuPath`) uses the image\n        dimensions defined by this data bound.\n        \"\"\"\n        return self._reader.data_bounds\n\n    @property\n    def dimensions(self) -&gt; tuple[int, int]:\n        \"\"\"Image dimensions (height, width) at `level=0`.\"\"\"\n        return self._reader.dimensions\n\n    @property\n    def level_count(self) -&gt; int:\n        \"\"\"Number of slide pyramid levels.\"\"\"\n        return self._reader.level_count\n\n    @property\n    def level_dimensions(self) -&gt; dict[int, tuple[int, int]]:\n        \"\"\"Image dimensions (height, width) for each pyramid level.\"\"\"\n        return self._reader.level_dimensions\n\n    @property\n    def level_downsamples(self) -&gt; dict[int, tuple[float, float]]:\n        \"\"\"Image downsample factors (height, width) for each pyramid level.\"\"\"\n        return self._reader.level_downsamples\n\n    def read_level(self, level: int) -&gt; np.ndarray:\n        \"\"\"Read full pyramid level data.\n\n        Parameters:\n            level (int):\n                Slide pyramid level to read.\n\n        Raises:\n            ValueError: Invalid level argument.\n\n        Returns:\n            np.ndarray:\n                Array containing image data from `level`.\n        \"\"\"\n        return self._reader.read_level(level=level)\n\n    def read_region(\n        self, xywh: tuple[int, int, int, int], level: int = 0\n    ) -&gt; np.ndarray:\n        \"\"\"Read region based on `xywh`-coordinates.\n\n        Parameters:\n            xywh (tuple[int, int, int, int]):\n                Coordinates for the region.\n            level (int):\n                Slide pyramid level to read from.\n\n        Raises:\n            ValueError: Invalid `level` argument.\n\n        Returns:\n            np.ndarray:\n                Array containing image data from `xywh`-region.\n        \"\"\"\n        if isinstance(xywh, shapely.geometry.Polygon):\n            minx, miny, maxx, maxy = xywh.bounds\n            xywh = (int(minx), int(miny), int(maxx - minx), int(maxy - miny))\n\n        return self._reader.read_region(xywh=xywh, level=level)\n\n    def level_from_max_dimension(self, max_dimension: int = 4096) -&gt; int:\n        \"\"\"Find pyramid level with *both* dimensions less or equal to `max_dimension`.\n        If one isn't found, return the last pyramid level.\n\n        Parameters:\n            max_dimension (int):\n                Maximum dimension for the level.\n\n        Returns:\n            int:\n                Slide pyramid level.\n        \"\"\"\n        for level, (level_h, level_w) in self.level_dimensions.items():\n            if level_h &lt;= max_dimension and level_w &lt;= max_dimension:\n                return level\n        return list(self.level_dimensions.keys())[-1]\n\n    def level_from_dimensions(self, dimensions: tuple[int, int]) -&gt; int:\n        \"\"\"Find pyramid level which is closest to `dimensions`.\n\n        Parameters:\n            dimensions (tuple[int, int]):\n                Height and width.\n\n        Returns:\n            int:\n                Slide pyramid level.\n        \"\"\"\n        height, width = dimensions\n        available = []\n        distances = []\n        for level, (level_h, level_w) in self.level_dimensions.items():\n            available.append(level)\n            distances.append(abs(level_h - height) + abs(level_w - width))\n        return available[distances.index(min(distances))]\n\n    def get_tissue_mask(\n        self,\n        *,\n        level: Optional[int] = None,\n        threshold: Optional[int] = None,\n        multiplier: float = 1.05,\n        sigma: float = 0.0,\n    ) -&gt; tuple[int, np.ndarray]:\n        \"\"\"Detect tissue from slide pyramid level image.\n\n        Parameters:\n            level (int):\n                Slide pyramid level to use for tissue detection. If None, uses the\n                `level_from_max_dimension` method.\n            threshold (int):\n                Threshold for tissue detection. If set, will detect tissue by global\n                thresholding. Otherwise Otsu's method is used to find a threshold.\n            multiplier (float):\n                Otsu's method finds an optimal threshold by minimizing the weighted\n                within-class variance. This threshold is then multiplied with\n                `multiplier`. Ignored if `threshold` is not None.\n            sigma (float):\n                Sigma for gaussian blurring.\n\n        Raises:\n            ValueError: Threshold not between 0 and 255.\n\n        Returns:\n            tuple[int, np.ndarray]:\n                Threshold and tissue mask.\n        \"\"\"\n        level = (\n            self.level_from_max_dimension()\n            if level is None\n            else format_level(level, available=list(self.level_dimensions))\n        )\n        return get_tissue_mask(\n            image=self.read_level(level),\n            threshold=threshold,\n            multiplier=multiplier,\n            sigma=sigma,\n        )\n\n    def get_tile_coordinates(\n        self,\n        width: int,\n        *,\n        tissue_mask: Optional[np.ndarray],\n        annotations: Optional[Polygon] = None,\n        height: Optional[int] = None,\n        overlap: float = 0.0,\n        max_background: float = 0.95,\n        out_of_bounds: bool = True,\n    ) -&gt; TileCoordinates:\n        \"\"\"Generate tile coordinates.\n\n        Parameters:\n            width (int):\n                Width of a tile.\n            tissue_mask (np.ndarray):\n                Tissue mask for filtering tiles with too much background. If None,\n                the filtering is disabled.\n            annotations (Optional[Polygon]):\n                Annotations to filter tiles by. If provided, only tiles that intersect\n                with the annotations will be returned.\n            height (int):\n                Height of a tile. If None, will be set to `width`.\n            overlap (float):\n                Overlap between neighbouring tiles.\n            max_background (float):\n                Maximum proportion of background in tiles. Ignored if `tissue_mask`\n                is None.\n            out_of_bounds (bool):\n                Keep tiles which contain regions outside of the image.\n\n        Raises:\n            ValueError: Height and/or width are smaller than 1.\n            ValueError: Height and/or width is larger than dimensions.\n            ValueError: Overlap is not in range [0, 1).\n\n        Returns:\n            TileCoordinates:\n                `TileCoordinates` dataclass.\n        \"\"\"\n        tile_coordinates = get_tile_coordinates(\n            dimensions=self.dimensions,\n            width=width,\n            height=height,\n            overlap=overlap,\n            out_of_bounds=out_of_bounds,\n        )\n        if tissue_mask is not None:\n            all_backgrounds = get_background_percentages(\n                tile_coordinates=tile_coordinates,\n                tissue_mask=tissue_mask,\n                downsample=get_downsample(tissue_mask, self.dimensions),\n            )\n            filtered_coordinates = []\n            for xywh, background in zip(tile_coordinates, all_backgrounds):\n                if background &lt;= max_background:\n                    filtered_coordinates.append(xywh)\n            tile_coordinates = filtered_coordinates\n\n        if annotations is not None:\n            # Convert tile coordinates to polygons\n            tiles_gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": [\n                        box(x, y, x + w, y + h) for x, y, w, h in tile_coordinates\n                    ]\n                }\n            )\n\n            # Filter tiles that intersect with the annotation bbox\n            filtered_tiles = tiles_gdf[tiles_gdf.intersects(annotations)]\n\n            tile_coordinates = [\n                (\n                    int(poly.bounds[0]),\n                    int(poly.bounds[1]),\n                    int(poly.bounds[2] - poly.bounds[0]),\n                    int(poly.bounds[3] - poly.bounds[1]),\n                )\n                for poly in filtered_tiles.geometry\n            ]\n\n        return TileCoordinates(\n            coordinates=tile_coordinates,\n            width=width,\n            height=width if height is None else height,\n            overlap=overlap,\n            max_background=None if tissue_mask is None else max_background,\n            tissue_mask=tissue_mask,\n        )\n\n    def get_spot_coordinates(\n        self,\n        tissue_mask: np.ndarray,\n        *,\n        min_area_pixel: int = 10,\n        max_area_pixel: Optional[int] = None,\n        min_area_relative: float = 0.2,\n        max_area_relative: Optional[float] = 2.0,\n    ) -&gt; SpotCoordinates:\n        \"\"\"Generate tissue microarray spot coordinates.\n\n        Parameters:\n            tissue_mask:\n                Tissue mask of the slide. It's recommended to increase `sigma` value when\n                detecting tissue to remove non-TMA spots from the mask. Rest of the areas\n                can be handled with the following arguments.\n            min_area_pixel (int):\n                Minimum pixel area for contours.\n            max_area_pixel (int):\n                Maximum pixel area for contours.\n            min_area_relative (float):\n                Relative minimum contour area, calculated from the median contour area\n                after filtering contours with `[min,max]_pixel` arguments\n                (`min_area_relative * median(contour_areas)`).\n            max_area_relative (float):\n                Relative maximum contour area, calculated from the median contour area\n                after filtering contours with `[min,max]_pixel` arguments\n                (`max_area_relative * median(contour_areas)`).\n\n        Returns:\n            SpotCoordinates:\n                `SpotCoordinates` instance.\n        \"\"\"\n        spot_mask = clean_tissue_mask(\n            tissue_mask=tissue_mask,\n            min_area_pixel=min_area_pixel,\n            max_area_pixel=max_area_pixel,\n            min_area_relative=min_area_relative,\n            max_area_relative=max_area_relative,\n        )\n        # Dearray spots.\n        spot_info = get_spot_coordinates(spot_mask)\n        spot_coordinates = [  # upsample to level zero.\n            _multiply_xywh(x, get_downsample(tissue_mask, self.dimensions))\n            for x in spot_info.values()\n        ]\n\n        return SpotCoordinates(\n            coordinates=spot_coordinates,\n            spot_names=list(spot_info.keys()),\n            tissue_mask=spot_mask,\n        )\n\n    def get_annotated_thumbnail(\n        self,\n        image: np.ndarray,\n        coordinates: Iterator[tuple[int, int, int, int]],\n        linewidth: int = 1,\n        cmap: str = None,\n        values: np.ndarray = None,\n        breaks: Sequence[float] = None,\n    ) -&gt; Image.Image:\n        \"\"\"Generate annotated thumbnail from coordinates.\n\n        Parameters:\n            image (np.ndarray):\n                Input image.\n            coordinates (Iterator[tuple[int, int, int, int]]):\n                Coordinates to annotate.\n            linewidth (int):\n                Width of rectangle lines.\n            cmap (str):\n                Colormap to use for the annotation.\n            values (np.ndarray):\n                Values to use for cmap. Needs to be the same length as `coordinates`.\n            breaks (Sequence[float]):\n                Breakpoints for the colormap.\n\n        Returns:\n            PIL.Image.Image:\n                Annotated thumbnail.\n        \"\"\"\n        if isinstance(coordinates, gpd.GeoDataFrame):\n            coordinates = coordinates.geometry.apply(_polygon_to_xywh).tolist()\n\n        kwargs = {\n            \"image\": image,\n            \"downsample\": get_downsample(image, self.dimensions),\n            \"rectangle_width\": linewidth,\n            \"cmap\": cmap,\n            \"values\": values,\n            \"breaks\": breaks,\n        }\n        if isinstance(coordinates, SpotCoordinates):\n            text_items = [x.lstrip(\"spot_\") for x in coordinates.spot_names]\n            kwargs.update(\n                {\"coordinates\": coordinates.coordinates, \"text_items\": text_items}\n            )\n        elif isinstance(coordinates, TileCoordinates):\n            kwargs.update(\n                {\"coordinates\": coordinates.coordinates, \"highlight_first\": True}\n            )\n        else:\n            kwargs.update({\"coordinates\": coordinates})\n        return get_annotated_image(**kwargs)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(path={self.path}, \"\n            f\"backend={self._reader.BACKEND_NAME})\"\n        )\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.path","title":"path  <code>property</code>","text":"<pre><code>path: str\n</code></pre> <p>Full slide filepath.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Slide filename without an extension.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.suffix","title":"suffix  <code>property</code>","text":"<pre><code>suffix: str\n</code></pre> <p>Slide file-extension.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.backend_name","title":"backend_name  <code>property</code>","text":"<pre><code>backend_name: str\n</code></pre> <p>Name of the slide reader backend.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.data_bounds","title":"data_bounds  <code>property</code>","text":"<pre><code>data_bounds: tuple[int, int, int, int]\n</code></pre> <p>Data bounds defined by <code>xywh</code>-coordinates at <code>level=0</code>.</p> <p>Some image formats (eg. <code>.mrxs</code>) define a bounding box where image data resides, which may differ from the actual image dimensions. <code>HistoPrep</code> always uses the full image dimensions, but other software (such as <code>QuPath</code>) uses the image dimensions defined by this data bound.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.dimensions","title":"dimensions  <code>property</code>","text":"<pre><code>dimensions: tuple[int, int]\n</code></pre> <p>Image dimensions (height, width) at <code>level=0</code>.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.level_count","title":"level_count  <code>property</code>","text":"<pre><code>level_count: int\n</code></pre> <p>Number of slide pyramid levels.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.level_dimensions","title":"level_dimensions  <code>property</code>","text":"<pre><code>level_dimensions: dict[int, tuple[int, int]]\n</code></pre> <p>Image dimensions (height, width) for each pyramid level.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.level_downsamples","title":"level_downsamples  <code>property</code>","text":"<pre><code>level_downsamples: dict[int, tuple[float, float]]\n</code></pre> <p>Image downsample factors (height, width) for each pyramid level.</p>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.__init__","title":"__init__","text":"<pre><code>__init__(path: Union[str, Path], backend: str = 'OPENSLIDE') -&gt; None\n</code></pre> <p>Reader class for histological whole slide images.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>(str, Path)</code> <p>Path to slide image.</p> required <code>backend</code> <code>str</code> <p>Backend to use for reading slide images. One of:</p> <ul> <li>\"OPENSLIDE\": Uses OpenSlideReader.</li> <li>\"CUCIM\": Uses CucimReader.</li> <li>\"BIOIO\": Uses BioIOReader.</li> </ul> <code>'OPENSLIDE'</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Path does not exist.</p> <code>ValueError</code> <p>Backend name not recognised.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def __init__(\n    self,\n    path: Union[str, Path],\n    backend: str = \"OPENSLIDE\",\n) -&gt; None:\n    \"\"\"Reader class for histological whole slide images.\n\n    Parameters:\n        path (str, Path):\n            Path to slide image.\n        backend (str):\n            Backend to use for reading slide images. One of:\n\n            - \"OPENSLIDE\": Uses OpenSlideReader.\n            - \"CUCIM\": Uses CucimReader.\n            - \"BIOIO\": Uses BioIOReader.\n\n    Raises:\n        FileNotFoundError: Path does not exist.\n        ValueError: Backend name not recognised.\n    \"\"\"\n    super().__init__()\n    if backend not in AVAILABLE_BACKENDS:\n        raise ValueError(f\"Backend {backend} not recognised or not supported.\")\n\n    if backend == \"OPENSLIDE\":\n        self._reader = OpenSlideReader(path=path)\n    elif backend == \"CUCIM\":\n        self._reader = CucimReader(path=path)\n    elif backend == \"BIOIO\":\n        self._reader = BioIOReader(path=path)\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.read_level","title":"read_level","text":"<pre><code>read_level(level: int) -&gt; np.ndarray\n</code></pre> <p>Read full pyramid level data.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Slide pyramid level to read.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Invalid level argument.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array containing image data from <code>level</code>.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def read_level(self, level: int) -&gt; np.ndarray:\n    \"\"\"Read full pyramid level data.\n\n    Parameters:\n        level (int):\n            Slide pyramid level to read.\n\n    Raises:\n        ValueError: Invalid level argument.\n\n    Returns:\n        np.ndarray:\n            Array containing image data from `level`.\n    \"\"\"\n    return self._reader.read_level(level=level)\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.read_region","title":"read_region","text":"<pre><code>read_region(xywh: tuple[int, int, int, int], level: int = 0) -&gt; np.ndarray\n</code></pre> <p>Read region based on <code>xywh</code>-coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>xywh</code> <code>tuple[int, int, int, int]</code> <p>Coordinates for the region.</p> required <code>level</code> <code>int</code> <p>Slide pyramid level to read from.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Invalid <code>level</code> argument.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array containing image data from <code>xywh</code>-region.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def read_region(\n    self, xywh: tuple[int, int, int, int], level: int = 0\n) -&gt; np.ndarray:\n    \"\"\"Read region based on `xywh`-coordinates.\n\n    Parameters:\n        xywh (tuple[int, int, int, int]):\n            Coordinates for the region.\n        level (int):\n            Slide pyramid level to read from.\n\n    Raises:\n        ValueError: Invalid `level` argument.\n\n    Returns:\n        np.ndarray:\n            Array containing image data from `xywh`-region.\n    \"\"\"\n    if isinstance(xywh, shapely.geometry.Polygon):\n        minx, miny, maxx, maxy = xywh.bounds\n        xywh = (int(minx), int(miny), int(maxx - minx), int(maxy - miny))\n\n    return self._reader.read_region(xywh=xywh, level=level)\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.level_from_max_dimension","title":"level_from_max_dimension","text":"<pre><code>level_from_max_dimension(max_dimension: int = 4096) -&gt; int\n</code></pre> <p>Find pyramid level with both dimensions less or equal to <code>max_dimension</code>. If one isn't found, return the last pyramid level.</p> <p>Parameters:</p> Name Type Description Default <code>max_dimension</code> <code>int</code> <p>Maximum dimension for the level.</p> <code>4096</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Slide pyramid level.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def level_from_max_dimension(self, max_dimension: int = 4096) -&gt; int:\n    \"\"\"Find pyramid level with *both* dimensions less or equal to `max_dimension`.\n    If one isn't found, return the last pyramid level.\n\n    Parameters:\n        max_dimension (int):\n            Maximum dimension for the level.\n\n    Returns:\n        int:\n            Slide pyramid level.\n    \"\"\"\n    for level, (level_h, level_w) in self.level_dimensions.items():\n        if level_h &lt;= max_dimension and level_w &lt;= max_dimension:\n            return level\n    return list(self.level_dimensions.keys())[-1]\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.level_from_dimensions","title":"level_from_dimensions","text":"<pre><code>level_from_dimensions(dimensions: tuple[int, int]) -&gt; int\n</code></pre> <p>Find pyramid level which is closest to <code>dimensions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>tuple[int, int]</code> <p>Height and width.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Slide pyramid level.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def level_from_dimensions(self, dimensions: tuple[int, int]) -&gt; int:\n    \"\"\"Find pyramid level which is closest to `dimensions`.\n\n    Parameters:\n        dimensions (tuple[int, int]):\n            Height and width.\n\n    Returns:\n        int:\n            Slide pyramid level.\n    \"\"\"\n    height, width = dimensions\n    available = []\n    distances = []\n    for level, (level_h, level_w) in self.level_dimensions.items():\n        available.append(level)\n        distances.append(abs(level_h - height) + abs(level_w - width))\n    return available[distances.index(min(distances))]\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.get_tissue_mask","title":"get_tissue_mask","text":"<pre><code>get_tissue_mask(*, level: Optional[int] = None, threshold: Optional[int] = None, multiplier: float = 1.05, sigma: float = 0.0) -&gt; tuple[int, np.ndarray]\n</code></pre> <p>Detect tissue from slide pyramid level image.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Slide pyramid level to use for tissue detection. If None, uses the <code>level_from_max_dimension</code> method.</p> <code>None</code> <code>threshold</code> <code>int</code> <p>Threshold for tissue detection. If set, will detect tissue by global thresholding. Otherwise Otsu's method is used to find a threshold.</p> <code>None</code> <code>multiplier</code> <code>float</code> <p>Otsu's method finds an optimal threshold by minimizing the weighted within-class variance. This threshold is then multiplied with <code>multiplier</code>. Ignored if <code>threshold</code> is not None.</p> <code>1.05</code> <code>sigma</code> <code>float</code> <p>Sigma for gaussian blurring.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Threshold not between 0 and 255.</p> <p>Returns:</p> Type Description <code>tuple[int, ndarray]</code> <p>tuple[int, np.ndarray]: Threshold and tissue mask.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def get_tissue_mask(\n    self,\n    *,\n    level: Optional[int] = None,\n    threshold: Optional[int] = None,\n    multiplier: float = 1.05,\n    sigma: float = 0.0,\n) -&gt; tuple[int, np.ndarray]:\n    \"\"\"Detect tissue from slide pyramid level image.\n\n    Parameters:\n        level (int):\n            Slide pyramid level to use for tissue detection. If None, uses the\n            `level_from_max_dimension` method.\n        threshold (int):\n            Threshold for tissue detection. If set, will detect tissue by global\n            thresholding. Otherwise Otsu's method is used to find a threshold.\n        multiplier (float):\n            Otsu's method finds an optimal threshold by minimizing the weighted\n            within-class variance. This threshold is then multiplied with\n            `multiplier`. Ignored if `threshold` is not None.\n        sigma (float):\n            Sigma for gaussian blurring.\n\n    Raises:\n        ValueError: Threshold not between 0 and 255.\n\n    Returns:\n        tuple[int, np.ndarray]:\n            Threshold and tissue mask.\n    \"\"\"\n    level = (\n        self.level_from_max_dimension()\n        if level is None\n        else format_level(level, available=list(self.level_dimensions))\n    )\n    return get_tissue_mask(\n        image=self.read_level(level),\n        threshold=threshold,\n        multiplier=multiplier,\n        sigma=sigma,\n    )\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.get_tile_coordinates","title":"get_tile_coordinates","text":"<pre><code>get_tile_coordinates(width: int, *, tissue_mask: Optional[ndarray], annotations: Optional[Polygon] = None, height: Optional[int] = None, overlap: float = 0.0, max_background: float = 0.95, out_of_bounds: bool = True) -&gt; TileCoordinates\n</code></pre> <p>Generate tile coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Width of a tile.</p> required <code>tissue_mask</code> <code>ndarray</code> <p>Tissue mask for filtering tiles with too much background. If None, the filtering is disabled.</p> required <code>annotations</code> <code>Optional[Polygon]</code> <p>Annotations to filter tiles by. If provided, only tiles that intersect with the annotations will be returned.</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of a tile. If None, will be set to <code>width</code>.</p> <code>None</code> <code>overlap</code> <code>float</code> <p>Overlap between neighbouring tiles.</p> <code>0.0</code> <code>max_background</code> <code>float</code> <p>Maximum proportion of background in tiles. Ignored if <code>tissue_mask</code> is None.</p> <code>0.95</code> <code>out_of_bounds</code> <code>bool</code> <p>Keep tiles which contain regions outside of the image.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Height and/or width are smaller than 1.</p> <code>ValueError</code> <p>Height and/or width is larger than dimensions.</p> <code>ValueError</code> <p>Overlap is not in range [0, 1).</p> <p>Returns:</p> Name Type Description <code>TileCoordinates</code> <code>TileCoordinates</code> <p><code>TileCoordinates</code> dataclass.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def get_tile_coordinates(\n    self,\n    width: int,\n    *,\n    tissue_mask: Optional[np.ndarray],\n    annotations: Optional[Polygon] = None,\n    height: Optional[int] = None,\n    overlap: float = 0.0,\n    max_background: float = 0.95,\n    out_of_bounds: bool = True,\n) -&gt; TileCoordinates:\n    \"\"\"Generate tile coordinates.\n\n    Parameters:\n        width (int):\n            Width of a tile.\n        tissue_mask (np.ndarray):\n            Tissue mask for filtering tiles with too much background. If None,\n            the filtering is disabled.\n        annotations (Optional[Polygon]):\n            Annotations to filter tiles by. If provided, only tiles that intersect\n            with the annotations will be returned.\n        height (int):\n            Height of a tile. If None, will be set to `width`.\n        overlap (float):\n            Overlap between neighbouring tiles.\n        max_background (float):\n            Maximum proportion of background in tiles. Ignored if `tissue_mask`\n            is None.\n        out_of_bounds (bool):\n            Keep tiles which contain regions outside of the image.\n\n    Raises:\n        ValueError: Height and/or width are smaller than 1.\n        ValueError: Height and/or width is larger than dimensions.\n        ValueError: Overlap is not in range [0, 1).\n\n    Returns:\n        TileCoordinates:\n            `TileCoordinates` dataclass.\n    \"\"\"\n    tile_coordinates = get_tile_coordinates(\n        dimensions=self.dimensions,\n        width=width,\n        height=height,\n        overlap=overlap,\n        out_of_bounds=out_of_bounds,\n    )\n    if tissue_mask is not None:\n        all_backgrounds = get_background_percentages(\n            tile_coordinates=tile_coordinates,\n            tissue_mask=tissue_mask,\n            downsample=get_downsample(tissue_mask, self.dimensions),\n        )\n        filtered_coordinates = []\n        for xywh, background in zip(tile_coordinates, all_backgrounds):\n            if background &lt;= max_background:\n                filtered_coordinates.append(xywh)\n        tile_coordinates = filtered_coordinates\n\n    if annotations is not None:\n        # Convert tile coordinates to polygons\n        tiles_gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": [\n                    box(x, y, x + w, y + h) for x, y, w, h in tile_coordinates\n                ]\n            }\n        )\n\n        # Filter tiles that intersect with the annotation bbox\n        filtered_tiles = tiles_gdf[tiles_gdf.intersects(annotations)]\n\n        tile_coordinates = [\n            (\n                int(poly.bounds[0]),\n                int(poly.bounds[1]),\n                int(poly.bounds[2] - poly.bounds[0]),\n                int(poly.bounds[3] - poly.bounds[1]),\n            )\n            for poly in filtered_tiles.geometry\n        ]\n\n    return TileCoordinates(\n        coordinates=tile_coordinates,\n        width=width,\n        height=width if height is None else height,\n        overlap=overlap,\n        max_background=None if tissue_mask is None else max_background,\n        tissue_mask=tissue_mask,\n    )\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.get_spot_coordinates","title":"get_spot_coordinates","text":"<pre><code>get_spot_coordinates(tissue_mask: ndarray, *, min_area_pixel: int = 10, max_area_pixel: Optional[int] = None, min_area_relative: float = 0.2, max_area_relative: Optional[float] = 2.0) -&gt; SpotCoordinates\n</code></pre> <p>Generate tissue microarray spot coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>tissue_mask</code> <code>ndarray</code> <p>Tissue mask of the slide. It's recommended to increase <code>sigma</code> value when detecting tissue to remove non-TMA spots from the mask. Rest of the areas can be handled with the following arguments.</p> required <code>min_area_pixel</code> <code>int</code> <p>Minimum pixel area for contours.</p> <code>10</code> <code>max_area_pixel</code> <code>int</code> <p>Maximum pixel area for contours.</p> <code>None</code> <code>min_area_relative</code> <code>float</code> <p>Relative minimum contour area, calculated from the median contour area after filtering contours with <code>[min,max]_pixel</code> arguments (<code>min_area_relative * median(contour_areas)</code>).</p> <code>0.2</code> <code>max_area_relative</code> <code>float</code> <p>Relative maximum contour area, calculated from the median contour area after filtering contours with <code>[min,max]_pixel</code> arguments (<code>max_area_relative * median(contour_areas)</code>).</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>SpotCoordinates</code> <code>SpotCoordinates</code> <p><code>SpotCoordinates</code> instance.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def get_spot_coordinates(\n    self,\n    tissue_mask: np.ndarray,\n    *,\n    min_area_pixel: int = 10,\n    max_area_pixel: Optional[int] = None,\n    min_area_relative: float = 0.2,\n    max_area_relative: Optional[float] = 2.0,\n) -&gt; SpotCoordinates:\n    \"\"\"Generate tissue microarray spot coordinates.\n\n    Parameters:\n        tissue_mask:\n            Tissue mask of the slide. It's recommended to increase `sigma` value when\n            detecting tissue to remove non-TMA spots from the mask. Rest of the areas\n            can be handled with the following arguments.\n        min_area_pixel (int):\n            Minimum pixel area for contours.\n        max_area_pixel (int):\n            Maximum pixel area for contours.\n        min_area_relative (float):\n            Relative minimum contour area, calculated from the median contour area\n            after filtering contours with `[min,max]_pixel` arguments\n            (`min_area_relative * median(contour_areas)`).\n        max_area_relative (float):\n            Relative maximum contour area, calculated from the median contour area\n            after filtering contours with `[min,max]_pixel` arguments\n            (`max_area_relative * median(contour_areas)`).\n\n    Returns:\n        SpotCoordinates:\n            `SpotCoordinates` instance.\n    \"\"\"\n    spot_mask = clean_tissue_mask(\n        tissue_mask=tissue_mask,\n        min_area_pixel=min_area_pixel,\n        max_area_pixel=max_area_pixel,\n        min_area_relative=min_area_relative,\n        max_area_relative=max_area_relative,\n    )\n    # Dearray spots.\n    spot_info = get_spot_coordinates(spot_mask)\n    spot_coordinates = [  # upsample to level zero.\n        _multiply_xywh(x, get_downsample(tissue_mask, self.dimensions))\n        for x in spot_info.values()\n    ]\n\n    return SpotCoordinates(\n        coordinates=spot_coordinates,\n        spot_names=list(spot_info.keys()),\n        tissue_mask=spot_mask,\n    )\n</code></pre>"},{"location":"api/wsi/slide_reader/#histolytics.wsi.slide_reader.SlideReader.get_annotated_thumbnail","title":"get_annotated_thumbnail","text":"<pre><code>get_annotated_thumbnail(image: ndarray, coordinates: Iterator[tuple[int, int, int, int]], linewidth: int = 1, cmap: str = None, values: ndarray = None, breaks: Sequence[float] = None) -&gt; Image.Image\n</code></pre> <p>Generate annotated thumbnail from coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image.</p> required <code>coordinates</code> <code>Iterator[tuple[int, int, int, int]]</code> <p>Coordinates to annotate.</p> required <code>linewidth</code> <code>int</code> <p>Width of rectangle lines.</p> <code>1</code> <code>cmap</code> <code>str</code> <p>Colormap to use for the annotation.</p> <code>None</code> <code>values</code> <code>ndarray</code> <p>Values to use for cmap. Needs to be the same length as <code>coordinates</code>.</p> <code>None</code> <code>breaks</code> <code>Sequence[float]</code> <p>Breakpoints for the colormap.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL.Image.Image: Annotated thumbnail.</p> Source code in <code>src/histolytics/wsi/slide_reader.py</code> <pre><code>def get_annotated_thumbnail(\n    self,\n    image: np.ndarray,\n    coordinates: Iterator[tuple[int, int, int, int]],\n    linewidth: int = 1,\n    cmap: str = None,\n    values: np.ndarray = None,\n    breaks: Sequence[float] = None,\n) -&gt; Image.Image:\n    \"\"\"Generate annotated thumbnail from coordinates.\n\n    Parameters:\n        image (np.ndarray):\n            Input image.\n        coordinates (Iterator[tuple[int, int, int, int]]):\n            Coordinates to annotate.\n        linewidth (int):\n            Width of rectangle lines.\n        cmap (str):\n            Colormap to use for the annotation.\n        values (np.ndarray):\n            Values to use for cmap. Needs to be the same length as `coordinates`.\n        breaks (Sequence[float]):\n            Breakpoints for the colormap.\n\n    Returns:\n        PIL.Image.Image:\n            Annotated thumbnail.\n    \"\"\"\n    if isinstance(coordinates, gpd.GeoDataFrame):\n        coordinates = coordinates.geometry.apply(_polygon_to_xywh).tolist()\n\n    kwargs = {\n        \"image\": image,\n        \"downsample\": get_downsample(image, self.dimensions),\n        \"rectangle_width\": linewidth,\n        \"cmap\": cmap,\n        \"values\": values,\n        \"breaks\": breaks,\n    }\n    if isinstance(coordinates, SpotCoordinates):\n        text_items = [x.lstrip(\"spot_\") for x in coordinates.spot_names]\n        kwargs.update(\n            {\"coordinates\": coordinates.coordinates, \"text_items\": text_items}\n        )\n    elif isinstance(coordinates, TileCoordinates):\n        kwargs.update(\n            {\"coordinates\": coordinates.coordinates, \"highlight_first\": True}\n        )\n    else:\n        kwargs.update({\"coordinates\": coordinates})\n    return get_annotated_image(**kwargs)\n</code></pre>"},{"location":"api/wsi/wsi_processor/","title":"WSIGridProcessor","text":""},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor","title":"histolytics.wsi.wsi_processor.WSIGridProcessor","text":"Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>class WSIGridProcessor:\n    def __init__(\n        self,\n        slide_reader: SlideReader,\n        grid: gpd.GeoDataFrame,\n        nuclei: gpd.GeoDataFrame,\n        pipeline_func: Callable,\n        tissue: gpd.GeoDataFrame = None,\n        nuclei_classes: Dict[str, int] = None,\n        tissue_classes: Dict[str, int] = None,\n        batch_size: int = 8,\n        num_workers: int = 8,\n        pin_memory: bool = True,\n        shuffle: bool = False,\n        drop_last: bool = False,\n    ):\n        \"\"\"Context manager for processing WSI grid cells.\n\n        Parameters:\n            slide_reader (SlideReader):\n                SlideReader instance.\n            grid (GeoDataFrame):\n                A grid GeoDataFrame containing rectangular grid cells.\n            nuclei (GeoDataFrame):\n                A GeoDataFrame containing nuclei data.\n            tissue (GeoDataFrame):\n                A GeoDataFrame containing tissue data.\n            nuclei_classes (Dict[str, int]):\n                A dictionary mapping nuclei class names to integers.\n            tissue_classes (Dict[str, int]):\n                A dictionary mapping tissue class names to integers.\n            batch_size (int):\n                The batch size for processing.\n            num_workers (int):\n                The number of worker processes.\n            pin_memory (bool):\n                Whether to pin memory for faster GPU transfer.\n            shuffle (bool):\n                Whether to shuffle the data.\n            drop_last (bool):\n                Whether to drop the last incomplete batch.\n\n        Examples:\n            &gt;&gt;&gt; from tqdm import tqdm\n            &gt;&gt;&gt; from histolystics.wsi.wsi_processor import WSIGridProcessor\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # ...  initialize reader, grid_gdf etc.\n            &gt;&gt;&gt; crop_loader = WSIGridProcessor(\n            ...     slide_reader=reader, # SlideReader object\n            ...     grid=grid_gdf, # GeoDataFrame containing grid cells\n            ...     nuclei=nuc_gdf, # GeoDataFrame containing nuclei data\n            ...     nuclei_classes=nuclei_classes, # Mapping of nuclei class names to integers\n            ...     pipeline_func=partial(chromatin_feats, metrics=(\"chrom_area\", \"chrom_nuc_prop\")),\n            ...     batch_size=8,\n            ...     num_workers=8,\n            ...     pin_memory=False,\n            ...     shuffle=False,\n            ...     drop_last=False,\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; crop_feats = []\n            &gt;&gt;&gt; with crop_loader as loader:\n            &gt;&gt;&gt;     with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:\n            &gt;&gt;&gt;         for batch_idx, batch in enumerate(pbar):\n            &gt;&gt;&gt;             crop_feats.append(batch)\n        \"\"\"\n        self.slide_reader = slide_reader\n        self.grid = grid\n        self.nuclei = nuclei\n        self.tissue = tissue\n        self.nuclei_classes = nuclei_classes or {}\n        self.tissue_classes = tissue_classes or {}\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.pin_memory = pin_memory\n        self.shuffle = shuffle\n        self.drop_last = drop_last\n        self.pipeline_func = pipeline_func\n\n        # Internal state\n        self._dataset = None\n        self._loader = None\n        self._iterator = None\n\n    def __enter__(self):\n        \"\"\"Enter the context manager and initialize the dataset and loader.\"\"\"\n        # Create the dataset\n        self._dataset = WSIGridDataset(\n            slider_reader=self.slide_reader,\n            grid=self.grid,\n            nuclei=self.nuclei,\n            pipeline_func=self.pipeline_func,\n            tissue=self.tissue,\n            nuclei_classes=self.nuclei_classes,\n            tissue_classes=self.tissue_classes,\n        )\n\n        # Create the loader\n        self._loader = NodesDataLoader(\n            dataset=self._dataset,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            drop_last=self.drop_last,\n            collate=self._dataset.collate,\n        )\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Exit the context manager and clean up resources.\"\"\"\n        # Clean up iterator\n        if self._iterator is not None:\n            del self._iterator\n            self._iterator = None\n\n        # Clean up loader\n        if self._loader is not None:\n            del self._loader\n            self._loader = None\n\n        # Clean up dataset\n        if self._dataset is not None:\n            del self._dataset\n            self._dataset = None\n\n        # Force garbage collection\n        gc.collect()\n\n        # Return False to propagate any exceptions\n        return False\n\n    def __iter__(self):\n        \"\"\"Make the class iterable.\"\"\"\n        if self._loader is None:\n            raise RuntimeError(\"Context manager not entered. Use 'with' statement.\")\n\n        self._iterator = iter(self._loader)\n        return self\n\n    def __next__(self):\n        \"\"\"Get the next batch.\"\"\"\n        if self._iterator is None:\n            raise RuntimeError(\n                \"Iterator not initialized. Use 'with' statement and iterate.\"\n            )\n\n        return next(self._iterator)\n\n    def __len__(self):\n        \"\"\"Get the total number of batches.\"\"\"\n        if self._dataset is None:\n            raise RuntimeError(\"Context manager not entered. Use 'with' statement.\")\n\n        return int(np.ceil(len(self._dataset) / self.batch_size))\n\n    @property\n    def total_samples(self):\n        \"\"\"Get the total number of samples (grid cells).\"\"\"\n        if self._dataset is None:\n            raise RuntimeError(\"Context manager not entered. Use 'with' statement.\")\n\n        return len(self._dataset)\n\n    def get_single_item(self, index: int):\n        \"\"\"Get a single item by index without batching.\"\"\"\n        if self._dataset is None:\n            raise RuntimeError(\"Context manager not entered. Use 'with' statement.\")\n\n        return self._dataset[index]\n</code></pre>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.total_samples","title":"total_samples  <code>property</code>","text":"<pre><code>total_samples\n</code></pre> <p>Get the total number of samples (grid cells).</p>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.__init__","title":"__init__","text":"<pre><code>__init__(slide_reader: SlideReader, grid: GeoDataFrame, nuclei: GeoDataFrame, pipeline_func: Callable, tissue: GeoDataFrame = None, nuclei_classes: Dict[str, int] = None, tissue_classes: Dict[str, int] = None, batch_size: int = 8, num_workers: int = 8, pin_memory: bool = True, shuffle: bool = False, drop_last: bool = False)\n</code></pre> <p>Context manager for processing WSI grid cells.</p> <p>Parameters:</p> Name Type Description Default <code>slide_reader</code> <code>SlideReader</code> <p>SlideReader instance.</p> required <code>grid</code> <code>GeoDataFrame</code> <p>A grid GeoDataFrame containing rectangular grid cells.</p> required <code>nuclei</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing nuclei data.</p> required <code>tissue</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing tissue data.</p> <code>None</code> <code>nuclei_classes</code> <code>Dict[str, int]</code> <p>A dictionary mapping nuclei class names to integers.</p> <code>None</code> <code>tissue_classes</code> <code>Dict[str, int]</code> <p>A dictionary mapping tissue class names to integers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size for processing.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>The number of worker processes.</p> <code>8</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory for faster GPU transfer.</p> <code>True</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from tqdm import tqdm\n&gt;&gt;&gt; from histolystics.wsi.wsi_processor import WSIGridProcessor\n&gt;&gt;&gt;\n&gt;&gt;&gt; # ...  initialize reader, grid_gdf etc.\n&gt;&gt;&gt; crop_loader = WSIGridProcessor(\n...     slide_reader=reader, # SlideReader object\n...     grid=grid_gdf, # GeoDataFrame containing grid cells\n...     nuclei=nuc_gdf, # GeoDataFrame containing nuclei data\n...     nuclei_classes=nuclei_classes, # Mapping of nuclei class names to integers\n...     pipeline_func=partial(chromatin_feats, metrics=(\"chrom_area\", \"chrom_nuc_prop\")),\n...     batch_size=8,\n...     num_workers=8,\n...     pin_memory=False,\n...     shuffle=False,\n...     drop_last=False,\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; crop_feats = []\n&gt;&gt;&gt; with crop_loader as loader:\n&gt;&gt;&gt;     with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:\n&gt;&gt;&gt;         for batch_idx, batch in enumerate(pbar):\n&gt;&gt;&gt;             crop_feats.append(batch)\n</code></pre> Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>def __init__(\n    self,\n    slide_reader: SlideReader,\n    grid: gpd.GeoDataFrame,\n    nuclei: gpd.GeoDataFrame,\n    pipeline_func: Callable,\n    tissue: gpd.GeoDataFrame = None,\n    nuclei_classes: Dict[str, int] = None,\n    tissue_classes: Dict[str, int] = None,\n    batch_size: int = 8,\n    num_workers: int = 8,\n    pin_memory: bool = True,\n    shuffle: bool = False,\n    drop_last: bool = False,\n):\n    \"\"\"Context manager for processing WSI grid cells.\n\n    Parameters:\n        slide_reader (SlideReader):\n            SlideReader instance.\n        grid (GeoDataFrame):\n            A grid GeoDataFrame containing rectangular grid cells.\n        nuclei (GeoDataFrame):\n            A GeoDataFrame containing nuclei data.\n        tissue (GeoDataFrame):\n            A GeoDataFrame containing tissue data.\n        nuclei_classes (Dict[str, int]):\n            A dictionary mapping nuclei class names to integers.\n        tissue_classes (Dict[str, int]):\n            A dictionary mapping tissue class names to integers.\n        batch_size (int):\n            The batch size for processing.\n        num_workers (int):\n            The number of worker processes.\n        pin_memory (bool):\n            Whether to pin memory for faster GPU transfer.\n        shuffle (bool):\n            Whether to shuffle the data.\n        drop_last (bool):\n            Whether to drop the last incomplete batch.\n\n    Examples:\n        &gt;&gt;&gt; from tqdm import tqdm\n        &gt;&gt;&gt; from histolystics.wsi.wsi_processor import WSIGridProcessor\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # ...  initialize reader, grid_gdf etc.\n        &gt;&gt;&gt; crop_loader = WSIGridProcessor(\n        ...     slide_reader=reader, # SlideReader object\n        ...     grid=grid_gdf, # GeoDataFrame containing grid cells\n        ...     nuclei=nuc_gdf, # GeoDataFrame containing nuclei data\n        ...     nuclei_classes=nuclei_classes, # Mapping of nuclei class names to integers\n        ...     pipeline_func=partial(chromatin_feats, metrics=(\"chrom_area\", \"chrom_nuc_prop\")),\n        ...     batch_size=8,\n        ...     num_workers=8,\n        ...     pin_memory=False,\n        ...     shuffle=False,\n        ...     drop_last=False,\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; crop_feats = []\n        &gt;&gt;&gt; with crop_loader as loader:\n        &gt;&gt;&gt;     with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:\n        &gt;&gt;&gt;         for batch_idx, batch in enumerate(pbar):\n        &gt;&gt;&gt;             crop_feats.append(batch)\n    \"\"\"\n    self.slide_reader = slide_reader\n    self.grid = grid\n    self.nuclei = nuclei\n    self.tissue = tissue\n    self.nuclei_classes = nuclei_classes or {}\n    self.tissue_classes = tissue_classes or {}\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    self.pin_memory = pin_memory\n    self.shuffle = shuffle\n    self.drop_last = drop_last\n    self.pipeline_func = pipeline_func\n\n    # Internal state\n    self._dataset = None\n    self._loader = None\n    self._iterator = None\n</code></pre>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Enter the context manager and initialize the dataset and loader.</p> Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>def __enter__(self):\n    \"\"\"Enter the context manager and initialize the dataset and loader.\"\"\"\n    # Create the dataset\n    self._dataset = WSIGridDataset(\n        slider_reader=self.slide_reader,\n        grid=self.grid,\n        nuclei=self.nuclei,\n        pipeline_func=self.pipeline_func,\n        tissue=self.tissue,\n        nuclei_classes=self.nuclei_classes,\n        tissue_classes=self.tissue_classes,\n    )\n\n    # Create the loader\n    self._loader = NodesDataLoader(\n        dataset=self._dataset,\n        batch_size=self.batch_size,\n        shuffle=self.shuffle,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=self.drop_last,\n        collate=self._dataset.collate,\n    )\n\n    return self\n</code></pre>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Exit the context manager and clean up resources.</p> Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Exit the context manager and clean up resources.\"\"\"\n    # Clean up iterator\n    if self._iterator is not None:\n        del self._iterator\n        self._iterator = None\n\n    # Clean up loader\n    if self._loader is not None:\n        del self._loader\n        self._loader = None\n\n    # Clean up dataset\n    if self._dataset is not None:\n        del self._dataset\n        self._dataset = None\n\n    # Force garbage collection\n    gc.collect()\n\n    # Return False to propagate any exceptions\n    return False\n</code></pre>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Make the class iterable.</p> Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>def __iter__(self):\n    \"\"\"Make the class iterable.\"\"\"\n    if self._loader is None:\n        raise RuntimeError(\"Context manager not entered. Use 'with' statement.\")\n\n    self._iterator = iter(self._loader)\n    return self\n</code></pre>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.__next__","title":"__next__","text":"<pre><code>__next__()\n</code></pre> <p>Get the next batch.</p> Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>def __next__(self):\n    \"\"\"Get the next batch.\"\"\"\n    if self._iterator is None:\n        raise RuntimeError(\n            \"Iterator not initialized. Use 'with' statement and iterate.\"\n        )\n\n    return next(self._iterator)\n</code></pre>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Get the total number of batches.</p> Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>def __len__(self):\n    \"\"\"Get the total number of batches.\"\"\"\n    if self._dataset is None:\n        raise RuntimeError(\"Context manager not entered. Use 'with' statement.\")\n\n    return int(np.ceil(len(self._dataset) / self.batch_size))\n</code></pre>"},{"location":"api/wsi/wsi_processor/#histolytics.wsi.wsi_processor.WSIGridProcessor.get_single_item","title":"get_single_item","text":"<pre><code>get_single_item(index: int)\n</code></pre> <p>Get a single item by index without batching.</p> Source code in <code>src/histolytics/wsi/wsi_processor.py</code> <pre><code>def get_single_item(self, index: int):\n    \"\"\"Get a single item by index without batching.\"\"\"\n    if self._dataset is None:\n        raise RuntimeError(\"Context manager not entered. Use 'with' statement.\")\n\n    return self._dataset[index]\n</code></pre>"},{"location":"api/wsi/wsi_segmenter/","title":"WSIPanopticSegmenter","text":""},{"location":"api/wsi/wsi_segmenter/#histolytics.wsi.wsi_segmenter.WsiPanopticSegmenter","title":"histolytics.wsi.wsi_segmenter.WsiPanopticSegmenter","text":"Source code in <code>src/histolytics/wsi/wsi_segmenter.py</code> <pre><code>class WsiPanopticSegmenter:\n    def __init__(\n        self,\n        reader: SlideReader,\n        model: BaseModelPanoptic,\n        level: int,\n        coordinates: List[Tuple[int, int, int, int]],\n        batch_size: int = 8,\n        transforms: A.Compose = None,\n    ) -&gt; None:\n        \"\"\"Class handling the panoptic segmentation of WSIs.\n\n        Parameters:\n            reader (SlideReader):\n                The `SlideReader` object for reading the WSIs.\n            model (BaseModelPanoptic):\n                The model for segmentation.\n            level (int):\n                The level of the WSI to segment.\n            coordinates (List[Tuple[int, int, int, int]]):\n                The bounding box coordinates from `reader.get_tile_coordinates()`.\n            batch_size (int):\n                The batch size for the DataLoader.\n            transforms (A.Compose):\n                The transformations for the input patches.\n        \"\"\"\n        if not has_albu:\n            warnings.warn(\n                \"The albumentations lib is needed to apply transformations. \"\n                \"Setting transforms=None\"\n            )\n            transforms = None\n\n        self.batch_size = batch_size\n        self.coordinates = coordinates\n        self.model = model\n\n        self.dataset = WSIDatasetInfer(\n            reader, coordinates, level=level, transforms=transforms\n        )\n        self.dataloader = DataLoader(\n            self.dataset, batch_size=batch_size, shuffle=False, pin_memory=True\n        )\n        self._has_processed = False\n\n    def segment(\n        self,\n        save_dir: str,\n        use_sliding_win: bool = False,\n        window_size: Tuple[int, int] = None,\n        stride: int = None,\n        use_async_postproc: bool = True,\n        postproc_njobs: int = 4,\n        postproc_start_method: str = \"threading\",\n        class_dict_nuc: Dict[int, str] = None,\n        class_dict_cyto: Dict[int, str] = None,\n        class_dict_tissue: Dict[int, str] = None,\n    ) -&gt; None:\n        \"\"\"Segment the WSIs and save the instances as parquet files to `save_dir`.\n\n        Parameters:\n            save_dir (str):\n                The directory to save the output segmentations in .parquet-format.\n        \"\"\"\n        save_dir = Path(save_dir)\n        tissue_dir = save_dir / \"tissue\"\n        nuc_dir = save_dir / \"nuc\"\n        cyto_dir = save_dir / \"cyto\"\n        tissue_dir.mkdir(parents=True, exist_ok=True)\n        nuc_dir.mkdir(parents=True, exist_ok=True)\n        cyto_dir.mkdir(parents=True, exist_ok=True)\n\n        with tqdm(self.dataloader, unit=\"batch\") as loader:\n            with torch.no_grad():\n                for data in loader:\n                    im = data[\"image\"].to(self.model.device).permute(0, 3, 1, 2).float()\n                    coords = data[\"coords\"]\n                    names = data[\"name\"]\n\n                    # set args\n                    save_paths_nuc = [\n                        (\n                            nuc_dir / f\"{n}_x{c[0]}-y{c[1]}_w{c[2]}-h{c[3]}_nuc\"\n                        ).with_suffix(\".parquet\")\n                        for n, c in zip(names, coords)\n                    ]\n                    save_paths_tissue = [\n                        (\n                            tissue_dir / f\"{n}_x{c[0]}-y{c[1]}_w{c[2]}-h{c[3]}_tissue\"\n                        ).with_suffix(\".parquet\")\n                        for n, c in zip(names, coords)\n                    ]\n                    save_paths_cyto = [\n                        (\n                            cyto_dir / f\"{n}_x{c[0]}-y{c[1]}_w{c[2]}-h{c[3]}_cyto\"\n                        ).with_suffix(\".parquet\")\n                        for n, c in zip(names, coords)\n                    ]\n                    coords = [tuple(map(int, coord)) for coord in coords]\n\n                    # predict\n                    probs = self.model.predict(\n                        im,\n                        use_sliding_win=use_sliding_win,\n                        window_size=window_size,\n                        stride=stride,\n                    )\n\n                    # post-process\n                    self.model.post_process(\n                        probs,\n                        use_async_postproc=use_async_postproc,\n                        start_method=postproc_start_method,\n                        n_jobs=postproc_njobs,\n                        save_paths_nuc=save_paths_nuc,\n                        save_paths_cyto=save_paths_cyto,\n                        save_paths_tissue=save_paths_tissue,\n                        coords=coords,\n                        class_dict_nuc=class_dict_nuc,\n                        class_dict_cyto=class_dict_cyto,\n                        class_dict_tissue=class_dict_tissue,\n                    )\n\n        self._has_processed = True\n\n    def merge_instances(\n        self,\n        src: str,\n        dst: str,\n        clear_in_dir: bool = False,\n        simplify_level: float = 0.3,\n        precision: int = None,\n    ) -&gt; None:\n        \"\"\"Merge the instances at the image boundaries.\n\n        Parameters:\n            src (str):\n                The directory containing the instances segmentations (.parquet-files).\n            dst (str):\n                The destination path for the output file. Allowed formats are\n                '.parquet', '.geojson', and '.feather'.\n            clear_in_dir (bool):\n                Whether to clear the source directory after merging.\n            simplify_level (float):\n                The level of simplification to apply to the merged instances.\n            precision (int):\n                The precision level to apply to the merged instances. If None, no rounding\n                will be made.\n        \"\"\"\n        if not self._has_processed:\n            raise ValueError(\"You must segment the instances first.\")\n\n        in_dir = Path(src)\n        gdf = gpd.read_parquet(in_dir)\n        merger = InstMerger(gdf, self.coordinates)\n        merger.merge(dst, simplify_level=simplify_level, precision=precision)\n\n        if clear_in_dir:\n            for f in in_dir.glob(\"*\"):\n                f.unlink()\n            in_dir.rmdir()\n\n    def merge_tissues(\n        self,\n        src: str,\n        dst: str,\n        clear_in_dir: bool = False,\n        simplify_level: float = 1,\n        precision: int = None,\n    ) -&gt; None:\n        \"\"\"Merge the tissue segmentations.\n\n        Parameters:\n            src (str):\n                The directory containing the tissue segmentations (.parquet-files).\n            dst (str):\n                The destination path for the output file. Allowed formats are\n                '.parquet', '.geojson', and '.feather'.\n            clear_in_dir (bool):\n                Whether to clear the source directory after merging.\n            simplify_level (float):\n                The level of simplification to apply to the merged tissues.\n            precision (int):\n                The precision level to apply to the merged tissues. If None, no rounding\n                will be made.\n        \"\"\"\n        if not self._has_processed:\n            raise ValueError(\"You must segment the instances first.\")\n\n        in_dir = Path(src)\n        gdf = gpd.read_parquet(in_dir)\n        merger = TissueMerger(gdf, self.coordinates)\n        merger.merge(dst, simplify_level=simplify_level, precision=precision)\n\n        if clear_in_dir:\n            for f in in_dir.glob(\"*\"):\n                f.unlink()\n            in_dir.rmdir()\n</code></pre>"},{"location":"api/wsi/wsi_segmenter/#histolytics.wsi.wsi_segmenter.WsiPanopticSegmenter.__init__","title":"__init__","text":"<pre><code>__init__(reader: SlideReader, model: BaseModelPanoptic, level: int, coordinates: List[Tuple[int, int, int, int]], batch_size: int = 8, transforms: Compose = None) -&gt; None\n</code></pre> <p>Class handling the panoptic segmentation of WSIs.</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>SlideReader</code> <p>The <code>SlideReader</code> object for reading the WSIs.</p> required <code>model</code> <code>BaseModelPanoptic</code> <p>The model for segmentation.</p> required <code>level</code> <code>int</code> <p>The level of the WSI to segment.</p> required <code>coordinates</code> <code>List[Tuple[int, int, int, int]]</code> <p>The bounding box coordinates from <code>reader.get_tile_coordinates()</code>.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the DataLoader.</p> <code>8</code> <code>transforms</code> <code>Compose</code> <p>The transformations for the input patches.</p> <code>None</code> Source code in <code>src/histolytics/wsi/wsi_segmenter.py</code> <pre><code>def __init__(\n    self,\n    reader: SlideReader,\n    model: BaseModelPanoptic,\n    level: int,\n    coordinates: List[Tuple[int, int, int, int]],\n    batch_size: int = 8,\n    transforms: A.Compose = None,\n) -&gt; None:\n    \"\"\"Class handling the panoptic segmentation of WSIs.\n\n    Parameters:\n        reader (SlideReader):\n            The `SlideReader` object for reading the WSIs.\n        model (BaseModelPanoptic):\n            The model for segmentation.\n        level (int):\n            The level of the WSI to segment.\n        coordinates (List[Tuple[int, int, int, int]]):\n            The bounding box coordinates from `reader.get_tile_coordinates()`.\n        batch_size (int):\n            The batch size for the DataLoader.\n        transforms (A.Compose):\n            The transformations for the input patches.\n    \"\"\"\n    if not has_albu:\n        warnings.warn(\n            \"The albumentations lib is needed to apply transformations. \"\n            \"Setting transforms=None\"\n        )\n        transforms = None\n\n    self.batch_size = batch_size\n    self.coordinates = coordinates\n    self.model = model\n\n    self.dataset = WSIDatasetInfer(\n        reader, coordinates, level=level, transforms=transforms\n    )\n    self.dataloader = DataLoader(\n        self.dataset, batch_size=batch_size, shuffle=False, pin_memory=True\n    )\n    self._has_processed = False\n</code></pre>"},{"location":"api/wsi/wsi_segmenter/#histolytics.wsi.wsi_segmenter.WsiPanopticSegmenter.segment","title":"segment","text":"<pre><code>segment(save_dir: str, use_sliding_win: bool = False, window_size: Tuple[int, int] = None, stride: int = None, use_async_postproc: bool = True, postproc_njobs: int = 4, postproc_start_method: str = 'threading', class_dict_nuc: Dict[int, str] = None, class_dict_cyto: Dict[int, str] = None, class_dict_tissue: Dict[int, str] = None) -&gt; None\n</code></pre> <p>Segment the WSIs and save the instances as parquet files to <code>save_dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The directory to save the output segmentations in .parquet-format.</p> required Source code in <code>src/histolytics/wsi/wsi_segmenter.py</code> <pre><code>def segment(\n    self,\n    save_dir: str,\n    use_sliding_win: bool = False,\n    window_size: Tuple[int, int] = None,\n    stride: int = None,\n    use_async_postproc: bool = True,\n    postproc_njobs: int = 4,\n    postproc_start_method: str = \"threading\",\n    class_dict_nuc: Dict[int, str] = None,\n    class_dict_cyto: Dict[int, str] = None,\n    class_dict_tissue: Dict[int, str] = None,\n) -&gt; None:\n    \"\"\"Segment the WSIs and save the instances as parquet files to `save_dir`.\n\n    Parameters:\n        save_dir (str):\n            The directory to save the output segmentations in .parquet-format.\n    \"\"\"\n    save_dir = Path(save_dir)\n    tissue_dir = save_dir / \"tissue\"\n    nuc_dir = save_dir / \"nuc\"\n    cyto_dir = save_dir / \"cyto\"\n    tissue_dir.mkdir(parents=True, exist_ok=True)\n    nuc_dir.mkdir(parents=True, exist_ok=True)\n    cyto_dir.mkdir(parents=True, exist_ok=True)\n\n    with tqdm(self.dataloader, unit=\"batch\") as loader:\n        with torch.no_grad():\n            for data in loader:\n                im = data[\"image\"].to(self.model.device).permute(0, 3, 1, 2).float()\n                coords = data[\"coords\"]\n                names = data[\"name\"]\n\n                # set args\n                save_paths_nuc = [\n                    (\n                        nuc_dir / f\"{n}_x{c[0]}-y{c[1]}_w{c[2]}-h{c[3]}_nuc\"\n                    ).with_suffix(\".parquet\")\n                    for n, c in zip(names, coords)\n                ]\n                save_paths_tissue = [\n                    (\n                        tissue_dir / f\"{n}_x{c[0]}-y{c[1]}_w{c[2]}-h{c[3]}_tissue\"\n                    ).with_suffix(\".parquet\")\n                    for n, c in zip(names, coords)\n                ]\n                save_paths_cyto = [\n                    (\n                        cyto_dir / f\"{n}_x{c[0]}-y{c[1]}_w{c[2]}-h{c[3]}_cyto\"\n                    ).with_suffix(\".parquet\")\n                    for n, c in zip(names, coords)\n                ]\n                coords = [tuple(map(int, coord)) for coord in coords]\n\n                # predict\n                probs = self.model.predict(\n                    im,\n                    use_sliding_win=use_sliding_win,\n                    window_size=window_size,\n                    stride=stride,\n                )\n\n                # post-process\n                self.model.post_process(\n                    probs,\n                    use_async_postproc=use_async_postproc,\n                    start_method=postproc_start_method,\n                    n_jobs=postproc_njobs,\n                    save_paths_nuc=save_paths_nuc,\n                    save_paths_cyto=save_paths_cyto,\n                    save_paths_tissue=save_paths_tissue,\n                    coords=coords,\n                    class_dict_nuc=class_dict_nuc,\n                    class_dict_cyto=class_dict_cyto,\n                    class_dict_tissue=class_dict_tissue,\n                )\n\n    self._has_processed = True\n</code></pre>"},{"location":"api/wsi/wsi_segmenter/#histolytics.wsi.wsi_segmenter.WsiPanopticSegmenter.merge_instances","title":"merge_instances","text":"<pre><code>merge_instances(src: str, dst: str, clear_in_dir: bool = False, simplify_level: float = 0.3, precision: int = None) -&gt; None\n</code></pre> <p>Merge the instances at the image boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The directory containing the instances segmentations (.parquet-files).</p> required <code>dst</code> <code>str</code> <p>The destination path for the output file. Allowed formats are '.parquet', '.geojson', and '.feather'.</p> required <code>clear_in_dir</code> <code>bool</code> <p>Whether to clear the source directory after merging.</p> <code>False</code> <code>simplify_level</code> <code>float</code> <p>The level of simplification to apply to the merged instances.</p> <code>0.3</code> <code>precision</code> <code>int</code> <p>The precision level to apply to the merged instances. If None, no rounding will be made.</p> <code>None</code> Source code in <code>src/histolytics/wsi/wsi_segmenter.py</code> <pre><code>def merge_instances(\n    self,\n    src: str,\n    dst: str,\n    clear_in_dir: bool = False,\n    simplify_level: float = 0.3,\n    precision: int = None,\n) -&gt; None:\n    \"\"\"Merge the instances at the image boundaries.\n\n    Parameters:\n        src (str):\n            The directory containing the instances segmentations (.parquet-files).\n        dst (str):\n            The destination path for the output file. Allowed formats are\n            '.parquet', '.geojson', and '.feather'.\n        clear_in_dir (bool):\n            Whether to clear the source directory after merging.\n        simplify_level (float):\n            The level of simplification to apply to the merged instances.\n        precision (int):\n            The precision level to apply to the merged instances. If None, no rounding\n            will be made.\n    \"\"\"\n    if not self._has_processed:\n        raise ValueError(\"You must segment the instances first.\")\n\n    in_dir = Path(src)\n    gdf = gpd.read_parquet(in_dir)\n    merger = InstMerger(gdf, self.coordinates)\n    merger.merge(dst, simplify_level=simplify_level, precision=precision)\n\n    if clear_in_dir:\n        for f in in_dir.glob(\"*\"):\n            f.unlink()\n        in_dir.rmdir()\n</code></pre>"},{"location":"api/wsi/wsi_segmenter/#histolytics.wsi.wsi_segmenter.WsiPanopticSegmenter.merge_tissues","title":"merge_tissues","text":"<pre><code>merge_tissues(src: str, dst: str, clear_in_dir: bool = False, simplify_level: float = 1, precision: int = None) -&gt; None\n</code></pre> <p>Merge the tissue segmentations.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The directory containing the tissue segmentations (.parquet-files).</p> required <code>dst</code> <code>str</code> <p>The destination path for the output file. Allowed formats are '.parquet', '.geojson', and '.feather'.</p> required <code>clear_in_dir</code> <code>bool</code> <p>Whether to clear the source directory after merging.</p> <code>False</code> <code>simplify_level</code> <code>float</code> <p>The level of simplification to apply to the merged tissues.</p> <code>1</code> <code>precision</code> <code>int</code> <p>The precision level to apply to the merged tissues. If None, no rounding will be made.</p> <code>None</code> Source code in <code>src/histolytics/wsi/wsi_segmenter.py</code> <pre><code>def merge_tissues(\n    self,\n    src: str,\n    dst: str,\n    clear_in_dir: bool = False,\n    simplify_level: float = 1,\n    precision: int = None,\n) -&gt; None:\n    \"\"\"Merge the tissue segmentations.\n\n    Parameters:\n        src (str):\n            The directory containing the tissue segmentations (.parquet-files).\n        dst (str):\n            The destination path for the output file. Allowed formats are\n            '.parquet', '.geojson', and '.feather'.\n        clear_in_dir (bool):\n            Whether to clear the source directory after merging.\n        simplify_level (float):\n            The level of simplification to apply to the merged tissues.\n        precision (int):\n            The precision level to apply to the merged tissues. If None, no rounding\n            will be made.\n    \"\"\"\n    if not self._has_processed:\n        raise ValueError(\"You must segment the instances first.\")\n\n    in_dir = Path(src)\n    gdf = gpd.read_parquet(in_dir)\n    merger = TissueMerger(gdf, self.coordinates)\n    merger.merge(dst, simplify_level=simplify_level, precision=precision)\n\n    if clear_in_dir:\n        for f in in_dir.glob(\"*\"):\n            f.unlink()\n        in_dir.rmdir()\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":"<p>Welcome to the Histolytics User Guide. The user guide introduces the essential features of Histolytics. Each section focuses on a specific topic, explaining its implementation in Histolytics with clear, reproducible examples. The user guide is divided in to three main topics:</p> <ul> <li>Segmentation</li> <li>Basics of Spatial Analysis (after you have segmented your data)</li> <li>WSI-level analysis workflow examples</li> </ul>"},{"location":"user_guide/#segmentation","title":"Segmentation","text":"<p>In the Segmentation section, you will find how to:</p> <ul> <li>Get Started with Panoptic Segmentation in Histolytics</li> <li>Selecting Backbone Architectures for Panoptic Segmentation Models</li> <li>Finetuning Panoptic Segmentation Models</li> <li>Selecting Sub Regions of Your WSI for Segmentation</li> <li>WSI-level Panoptic Segmentation: Putting It All Together</li> </ul>"},{"location":"user_guide/#spatial-analysis-basics","title":"Spatial Analysis (Basics)","text":"<p>In the Spatial Analysis (Basics) section, you will find how to:</p> <ul> <li>Apply Spatial Querying to WSI-scale Panoptic Segmentation Maps</li> <li>Apply Spatial Partitioning to Selected Regions of Panoptic Segmentation Maps</li> <li>Apply Legendgrams in your Spatial Plots</li> <li>Fit Graphs to Nuclei Segmentation Data &amp; Extract Graph Features</li> <li>Extract Neighborhood Features from Nuclei Segmentation Data</li> <li>Apply Clustering &amp; Extract Cluster Features from Nuclei Segmentation Data</li> <li>Rasterizing Vector Data</li> <li>Extract Nuclear Features from Nuclei Segmentation Data and H&amp;E  Images</li> <li>Extract Stromal Features from H&amp;E Images</li> <li>Apply Medial Lines to Tissue Segmentation Data</li> </ul>"},{"location":"user_guide/#wsi-analysis-workflows","title":"WSI Analysis Workflows","text":"<p>In the WSI Analysis Workflows, you will find real world examples on feature extraction at WSI-scale:</p>"},{"location":"user_guide/#immuno-oncology-profiling","title":"Immuno-oncology Profiling:","text":"<ul> <li>Spatial Statistics of TILs.</li> <li>Profiling TLS and Lymphoid Aggregates.</li> </ul>"},{"location":"user_guide/#nuclear-pleomorphism","title":"Nuclear Pleomorphism:","text":"<ul> <li>Nuclear Morphology Analysis.</li> <li>Nuclear Chromatin Distribution Analysis.</li> </ul>"},{"location":"user_guide/#tme-characterization","title":"TME Characterization:","text":"<ul> <li>Collagen Fiber Disorder Analysis.</li> <li>Characterization of Desmoplastic Stroma.</li> </ul>"},{"location":"user_guide/#nuclei-neighborhoods","title":"Nuclei Neighborhoods:","text":"<ul> <li>Tumor Cell Accessibility.</li> </ul>"},{"location":"user_guide/seg/backbones/","title":"Model Backbones","text":"<p>Histolytics' modular model implementations allows you to select from a variety of pre-trained backbone encoders. For example, foundation models like UNI, Virchow, or Prov-GigaPath can be used, given that you have been granted the permissions to use these models. In general, any backbone from the pytorch-image-models (timm) library can be used.</p> In\u00a0[1]: Copied! <pre>from huggingface_hub import login\n\n# login to huggingface to load the weights\n# NOTE: You need to have granted permission for the weights before you can run this\nlogin()\n</pre> from huggingface_hub import login  # login to huggingface to load the weights # NOTE: You need to have granted permission for the weights before you can run this login() <pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026</pre> In\u00a0[3]: Copied! <pre>from histolytics.models.cellpose_panoptic import CellposePanoptic\n\ncpose_panoptic = CellposePanoptic(\n    n_nuc_classes=6,\n    n_tissue_classes=6,\n    enc_name=\"hf_hub:MahmoodLab/uni\",\n    enc_pretrain=True,\n    model_kwargs={\n        \"encoder_kws\": {\"init_values\": 1e-5, \"dynamic_img_size\": True},\n        \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from UNI\n    },\n)\n\n# Print only a summary of the model instead of the full output\nprint(str(cpose_panoptic.model)[:500] + \"\\n...\")\n</pre> from histolytics.models.cellpose_panoptic import CellposePanoptic  cpose_panoptic = CellposePanoptic(     n_nuc_classes=6,     n_tissue_classes=6,     enc_name=\"hf_hub:MahmoodLab/uni\",     enc_pretrain=True,     model_kwargs={         \"encoder_kws\": {\"init_values\": 1e-5, \"dynamic_img_size\": True},         \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from UNI     }, )  # Print only a summary of the model instead of the full output print(str(cpose_panoptic.model)[:500] + \"\\n...\") <pre>CellPoseUnet(\n  (hf_hub:MahmoodLab/uni): Encoder(\n    (encoder): TimmEncoder(\n      (encoder): VisionTransformer(\n        (patch_embed): PatchEmbed(\n          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n          (norm): Identity()\n        )\n        (pos_drop): Dropout(p=0.0, inplace=False)\n        (patch_drop): Identity()\n        (norm_pre): Identity()\n        (blocks): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=Tru\n...\n</pre> In\u00a0[4]: Copied! <pre>import torch\nfrom timm.layers import SwiGLUPacked\nfrom histolytics.models.hovernet_panoptic import HoverNetPanoptic\n\nhnet_panoptic = HoverNetPanoptic(\n    n_nuc_classes=6,\n    n_tissue_classes=6,\n    enc_name=\"hf-hub:paige-ai/Virchow\",\n    enc_pretrain=True,\n    model_kwargs={\n        \"encoder_kws\": {\"mlp_layer\": SwiGLUPacked, \"act_layer\": torch.nn.SiLU},\n        \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from Virchow\n    },\n)\n\n# Print only a summary of the model instead of the full output\nprint(str(hnet_panoptic.model)[:500] + \"\\n...\")\n</pre> import torch from timm.layers import SwiGLUPacked from histolytics.models.hovernet_panoptic import HoverNetPanoptic  hnet_panoptic = HoverNetPanoptic(     n_nuc_classes=6,     n_tissue_classes=6,     enc_name=\"hf-hub:paige-ai/Virchow\",     enc_pretrain=True,     model_kwargs={         \"encoder_kws\": {\"mlp_layer\": SwiGLUPacked, \"act_layer\": torch.nn.SiLU},         \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from Virchow     }, )  # Print only a summary of the model instead of the full output print(str(hnet_panoptic.model)[:500] + \"\\n...\") <pre>HoverNetUnet(\n  (hf-hub:paige-ai/Virchow): Encoder(\n    (encoder): TimmEncoder(\n      (encoder): VisionTransformer(\n        (patch_embed): PatchEmbed(\n          (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n          (norm): Identity()\n        )\n        (pos_drop): Dropout(p=0.0, inplace=False)\n        (patch_drop): Identity()\n        (norm_pre): Identity()\n        (blocks): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=T\n...\n</pre> In\u00a0[\u00a0]: Copied! <pre>from histolytics.models.stardist_panoptic import StarDistPanoptic\n\nsdist_panoptic = StarDistPanoptic(\n    n_nuc_classes=6,\n    n_tissue_classes=6,\n    enc_name=\"hf_hub:prov-gigapath/prov-gigapath\",\n    enc_pretrain=True,\n    model_kwargs={\n        \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from Prov-GigaPath\n    },\n)\n\n# Print only a summary of the model instead of the full output\nprint(str(sdist_panoptic.model)[:500] + \"\\n...\")\n</pre> from histolytics.models.stardist_panoptic import StarDistPanoptic  sdist_panoptic = StarDistPanoptic(     n_nuc_classes=6,     n_tissue_classes=6,     enc_name=\"hf_hub:prov-gigapath/prov-gigapath\",     enc_pretrain=True,     model_kwargs={         \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from Prov-GigaPath     }, )  # Print only a summary of the model instead of the full output print(str(sdist_panoptic.model)[:500] + \"\\n...\") <pre>StarDistUnet(\n  (hf_hub:prov-gigapath/prov-gigapath): Encoder(\n    (encoder): TimmEncoder(\n      (encoder): VisionTransformer(\n        (patch_embed): PatchEmbed(\n          (proj): Conv2d(3, 1536, kernel_size=(16, 16), stride=(16, 16))\n          (norm): Identity()\n        )\n        (pos_drop): Dropout(p=0.0, inplace=False)\n        (patch_drop): Identity()\n        (norm_pre): Identity()\n        (blocks): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((1536,), eps=1e-06, elementwi\n...\n</pre> In\u00a0[6]: Copied! <pre>from histolytics.models.cppnet_panoptic import CPPNetPanoptic\n\ncpp_panoptic = CPPNetPanoptic(\n    n_nuc_classes=6,\n    n_tissue_classes=6,\n    enc_name=\"samvit_base_patch16\",\n    enc_pretrain=True,\n    model_kwargs={\n        \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from SAM\n    },\n)\n\n# Print only a summary of the model instead of the full output\nprint(str(cpp_panoptic.model)[:500] + \"\\n...\")\n</pre> from histolytics.models.cppnet_panoptic import CPPNetPanoptic  cpp_panoptic = CPPNetPanoptic(     n_nuc_classes=6,     n_tissue_classes=6,     enc_name=\"samvit_base_patch16\",     enc_pretrain=True,     model_kwargs={         \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from SAM     }, )  # Print only a summary of the model instead of the full output print(str(cpp_panoptic.model)[:500] + \"\\n...\") <pre>CPPNetUnet(\n  (samvit_base_patch16): Encoder(\n    (encoder): TimmEncoder(\n      (encoder): VisionTransformerSAM(\n        (patch_embed): PatchEmbed(\n          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n          (norm): Identity()\n        )\n        (pos_drop): Dropout(p=0.0, inplace=False)\n        (patch_drop): Identity()\n        (norm_pre): Identity()\n        (blocks): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n\n...\n</pre> In\u00a0[7]: Copied! <pre>from histolytics.models.cellvit_panoptic import CellVitPanoptic\n\ncvit_panoptic = CellVitPanoptic(\n    n_nuc_classes=6,\n    n_tissue_classes=6,\n    enc_name=\"samvit_huge_patch16\",\n    enc_pretrain=True,\n    model_kwargs={\n        \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from SAM Huge\n    },\n)\n\n# Print only a summary of the model instead of the full output\nprint(str(cvit_panoptic.model)[:500] + \"\\n...\")\n</pre> from histolytics.models.cellvit_panoptic import CellVitPanoptic  cvit_panoptic = CellVitPanoptic(     n_nuc_classes=6,     n_tissue_classes=6,     enc_name=\"samvit_huge_patch16\",     enc_pretrain=True,     model_kwargs={         \"enc_out_indices\": (2, 4, 6, 8),  # using layers 2, 4, 6, 8 from SAM Huge     }, )  # Print only a summary of the model instead of the full output print(str(cvit_panoptic.model)[:500] + \"\\n...\") <pre>CellVitSamUnet(\n  (samvit_huge_patch16): Encoder(\n    (encoder): TimmEncoder(\n      (encoder): VisionTransformerSAM(\n        (patch_embed): PatchEmbed(\n          (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n          (norm): Identity()\n        )\n        (pos_drop): Dropout(p=0.0, inplace=False)\n        (patch_drop): Identity()\n        (norm_pre): Identity()\n        (blocks): Sequential(\n          (0): Block(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=\n...\n</pre> <p>To start finetuning your model with a pre-trained backbone, see the next chapter about finetuning</p> In\u00a0[8]: Copied! <pre>import timm\n\ntimm.list_models()\n</pre> import timm  timm.list_models() Out[8]: <pre>['aimv2_1b_patch14_224',\n 'aimv2_1b_patch14_336',\n 'aimv2_1b_patch14_448',\n 'aimv2_3b_patch14_224',\n 'aimv2_3b_patch14_336',\n 'aimv2_3b_patch14_448',\n 'aimv2_huge_patch14_224',\n 'aimv2_huge_patch14_336',\n 'aimv2_huge_patch14_448',\n 'aimv2_large_patch14_224',\n 'aimv2_large_patch14_336',\n 'aimv2_large_patch14_448',\n 'bat_resnext26ts',\n 'beit_base_patch16_224',\n 'beit_base_patch16_384',\n 'beit_large_patch16_224',\n 'beit_large_patch16_384',\n 'beit_large_patch16_512',\n 'beitv2_base_patch16_224',\n 'beitv2_large_patch16_224',\n 'botnet26t_256',\n 'botnet50ts_256',\n 'caformer_b36',\n 'caformer_m36',\n 'caformer_s18',\n 'caformer_s36',\n 'cait_m36_384',\n 'cait_m48_448',\n 'cait_s24_224',\n 'cait_s24_384',\n 'cait_s36_384',\n 'cait_xs24_384',\n 'cait_xxs24_224',\n 'cait_xxs24_384',\n 'cait_xxs36_224',\n 'cait_xxs36_384',\n 'coat_lite_medium',\n 'coat_lite_medium_384',\n 'coat_lite_mini',\n 'coat_lite_small',\n 'coat_lite_tiny',\n 'coat_mini',\n 'coat_small',\n 'coat_tiny',\n 'coatnet_0_224',\n 'coatnet_0_rw_224',\n 'coatnet_1_224',\n 'coatnet_1_rw_224',\n 'coatnet_2_224',\n 'coatnet_2_rw_224',\n 'coatnet_3_224',\n 'coatnet_3_rw_224',\n 'coatnet_4_224',\n 'coatnet_5_224',\n 'coatnet_bn_0_rw_224',\n 'coatnet_nano_cc_224',\n 'coatnet_nano_rw_224',\n 'coatnet_pico_rw_224',\n 'coatnet_rmlp_0_rw_224',\n 'coatnet_rmlp_1_rw2_224',\n 'coatnet_rmlp_1_rw_224',\n 'coatnet_rmlp_2_rw_224',\n 'coatnet_rmlp_2_rw_384',\n 'coatnet_rmlp_3_rw_224',\n 'coatnet_rmlp_nano_rw_224',\n 'coatnext_nano_rw_224',\n 'convformer_b36',\n 'convformer_m36',\n 'convformer_s18',\n 'convformer_s36',\n 'convit_base',\n 'convit_small',\n 'convit_tiny',\n 'convmixer_768_32',\n 'convmixer_1024_20_ks9_p14',\n 'convmixer_1536_20',\n 'convnext_atto',\n 'convnext_atto_ols',\n 'convnext_atto_rms',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnext_zepto_rms',\n 'convnext_zepto_rms_ols',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny',\n 'crossvit_9_240',\n 'crossvit_9_dagger_240',\n 'crossvit_15_240',\n 'crossvit_15_dagger_240',\n 'crossvit_15_dagger_408',\n 'crossvit_18_240',\n 'crossvit_18_dagger_240',\n 'crossvit_18_dagger_408',\n 'crossvit_base_240',\n 'crossvit_small_240',\n 'crossvit_tiny_240',\n 'cs3darknet_focus_l',\n 'cs3darknet_focus_m',\n 'cs3darknet_focus_s',\n 'cs3darknet_focus_x',\n 'cs3darknet_l',\n 'cs3darknet_m',\n 'cs3darknet_s',\n 'cs3darknet_x',\n 'cs3edgenet_x',\n 'cs3se_edgenet_x',\n 'cs3sedarknet_l',\n 'cs3sedarknet_x',\n 'cs3sedarknet_xdw',\n 'cspdarknet53',\n 'cspresnet50',\n 'cspresnet50d',\n 'cspresnet50w',\n 'cspresnext50',\n 'darknet17',\n 'darknet21',\n 'darknet53',\n 'darknetaa53',\n 'davit_base',\n 'davit_base_fl',\n 'davit_giant',\n 'davit_huge',\n 'davit_huge_fl',\n 'davit_large',\n 'davit_small',\n 'davit_tiny',\n 'deit3_base_patch16_224',\n 'deit3_base_patch16_384',\n 'deit3_huge_patch14_224',\n 'deit3_large_patch16_224',\n 'deit3_large_patch16_384',\n 'deit3_medium_patch16_224',\n 'deit3_small_patch16_224',\n 'deit3_small_patch16_384',\n 'deit_base_distilled_patch16_224',\n 'deit_base_distilled_patch16_384',\n 'deit_base_patch16_224',\n 'deit_base_patch16_384',\n 'deit_small_distilled_patch16_224',\n 'deit_small_patch16_224',\n 'deit_tiny_distilled_patch16_224',\n 'deit_tiny_patch16_224',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'densenet264d',\n 'densenetblur121d',\n 'dla34',\n 'dla46_c',\n 'dla46x_c',\n 'dla60',\n 'dla60_res2net',\n 'dla60_res2next',\n 'dla60x',\n 'dla60x_c',\n 'dla102',\n 'dla102x',\n 'dla102x2',\n 'dla169',\n 'dm_nfnet_f0',\n 'dm_nfnet_f1',\n 'dm_nfnet_f2',\n 'dm_nfnet_f3',\n 'dm_nfnet_f4',\n 'dm_nfnet_f5',\n 'dm_nfnet_f6',\n 'dpn48b',\n 'dpn68',\n 'dpn68b',\n 'dpn92',\n 'dpn98',\n 'dpn107',\n 'dpn131',\n 'eca_botnext26ts_256',\n 'eca_halonext26ts',\n 'eca_nfnet_l0',\n 'eca_nfnet_l1',\n 'eca_nfnet_l2',\n 'eca_nfnet_l3',\n 'eca_resnet33ts',\n 'eca_resnext26ts',\n 'eca_vovnet39b',\n 'ecaresnet26t',\n 'ecaresnet50d',\n 'ecaresnet50d_pruned',\n 'ecaresnet50t',\n 'ecaresnet101d',\n 'ecaresnet101d_pruned',\n 'ecaresnet200d',\n 'ecaresnet269d',\n 'ecaresnetlight',\n 'ecaresnext26t_32x4d',\n 'ecaresnext50t_32x4d',\n 'edgenext_base',\n 'edgenext_small',\n 'edgenext_small_rw',\n 'edgenext_x_small',\n 'edgenext_xx_small',\n 'efficientformer_l1',\n 'efficientformer_l3',\n 'efficientformer_l7',\n 'efficientformerv2_l',\n 'efficientformerv2_s0',\n 'efficientformerv2_s1',\n 'efficientformerv2_s2',\n 'efficientnet_b0',\n 'efficientnet_b0_g8_gn',\n 'efficientnet_b0_g16_evos',\n 'efficientnet_b0_gn',\n 'efficientnet_b1',\n 'efficientnet_b1_pruned',\n 'efficientnet_b2',\n 'efficientnet_b2_pruned',\n 'efficientnet_b3',\n 'efficientnet_b3_g8_gn',\n 'efficientnet_b3_gn',\n 'efficientnet_b3_pruned',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_b8',\n 'efficientnet_blur_b0',\n 'efficientnet_cc_b0_4e',\n 'efficientnet_cc_b0_8e',\n 'efficientnet_cc_b1_8e',\n 'efficientnet_el',\n 'efficientnet_el_pruned',\n 'efficientnet_em',\n 'efficientnet_es',\n 'efficientnet_es_pruned',\n 'efficientnet_h_b5',\n 'efficientnet_l2',\n 'efficientnet_lite0',\n 'efficientnet_lite1',\n 'efficientnet_lite2',\n 'efficientnet_lite3',\n 'efficientnet_lite4',\n 'efficientnet_x_b3',\n 'efficientnet_x_b5',\n 'efficientnetv2_l',\n 'efficientnetv2_m',\n 'efficientnetv2_rw_m',\n 'efficientnetv2_rw_s',\n 'efficientnetv2_rw_t',\n 'efficientnetv2_s',\n 'efficientnetv2_xl',\n 'efficientvit_b0',\n 'efficientvit_b1',\n 'efficientvit_b2',\n 'efficientvit_b3',\n 'efficientvit_l1',\n 'efficientvit_l2',\n 'efficientvit_l3',\n 'efficientvit_m0',\n 'efficientvit_m1',\n 'efficientvit_m2',\n 'efficientvit_m3',\n 'efficientvit_m4',\n 'efficientvit_m5',\n 'ese_vovnet19b_dw',\n 'ese_vovnet19b_slim',\n 'ese_vovnet19b_slim_dw',\n 'ese_vovnet39b',\n 'ese_vovnet39b_evos',\n 'ese_vovnet57b',\n 'ese_vovnet99b',\n 'eva02_base_patch14_224',\n 'eva02_base_patch14_448',\n 'eva02_base_patch16_clip_224',\n 'eva02_enormous_patch14_clip_224',\n 'eva02_large_patch14_224',\n 'eva02_large_patch14_448',\n 'eva02_large_patch14_clip_224',\n 'eva02_large_patch14_clip_336',\n 'eva02_small_patch14_224',\n 'eva02_small_patch14_336',\n 'eva02_tiny_patch14_224',\n 'eva02_tiny_patch14_336',\n 'eva_giant_patch14_224',\n 'eva_giant_patch14_336',\n 'eva_giant_patch14_560',\n 'eva_giant_patch14_clip_224',\n 'eva_large_patch14_196',\n 'eva_large_patch14_336',\n 'fastvit_ma36',\n 'fastvit_mci0',\n 'fastvit_mci1',\n 'fastvit_mci2',\n 'fastvit_s12',\n 'fastvit_sa12',\n 'fastvit_sa24',\n 'fastvit_sa36',\n 'fastvit_t8',\n 'fastvit_t12',\n 'fbnetc_100',\n 'fbnetv3_b',\n 'fbnetv3_d',\n 'fbnetv3_g',\n 'flexivit_base',\n 'flexivit_large',\n 'flexivit_small',\n 'focalnet_base_lrf',\n 'focalnet_base_srf',\n 'focalnet_huge_fl3',\n 'focalnet_huge_fl4',\n 'focalnet_large_fl3',\n 'focalnet_large_fl4',\n 'focalnet_small_lrf',\n 'focalnet_small_srf',\n 'focalnet_tiny_lrf',\n 'focalnet_tiny_srf',\n 'focalnet_xlarge_fl3',\n 'focalnet_xlarge_fl4',\n 'gc_efficientnetv2_rw_t',\n 'gcresnet33ts',\n 'gcresnet50t',\n 'gcresnext26ts',\n 'gcresnext50ts',\n 'gcvit_base',\n 'gcvit_small',\n 'gcvit_tiny',\n 'gcvit_xtiny',\n 'gcvit_xxtiny',\n 'gernet_l',\n 'gernet_m',\n 'gernet_s',\n 'ghostnet_050',\n 'ghostnet_100',\n 'ghostnet_130',\n 'ghostnetv2_100',\n 'ghostnetv2_130',\n 'ghostnetv2_160',\n 'gmixer_12_224',\n 'gmixer_24_224',\n 'gmlp_b16_224',\n 'gmlp_s16_224',\n 'gmlp_ti16_224',\n 'halo2botnet50ts_256',\n 'halonet26t',\n 'halonet50ts',\n 'halonet_h1',\n 'haloregnetz_b',\n 'hardcorenas_a',\n 'hardcorenas_b',\n 'hardcorenas_c',\n 'hardcorenas_d',\n 'hardcorenas_e',\n 'hardcorenas_f',\n 'hgnet_base',\n 'hgnet_small',\n 'hgnet_tiny',\n 'hgnetv2_b0',\n 'hgnetv2_b1',\n 'hgnetv2_b2',\n 'hgnetv2_b3',\n 'hgnetv2_b4',\n 'hgnetv2_b5',\n 'hgnetv2_b6',\n 'hiera_base_224',\n 'hiera_base_abswin_256',\n 'hiera_base_plus_224',\n 'hiera_huge_224',\n 'hiera_large_224',\n 'hiera_small_224',\n 'hiera_small_abswin_256',\n 'hiera_tiny_224',\n 'hieradet_small',\n 'hrnet_w18',\n 'hrnet_w18_small',\n 'hrnet_w18_small_v2',\n 'hrnet_w18_ssld',\n 'hrnet_w30',\n 'hrnet_w32',\n 'hrnet_w40',\n 'hrnet_w44',\n 'hrnet_w48',\n 'hrnet_w48_ssld',\n 'hrnet_w64',\n 'inception_next_atto',\n 'inception_next_base',\n 'inception_next_small',\n 'inception_next_tiny',\n 'inception_resnet_v2',\n 'inception_v3',\n 'inception_v4',\n 'lambda_resnet26rpt_256',\n 'lambda_resnet26t',\n 'lambda_resnet50ts',\n 'lamhalobotnet50ts_256',\n 'lcnet_035',\n 'lcnet_050',\n 'lcnet_075',\n 'lcnet_100',\n 'lcnet_150',\n 'legacy_senet154',\n 'legacy_seresnet18',\n 'legacy_seresnet34',\n 'legacy_seresnet50',\n 'legacy_seresnet101',\n 'legacy_seresnet152',\n 'legacy_seresnext26_32x4d',\n 'legacy_seresnext50_32x4d',\n 'legacy_seresnext101_32x4d',\n 'legacy_xception',\n 'levit_128',\n 'levit_128s',\n 'levit_192',\n 'levit_256',\n 'levit_256d',\n 'levit_384',\n 'levit_384_s8',\n 'levit_512',\n 'levit_512_s8',\n 'levit_512d',\n 'levit_conv_128',\n 'levit_conv_128s',\n 'levit_conv_192',\n 'levit_conv_256',\n 'levit_conv_256d',\n 'levit_conv_384',\n 'levit_conv_384_s8',\n 'levit_conv_512',\n 'levit_conv_512_s8',\n 'levit_conv_512d',\n 'mambaout_base',\n 'mambaout_base_plus_rw',\n 'mambaout_base_short_rw',\n 'mambaout_base_tall_rw',\n 'mambaout_base_wide_rw',\n 'mambaout_femto',\n 'mambaout_kobe',\n 'mambaout_small',\n 'mambaout_small_rw',\n 'mambaout_tiny',\n 'maxvit_base_tf_224',\n 'maxvit_base_tf_384',\n 'maxvit_base_tf_512',\n 'maxvit_large_tf_224',\n 'maxvit_large_tf_384',\n 'maxvit_large_tf_512',\n 'maxvit_nano_rw_256',\n 'maxvit_pico_rw_256',\n 'maxvit_rmlp_base_rw_224',\n 'maxvit_rmlp_base_rw_384',\n 'maxvit_rmlp_nano_rw_256',\n 'maxvit_rmlp_pico_rw_256',\n 'maxvit_rmlp_small_rw_224',\n 'maxvit_rmlp_small_rw_256',\n 'maxvit_rmlp_tiny_rw_256',\n 'maxvit_small_tf_224',\n 'maxvit_small_tf_384',\n 'maxvit_small_tf_512',\n 'maxvit_tiny_pm_256',\n 'maxvit_tiny_rw_224',\n 'maxvit_tiny_rw_256',\n 'maxvit_tiny_tf_224',\n 'maxvit_tiny_tf_384',\n 'maxvit_tiny_tf_512',\n 'maxvit_xlarge_tf_224',\n 'maxvit_xlarge_tf_384',\n 'maxvit_xlarge_tf_512',\n 'maxxvit_rmlp_nano_rw_256',\n 'maxxvit_rmlp_small_rw_256',\n 'maxxvit_rmlp_tiny_rw_256',\n 'maxxvitv2_nano_rw_256',\n 'maxxvitv2_rmlp_base_rw_224',\n 'maxxvitv2_rmlp_base_rw_384',\n 'maxxvitv2_rmlp_large_rw_224',\n 'mixer_b16_224',\n 'mixer_b32_224',\n 'mixer_l16_224',\n 'mixer_l32_224',\n 'mixer_s16_224',\n 'mixer_s32_224',\n 'mixnet_l',\n 'mixnet_m',\n 'mixnet_s',\n 'mixnet_xl',\n 'mixnet_xxl',\n 'mnasnet_050',\n 'mnasnet_075',\n 'mnasnet_100',\n 'mnasnet_140',\n 'mnasnet_small',\n 'mobilenet_edgetpu_100',\n 'mobilenet_edgetpu_v2_l',\n 'mobilenet_edgetpu_v2_m',\n 'mobilenet_edgetpu_v2_s',\n 'mobilenet_edgetpu_v2_xs',\n 'mobilenetv1_100',\n 'mobilenetv1_100h',\n 'mobilenetv1_125',\n 'mobilenetv2_035',\n 'mobilenetv2_050',\n 'mobilenetv2_075',\n 'mobilenetv2_100',\n 'mobilenetv2_110d',\n 'mobilenetv2_120d',\n 'mobilenetv2_140',\n 'mobilenetv3_large_075',\n 'mobilenetv3_large_100',\n 'mobilenetv3_large_150d',\n 'mobilenetv3_rw',\n 'mobilenetv3_small_050',\n 'mobilenetv3_small_075',\n 'mobilenetv3_small_100',\n 'mobilenetv4_conv_aa_large',\n 'mobilenetv4_conv_aa_medium',\n 'mobilenetv4_conv_blur_medium',\n 'mobilenetv4_conv_large',\n 'mobilenetv4_conv_medium',\n 'mobilenetv4_conv_small',\n 'mobilenetv4_conv_small_035',\n 'mobilenetv4_conv_small_050',\n 'mobilenetv4_hybrid_large',\n 'mobilenetv4_hybrid_large_075',\n 'mobilenetv4_hybrid_medium',\n 'mobilenetv4_hybrid_medium_075',\n 'mobileone_s0',\n 'mobileone_s1',\n 'mobileone_s2',\n 'mobileone_s3',\n 'mobileone_s4',\n 'mobilevit_s',\n 'mobilevit_xs',\n 'mobilevit_xxs',\n 'mobilevitv2_050',\n 'mobilevitv2_075',\n 'mobilevitv2_100',\n 'mobilevitv2_125',\n 'mobilevitv2_150',\n 'mobilevitv2_175',\n 'mobilevitv2_200',\n 'mvitv2_base',\n 'mvitv2_base_cls',\n 'mvitv2_huge_cls',\n 'mvitv2_large',\n 'mvitv2_large_cls',\n 'mvitv2_small',\n 'mvitv2_small_cls',\n 'mvitv2_tiny',\n 'nasnetalarge',\n 'nest_base',\n 'nest_base_jx',\n 'nest_small',\n 'nest_small_jx',\n 'nest_tiny',\n 'nest_tiny_jx',\n 'nextvit_base',\n 'nextvit_large',\n 'nextvit_small',\n 'nf_ecaresnet26',\n 'nf_ecaresnet50',\n 'nf_ecaresnet101',\n 'nf_regnet_b0',\n 'nf_regnet_b1',\n 'nf_regnet_b2',\n 'nf_regnet_b3',\n 'nf_regnet_b4',\n 'nf_regnet_b5',\n 'nf_resnet26',\n 'nf_resnet50',\n 'nf_resnet101',\n 'nf_seresnet26',\n 'nf_seresnet50',\n 'nf_seresnet101',\n 'nfnet_f0',\n 'nfnet_f1',\n 'nfnet_f2',\n 'nfnet_f3',\n 'nfnet_f4',\n 'nfnet_f5',\n 'nfnet_f6',\n 'nfnet_f7',\n 'nfnet_l0',\n 'pit_b_224',\n 'pit_b_distilled_224',\n 'pit_s_224',\n 'pit_s_distilled_224',\n 'pit_ti_224',\n 'pit_ti_distilled_224',\n 'pit_xs_224',\n 'pit_xs_distilled_224',\n 'pnasnet5large',\n 'poolformer_m36',\n 'poolformer_m48',\n 'poolformer_s12',\n 'poolformer_s24',\n 'poolformer_s36',\n 'poolformerv2_m36',\n 'poolformerv2_m48',\n 'poolformerv2_s12',\n 'poolformerv2_s24',\n 'poolformerv2_s36',\n 'pvt_v2_b0',\n 'pvt_v2_b1',\n 'pvt_v2_b2',\n 'pvt_v2_b2_li',\n 'pvt_v2_b3',\n 'pvt_v2_b4',\n 'pvt_v2_b5',\n 'rdnet_base',\n 'rdnet_large',\n 'rdnet_small',\n 'rdnet_tiny',\n 'regnetv_040',\n 'regnetv_064',\n 'regnetx_002',\n 'regnetx_004',\n 'regnetx_004_tv',\n 'regnetx_006',\n 'regnetx_008',\n 'regnetx_016',\n 'regnetx_032',\n 'regnetx_040',\n 'regnetx_064',\n 'regnetx_080',\n 'regnetx_120',\n 'regnetx_160',\n 'regnetx_320',\n 'regnety_002',\n 'regnety_004',\n 'regnety_006',\n 'regnety_008',\n 'regnety_008_tv',\n 'regnety_016',\n 'regnety_032',\n 'regnety_040',\n 'regnety_040_sgn',\n 'regnety_064',\n 'regnety_080',\n 'regnety_080_tv',\n 'regnety_120',\n 'regnety_160',\n 'regnety_320',\n 'regnety_640',\n 'regnety_1280',\n 'regnety_2560',\n 'regnetz_005',\n 'regnetz_040',\n 'regnetz_040_h',\n 'regnetz_b16',\n 'regnetz_b16_evos',\n 'regnetz_c16',\n 'regnetz_c16_evos',\n 'regnetz_d8',\n 'regnetz_d8_evos',\n 'regnetz_d32',\n 'regnetz_e8',\n 'repghostnet_050',\n 'repghostnet_058',\n 'repghostnet_080',\n 'repghostnet_100',\n 'repghostnet_111',\n 'repghostnet_130',\n 'repghostnet_150',\n 'repghostnet_200',\n 'repvgg_a0',\n 'repvgg_a1',\n 'repvgg_a2',\n 'repvgg_b0',\n 'repvgg_b1',\n 'repvgg_b1g4',\n 'repvgg_b2',\n 'repvgg_b2g4',\n 'repvgg_b3',\n 'repvgg_b3g4',\n 'repvgg_d2se',\n 'repvit_m0_9',\n 'repvit_m1',\n 'repvit_m1_0',\n 'repvit_m1_1',\n 'repvit_m1_5',\n 'repvit_m2',\n 'repvit_m2_3',\n 'repvit_m3',\n 'res2net50_14w_8s',\n 'res2net50_26w_4s',\n 'res2net50_26w_6s',\n 'res2net50_26w_8s',\n 'res2net50_48w_2s',\n 'res2net50d',\n 'res2net101_26w_4s',\n 'res2net101d',\n 'res2next50',\n 'resmlp_12_224',\n 'resmlp_24_224',\n 'resmlp_36_224',\n 'resmlp_big_24_224',\n 'resnest14d',\n 'resnest26d',\n 'resnest50d',\n 'resnest50d_1s4x24d',\n 'resnest50d_4s2x40d',\n 'resnest101e',\n 'resnest200e',\n 'resnest269e',\n 'resnet10t',\n 'resnet14t',\n 'resnet18',\n 'resnet18d',\n 'resnet26',\n 'resnet26d',\n 'resnet26t',\n 'resnet32ts',\n 'resnet33ts',\n 'resnet34',\n 'resnet34d',\n 'resnet50',\n 'resnet50_clip',\n 'resnet50_clip_gap',\n 'resnet50_gn',\n 'resnet50_mlp',\n 'resnet50c',\n 'resnet50d',\n 'resnet50s',\n 'resnet50t',\n 'resnet50x4_clip',\n 'resnet50x4_clip_gap',\n 'resnet50x16_clip',\n 'resnet50x16_clip_gap',\n 'resnet50x64_clip',\n 'resnet50x64_clip_gap',\n 'resnet51q',\n 'resnet61q',\n 'resnet101',\n 'resnet101_clip',\n 'resnet101_clip_gap',\n 'resnet101c',\n 'resnet101d',\n 'resnet101s',\n 'resnet152',\n 'resnet152c',\n 'resnet152d',\n 'resnet152s',\n 'resnet200',\n 'resnet200d',\n 'resnetaa34d',\n 'resnetaa50',\n 'resnetaa50d',\n 'resnetaa101d',\n 'resnetblur18',\n 'resnetblur50',\n 'resnetblur50d',\n 'resnetblur101d',\n 'resnetrs50',\n 'resnetrs101',\n 'resnetrs152',\n 'resnetrs200',\n 'resnetrs270',\n 'resnetrs350',\n 'resnetrs420',\n 'resnetv2_18',\n 'resnetv2_18d',\n 'resnetv2_34',\n 'resnetv2_34d',\n 'resnetv2_50',\n 'resnetv2_50d',\n 'resnetv2_50d_evos',\n 'resnetv2_50d_frn',\n 'resnetv2_50d_gn',\n 'resnetv2_50t',\n 'resnetv2_50x1_bit',\n 'resnetv2_50x3_bit',\n 'resnetv2_101',\n 'resnetv2_101d',\n 'resnetv2_101x1_bit',\n 'resnetv2_101x3_bit',\n 'resnetv2_152',\n 'resnetv2_152d',\n 'resnetv2_152x2_bit',\n 'resnetv2_152x4_bit',\n 'resnext26ts',\n 'resnext50_32x4d',\n 'resnext50d_32x4d',\n 'resnext101_32x4d',\n 'resnext101_32x8d',\n 'resnext101_32x16d',\n 'resnext101_32x32d',\n 'resnext101_64x4d',\n 'rexnet_100',\n 'rexnet_130',\n 'rexnet_150',\n 'rexnet_200',\n 'rexnet_300',\n 'rexnetr_100',\n 'rexnetr_130',\n 'rexnetr_150',\n 'rexnetr_200',\n 'rexnetr_300',\n 'sam2_hiera_base_plus',\n 'sam2_hiera_large',\n 'sam2_hiera_small',\n 'sam2_hiera_tiny',\n 'samvit_base_patch16',\n 'samvit_base_patch16_224',\n 'samvit_huge_patch16',\n 'samvit_large_patch16',\n 'sebotnet33ts_256',\n 'sedarknet21',\n 'sehalonet33ts',\n 'selecsls42',\n 'selecsls42b',\n 'selecsls60',\n 'selecsls60b',\n 'selecsls84',\n 'semnasnet_050',\n 'semnasnet_075',\n 'semnasnet_100',\n 'semnasnet_140',\n 'senet154',\n 'sequencer2d_l',\n 'sequencer2d_m',\n 'sequencer2d_s',\n 'seresnet18',\n 'seresnet33ts',\n 'seresnet34',\n 'seresnet50',\n 'seresnet50t',\n 'seresnet101',\n 'seresnet152',\n 'seresnet152d',\n 'seresnet200d',\n 'seresnet269d',\n 'seresnetaa50d',\n 'seresnext26d_32x4d',\n 'seresnext26t_32x4d',\n 'seresnext26ts',\n 'seresnext50_32x4d',\n 'seresnext101_32x4d',\n 'seresnext101_32x8d',\n 'seresnext101_64x4d',\n 'seresnext101d_32x8d',\n 'seresnextaa101d_32x8d',\n 'seresnextaa201d_32x8d',\n 'skresnet18',\n 'skresnet34',\n 'skresnet50',\n 'skresnet50d',\n 'skresnext50_32x4d',\n 'spnasnet_100',\n 'swin_base_patch4_window7_224',\n 'swin_base_patch4_window12_384',\n 'swin_large_patch4_window7_224',\n 'swin_large_patch4_window12_384',\n 'swin_s3_base_224',\n 'swin_s3_small_224',\n 'swin_s3_tiny_224',\n 'swin_small_patch4_window7_224',\n 'swin_tiny_patch4_window7_224',\n 'swinv2_base_window8_256',\n 'swinv2_base_window12_192',\n 'swinv2_base_window12to16_192to256',\n 'swinv2_base_window12to24_192to384',\n 'swinv2_base_window16_256',\n 'swinv2_cr_base_224',\n 'swinv2_cr_base_384',\n 'swinv2_cr_base_ns_224',\n 'swinv2_cr_giant_224',\n 'swinv2_cr_giant_384',\n 'swinv2_cr_huge_224',\n 'swinv2_cr_huge_384',\n 'swinv2_cr_large_224',\n 'swinv2_cr_large_384',\n 'swinv2_cr_small_224',\n 'swinv2_cr_small_384',\n 'swinv2_cr_small_ns_224',\n 'swinv2_cr_small_ns_256',\n 'swinv2_cr_tiny_224',\n 'swinv2_cr_tiny_384',\n 'swinv2_cr_tiny_ns_224',\n 'swinv2_large_window12_192',\n 'swinv2_large_window12to16_192to256',\n 'swinv2_large_window12to24_192to384',\n 'swinv2_small_window8_256',\n 'swinv2_small_window16_256',\n 'swinv2_tiny_window8_256',\n 'swinv2_tiny_window16_256',\n 'test_byobnet',\n 'test_convnext',\n 'test_convnext2',\n 'test_convnext3',\n 'test_efficientnet',\n 'test_efficientnet_evos',\n 'test_efficientnet_gn',\n 'test_efficientnet_ln',\n 'test_mambaout',\n 'test_nfnet',\n 'test_resnet',\n 'test_vit',\n 'test_vit2',\n 'test_vit3',\n 'test_vit4',\n 'tf_efficientnet_b0',\n 'tf_efficientnet_b1',\n 'tf_efficientnet_b2',\n 'tf_efficientnet_b3',\n 'tf_efficientnet_b4',\n 'tf_efficientnet_b5',\n 'tf_efficientnet_b6',\n 'tf_efficientnet_b7',\n 'tf_efficientnet_b8',\n 'tf_efficientnet_cc_b0_4e',\n 'tf_efficientnet_cc_b0_8e',\n 'tf_efficientnet_cc_b1_8e',\n 'tf_efficientnet_el',\n 'tf_efficientnet_em',\n 'tf_efficientnet_es',\n 'tf_efficientnet_l2',\n 'tf_efficientnet_lite0',\n 'tf_efficientnet_lite1',\n 'tf_efficientnet_lite2',\n 'tf_efficientnet_lite3',\n 'tf_efficientnet_lite4',\n 'tf_efficientnetv2_b0',\n 'tf_efficientnetv2_b1',\n 'tf_efficientnetv2_b2',\n 'tf_efficientnetv2_b3',\n 'tf_efficientnetv2_l',\n 'tf_efficientnetv2_m',\n 'tf_efficientnetv2_s',\n 'tf_efficientnetv2_xl',\n 'tf_mixnet_l',\n 'tf_mixnet_m',\n 'tf_mixnet_s',\n 'tf_mobilenetv3_large_075',\n 'tf_mobilenetv3_large_100',\n 'tf_mobilenetv3_large_minimal_100',\n 'tf_mobilenetv3_small_075',\n 'tf_mobilenetv3_small_100',\n 'tf_mobilenetv3_small_minimal_100',\n 'tiny_vit_5m_224',\n 'tiny_vit_11m_224',\n 'tiny_vit_21m_224',\n 'tiny_vit_21m_384',\n 'tiny_vit_21m_512',\n 'tinynet_a',\n 'tinynet_b',\n 'tinynet_c',\n 'tinynet_d',\n 'tinynet_e',\n 'tnt_b_patch16_224',\n 'tnt_s_patch16_224',\n 'tresnet_l',\n 'tresnet_m',\n 'tresnet_v2_l',\n 'tresnet_xl',\n 'twins_pcpvt_base',\n 'twins_pcpvt_large',\n 'twins_pcpvt_small',\n 'twins_svt_base',\n 'twins_svt_large',\n 'twins_svt_small',\n 'vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn',\n 'visformer_small',\n 'visformer_tiny',\n 'vit_base_mci_224',\n 'vit_base_patch8_224',\n 'vit_base_patch14_dinov2',\n 'vit_base_patch14_reg4_dinov2',\n 'vit_base_patch16_18x2_224',\n 'vit_base_patch16_224',\n 'vit_base_patch16_224_miil',\n 'vit_base_patch16_384',\n 'vit_base_patch16_clip_224',\n 'vit_base_patch16_clip_384',\n 'vit_base_patch16_clip_quickgelu_224',\n 'vit_base_patch16_gap_224',\n 'vit_base_patch16_plus_240',\n 'vit_base_patch16_plus_clip_240',\n 'vit_base_patch16_reg4_gap_256',\n 'vit_base_patch16_rope_reg1_gap_256',\n ...]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/seg/backbones/#initializing-panoptic-cellpose-with-uni-backbone","title":"Initializing panoptic Cellpose with UNI backbone\u00b6","text":"<p>Note: Models like UNI and Virchow require some extra key-word arguments to be passed during the timm initialization. These can be passed in the <code>model_kwargs</code> dictionary that takes in arbitrary arguments needed for initializing the models. The <code>encoder_kws</code> is the argument inside the <code>model_kwargs</code> that is used to pass the extra key-word arguments to the timm backbone encoder.</p> <p>Note: The <code>enc_out_indices</code> argument in the <code>model_kwargs</code> is used to specify which layers of the backbone encoder are passed to the panoptic segmentation model as the U-net skip connections. The indices correspond to the layers in the backbone encoder. By default, 4 layers is passed to the pixel decoders so four indices are specified in the list.</p>"},{"location":"user_guide/seg/backbones/#initializing-panoptic-hovernet-with-virchow-backbone","title":"Initializing panoptic HoverNet with Virchow backbone\u00b6","text":""},{"location":"user_guide/seg/backbones/#initializing-panoptic-stardist-with-prov-gigapath-backbone","title":"Initializing panoptic Stardist with Prov-GigaPath backbone\u00b6","text":""},{"location":"user_guide/seg/backbones/#initializing-panoptic-cppnet-with-sam-image-encoder-backbone","title":"Initializing panoptic CPPNet with SAM image encoder backbone\u00b6","text":""},{"location":"user_guide/seg/backbones/#initializing-panoptic-cellvit-with-the-sam-huge-backbone","title":"Initializing panoptic CellVit with the SAM huge backbone\u00b6","text":""},{"location":"user_guide/seg/backbones/#available-timm-encoders","title":"Available timm encoders\u00b6","text":"<p>In general, any pre-trained model from the timm (pytorch-image-models) library can be used as long as the model has the <code>forward_intermediates</code>-method implemented. You can list the available timm encoders followingly:</p>"},{"location":"user_guide/seg/finetuning/","title":"Panoptic Segmentation Model Finetuning","text":"In\u00a0[1]: Copied! <pre># Some installations\n# !pip install torchdata\n# !pip install tables\n</pre> # Some installations # !pip install torchdata # !pip install tables In\u00a0[3]: Copied! <pre>from platform import python_version\n\nimport torch\nimport albumentations as A\nimport cellseg_models_pytorch\n\nprint(\"torch version:\", torch.__version__)\nprint(\"albumentations version:\", A.__version__)\nprint(\"cellseg_models_pytorch version:\", cellseg_models_pytorch.__version__)\nprint(\"python version:\", python_version())\n</pre> from platform import python_version  import torch import albumentations as A import cellseg_models_pytorch  print(\"torch version:\", torch.__version__) print(\"albumentations version:\", A.__version__) print(\"cellseg_models_pytorch version:\", cellseg_models_pytorch.__version__) print(\"python version:\", python_version()) <pre>torch version: 2.7.0+cu126\nalbumentations version: 2.0.6\ncellseg_models_pytorch version: 0.1.26\npython version: 3.12.3\n</pre> In\u00a0[4]: Copied! <pre>import warnings\nimport pandas as pd\nimport numpy as np\nimport io\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom skimage.color import label2rgb\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# for decoding PNG bytes to numpy array\ndef png_bytes_to_array(png_bytes):\n    img = Image.open(io.BytesIO(png_bytes))\n    return np.array(img)\n\n\n# Load the dataset from Hugging Face Hub\nds = load_dataset(\"histolytics-hub/panoptils_refined\", split=\"train\")\n\n# convert to pandas df\ndf = ds.with_format(\"pandas\")[:]\n\n# Example: decode one row\nrow = df.iloc[1]\nimage = png_bytes_to_array(row[\"image\"])\ninst_mask = png_bytes_to_array(row[\"inst\"])\ntype_mask = png_bytes_to_array(row[\"type\"])\nsem_mask = png_bytes_to_array(row[\"sem\"])\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\naxes[0].imshow(image)\naxes[1].imshow(label2rgb(inst_mask, bg_label=0))\naxes[2].imshow(label2rgb(type_mask, bg_label=0))\naxes[3].imshow(label2rgb(sem_mask, bg_label=0))\n\nfor ax in axes:\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> import warnings import pandas as pd import numpy as np import io import matplotlib.pyplot as plt  from datasets import load_dataset from PIL import Image from skimage.color import label2rgb  warnings.filterwarnings(\"ignore\")   # for decoding PNG bytes to numpy array def png_bytes_to_array(png_bytes):     img = Image.open(io.BytesIO(png_bytes))     return np.array(img)   # Load the dataset from Hugging Face Hub ds = load_dataset(\"histolytics-hub/panoptils_refined\", split=\"train\")  # convert to pandas df df = ds.with_format(\"pandas\")[:]  # Example: decode one row row = df.iloc[1] image = png_bytes_to_array(row[\"image\"]) inst_mask = png_bytes_to_array(row[\"inst\"]) type_mask = png_bytes_to_array(row[\"type\"]) sem_mask = png_bytes_to_array(row[\"sem\"])  fig, axes = plt.subplots(1, 4, figsize=(20, 5)) axes[0].imshow(image) axes[1].imshow(label2rgb(inst_mask, bg_label=0)) axes[2].imshow(label2rgb(type_mask, bg_label=0)) axes[3].imshow(label2rgb(sem_mask, bg_label=0))  for ax in axes:     ax.axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[5]: Copied! <pre>from sklearn.model_selection import GroupShuffleSplit\n\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(gss.split(df, groups=df[\"slide_name\"]))\n\ndf_train = df.iloc[train_idx]\ndf_val = df.iloc[val_idx]\ndf_train.head()\n</pre> from sklearn.model_selection import GroupShuffleSplit  gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42) train_idx, val_idx = next(gss.split(df, groups=df[\"slide_name\"]))  df_train = df.iloc[train_idx] df_val = df.iloc[val_idx] df_train.head() Out[5]: slide_name hospital sample image inst type sem 0 TCGA-A1-A0SK-DX1 A1 TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.250... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... 1 TCGA-A1-A0SK-DX1 A1 TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.250... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... 2 TCGA-A1-A0SK-DX1 A1 TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.250... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... 3 TCGA-A1-A0SK-DX1 A1 TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.250... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... 4 TCGA-A1-A0SK-DX1 A1 TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.250... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\... In\u00a0[\u00a0]: Copied! <pre>import tables as tb\nimport albumentations as A\n\nfrom pathlib import Path\nfrom histolytics.utils import H5Handler\nfrom cellseg_models_pytorch.inference.predictor import Predictor\nfrom cellseg_models_pytorch.wsi.tiles import _pad_tile\nfrom tqdm import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n\n# we will apply some simple augmentations to the patches\nimg_transforms = A.Compose(\n    [\n        A.HorizontalFlip(p=0.33),\n        A.VerticalFlip(p=0.33),\n    ]\n)\n\n# we'll save the h5 file in a directory in the home folder\nh5handler = H5Handler()\nsave_dir = Path.home() / \"panoptils_refined\"\nsave_dir.mkdir(parents=True, exist_ok=True)\n\n\n# Helper function to patch the images and to create H5 files from the patches\ndef create_h5(df: pd.DataFrame, fold: str, stride: int, patch_size: tuple):\n    fname = save_dir / f\"panoptils_{fold}_p{patch_size[0]}_{stride}.h5\"\n    h5 = tb.open_file(fname, \"w\")\n    h5handler.init_img(h5, patch_size)\n    h5handler.init_mask(h5, \"inst_mask\", patch_size)\n    h5handler.init_mask(h5, \"type_mask\", patch_size)\n    h5handler.init_mask(h5, \"sem_mask\", patch_size)\n\n    try:\n        for i, row in tqdm(\n            df.iterrows(), total=len(df), desc=f\"Processing {fold} patches\"\n        ):\n            image = png_bytes_to_array(row[\"image\"])\n            inst_mask = png_bytes_to_array(row[\"inst\"])\n            type_mask = png_bytes_to_array(row[\"type\"])\n            sem_mask = png_bytes_to_array(row[\"sem\"])\n\n            data = np.concatenate(\n                [\n                    image,\n                    inst_mask[..., None],\n                    type_mask[..., None],\n                    sem_mask[..., None],\n                ],\n                axis=2,\n            )\n            slices, pady, padx = Predictor._get_slices(\n                stride, patch_size, image.shape[:2]\n            )\n\n            padx, modx = divmod(padx, 2)\n            pady, mody = divmod(pady, 2)\n            padx += modx\n            pady += mody\n\n            for yslice, xslice in slices:\n                x_i = _pad_tile(data[yslice, xslice, ...], shape=patch_size, fill=0)\n                x_i = img_transforms(image=x_i)[\"image\"]  # apply flips\n                im_i = x_i[..., :3]\n                inst_i = x_i[..., 3]\n                type_i = x_i[..., 4]\n                sem_i = x_i[..., 5]\n                h5handler.append_array(h5, im_i[None, ...], \"image\")\n                h5handler.append_array(h5, inst_i[None, ...], \"inst_mask\")\n                h5handler.append_array(h5, type_i[None, ...], \"type_mask\")\n                h5handler.append_array(h5, sem_i[None, ...], \"sem_mask\")\n\n        h5.close()\n    except Exception as e:\n        h5.close()\n        raise e\n\n\n# Create H5 files for train and validation sets\ncreate_h5(df_train, \"train\", stride=224, patch_size=(224, 224))\ncreate_h5(df_val, \"val\", stride=224, patch_size=(224, 224))\n</pre> import tables as tb import albumentations as A  from pathlib import Path from histolytics.utils import H5Handler from cellseg_models_pytorch.inference.predictor import Predictor from cellseg_models_pytorch.wsi.tiles import _pad_tile from tqdm import tqdm  warnings.filterwarnings(\"ignore\")  # we will apply some simple augmentations to the patches img_transforms = A.Compose(     [         A.HorizontalFlip(p=0.33),         A.VerticalFlip(p=0.33),     ] )  # we'll save the h5 file in a directory in the home folder h5handler = H5Handler() save_dir = Path.home() / \"panoptils_refined\" save_dir.mkdir(parents=True, exist_ok=True)   # Helper function to patch the images and to create H5 files from the patches def create_h5(df: pd.DataFrame, fold: str, stride: int, patch_size: tuple):     fname = save_dir / f\"panoptils_{fold}_p{patch_size[0]}_{stride}.h5\"     h5 = tb.open_file(fname, \"w\")     h5handler.init_img(h5, patch_size)     h5handler.init_mask(h5, \"inst_mask\", patch_size)     h5handler.init_mask(h5, \"type_mask\", patch_size)     h5handler.init_mask(h5, \"sem_mask\", patch_size)      try:         for i, row in tqdm(             df.iterrows(), total=len(df), desc=f\"Processing {fold} patches\"         ):             image = png_bytes_to_array(row[\"image\"])             inst_mask = png_bytes_to_array(row[\"inst\"])             type_mask = png_bytes_to_array(row[\"type\"])             sem_mask = png_bytes_to_array(row[\"sem\"])              data = np.concatenate(                 [                     image,                     inst_mask[..., None],                     type_mask[..., None],                     sem_mask[..., None],                 ],                 axis=2,             )             slices, pady, padx = Predictor._get_slices(                 stride, patch_size, image.shape[:2]             )              padx, modx = divmod(padx, 2)             pady, mody = divmod(pady, 2)             padx += modx             pady += mody              for yslice, xslice in slices:                 x_i = _pad_tile(data[yslice, xslice, ...], shape=patch_size, fill=0)                 x_i = img_transforms(image=x_i)[\"image\"]  # apply flips                 im_i = x_i[..., :3]                 inst_i = x_i[..., 3]                 type_i = x_i[..., 4]                 sem_i = x_i[..., 5]                 h5handler.append_array(h5, im_i[None, ...], \"image\")                 h5handler.append_array(h5, inst_i[None, ...], \"inst_mask\")                 h5handler.append_array(h5, type_i[None, ...], \"type_mask\")                 h5handler.append_array(h5, sem_i[None, ...], \"sem_mask\")          h5.close()     except Exception as e:         h5.close()         raise e   # Create H5 files for train and validation sets create_h5(df_train, \"train\", stride=224, patch_size=(224, 224)) create_h5(df_val, \"val\", stride=224, patch_size=(224, 224)) <pre>Processing train patches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1113/1113 [01:13&lt;00:00, 15.08it/s]\nProcessing val patches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236/236 [00:15&lt;00:00, 14.81it/s]\n</pre> In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Open the H5 file for train patches\nh5_train_path = save_dir / \"panoptils_train_p224_224.h5\"\nwith tb.open_file(h5_train_path, \"r\") as h5:\n    n_patches = h5.root.image.shape[0]\n    idxs = np.random.choice(n_patches, size=5, replace=False)\n    fig, axes = plt.subplots(5, 4, figsize=(16, 16))\n    for i, idx in enumerate(idxs):\n        image = h5.root.image[idx]\n        inst = h5.root.inst_mask[idx]\n        type_ = h5.root.type_mask[idx]\n        sem = h5.root.sem_mask[idx]\n        axes[i, 0].imshow(image.astype(np.uint8))\n        axes[i, 1].imshow(label2rgb(inst, bg_label=0))\n        axes[i, 2].imshow(label2rgb(type_, bg_label=0))\n        axes[i, 3].imshow(label2rgb(sem, bg_label=0))\n        for j in range(4):\n            axes[i, j].axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n</pre> import matplotlib.pyplot as plt  # Open the H5 file for train patches h5_train_path = save_dir / \"panoptils_train_p224_224.h5\" with tb.open_file(h5_train_path, \"r\") as h5:     n_patches = h5.root.image.shape[0]     idxs = np.random.choice(n_patches, size=5, replace=False)     fig, axes = plt.subplots(5, 4, figsize=(16, 16))     for i, idx in enumerate(idxs):         image = h5.root.image[idx]         inst = h5.root.inst_mask[idx]         type_ = h5.root.type_mask[idx]         sem = h5.root.sem_mask[idx]         axes[i, 0].imshow(image.astype(np.uint8))         axes[i, 1].imshow(label2rgb(inst, bg_label=0))         axes[i, 2].imshow(label2rgb(type_, bg_label=0))         axes[i, 3].imshow(label2rgb(sem, bg_label=0))         for j in range(4):             axes[i, j].axis(\"off\")     plt.tight_layout()     plt.show() In\u00a0[2]: Copied! <pre>import sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport albumentations as A\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchdata.nodes as tn\nfrom torch.utils.data import Dataset, RandomSampler, SequentialSampler\nfrom safetensors.torch import save_model\n\n\ndef collate(x: List[Dict[str, np.ndarray]]) -&gt; Dict[str, np.ndarray]:\n    return {key: torch.stack([d[key] for d in x]) for key in x[0].keys()}\n\n\nclass MapAndCollate:\n    def __init__(self, dataset: Dataset):\n        self.dataset = dataset\n\n    def __call__(self, batch_of_indices: List[int]) -&gt; Dict[str, np.ndarray]:\n        batch = [self.dataset[i] for i in batch_of_indices]\n        return collate(batch)\n\n\n# To keep things simple, let's assume that the following args are provided by the caller\ndef NodesDataLoader(\n    dataset: Dataset,\n    batch_size: int,\n    shuffle: bool,\n    num_workers: int,\n    pin_memory: bool,\n    drop_last: bool,\n):\n    # Assume we're working with a map-style dataset\n    assert hasattr(dataset, \"__getitem__\") and hasattr(dataset, \"__len__\")\n\n    sampler = RandomSampler(dataset) if shuffle else SequentialSampler(dataset)\n    node = tn.SamplerWrapper(sampler)\n    node = tn.Batcher(node, batch_size=batch_size, drop_last=drop_last)\n\n    map_and_collate = MapAndCollate(dataset)\n    node = tn.ParallelMapper(\n        node,\n        map_fn=map_and_collate,\n        num_workers=num_workers,\n        method=\"process\",  # Set this to \"thread\" for multi-threading\n        in_order=True,\n    )\n\n    if pin_memory:\n        node = tn.PinMemory(node)\n    node = tn.Prefetcher(node, prefetch_factor=num_workers * 2)\n\n    return tn.Loader(node)\n</pre> import sys import time from pathlib import Path from typing import Dict, List  import albumentations as A import numpy as np import torch import torch.nn.functional as F import torchdata.nodes as tn from torch.utils.data import Dataset, RandomSampler, SequentialSampler from safetensors.torch import save_model   def collate(x: List[Dict[str, np.ndarray]]) -&gt; Dict[str, np.ndarray]:     return {key: torch.stack([d[key] for d in x]) for key in x[0].keys()}   class MapAndCollate:     def __init__(self, dataset: Dataset):         self.dataset = dataset      def __call__(self, batch_of_indices: List[int]) -&gt; Dict[str, np.ndarray]:         batch = [self.dataset[i] for i in batch_of_indices]         return collate(batch)   # To keep things simple, let's assume that the following args are provided by the caller def NodesDataLoader(     dataset: Dataset,     batch_size: int,     shuffle: bool,     num_workers: int,     pin_memory: bool,     drop_last: bool, ):     # Assume we're working with a map-style dataset     assert hasattr(dataset, \"__getitem__\") and hasattr(dataset, \"__len__\")      sampler = RandomSampler(dataset) if shuffle else SequentialSampler(dataset)     node = tn.SamplerWrapper(sampler)     node = tn.Batcher(node, batch_size=batch_size, drop_last=drop_last)      map_and_collate = MapAndCollate(dataset)     node = tn.ParallelMapper(         node,         map_fn=map_and_collate,         num_workers=num_workers,         method=\"process\",  # Set this to \"thread\" for multi-threading         in_order=True,     )      if pin_memory:         node = tn.PinMemory(node)     node = tn.Prefetcher(node, prefetch_factor=num_workers * 2)      return tn.Loader(node)  In\u00a0[4]: Copied! <pre>import torch.nn as nn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom histolytics.torch_datasets.h5dataset import DatasetH5\nfrom histolytics.losses import CELoss, DiceLoss, JointLoss, MultiTaskLoss\nfrom histolytics.models.cellpose_panoptic import CellposePanoptic\nfrom histolytics.transforms import (\n    AlbuStrongAugment,\n    CellposeTransform,\n    MinMaxNormalization,\n    ApplyEach,\n)\n\n# panoptils (refined) classes\nnuclei_classes = {\n    \"background\": 0,\n    \"neoplastic\": 1,\n    \"stromal\": 2,\n    \"inflammatory\": 3,\n    \"epithelial\": 4,\n    \"other\": 5,\n    \"unknown\": 6,\n}\n\ntissue_classes = {\n    \"background\": 0,\n    \"tumor\": 1,\n    \"stroma\": 2,\n    \"epithelium\": 3,\n    \"junk/debris\": 4,\n    \"blood\": 5,\n    \"other\": 6,\n}\n\n\n# Quick wrapper for MSE loss to make it fit the JointLoss API\nclass MSELoss(nn.Module):\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__()\n\n    def forward(\n        self, yhat: torch.Tensor, target: torch.Tensor, **kwargs\n    ) -&gt; torch.Tensor:\n        return F.mse_loss(yhat, target, reduction=\"mean\").float()\n\n\nbatch_size = 8\nin_keys = (\"image\", \"sem_mask\", \"inst_mask\", \"type_mask\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# init  CellposePanoptic model\ncpose_panoptic = CellposePanoptic(\n    n_nuc_classes=len(nuclei_classes),\n    n_tissue_classes=len(tissue_classes),\n    enc_name=\"efficientnet_b5\",  # you can change this to any supported encoder\n    enc_pretrain=True,\n)\nmodel = cpose_panoptic.model.to(device)\n\n# init training transforms\nimg_transforms = A.Compose(\n    [\n        A.HorizontalFlip(p=0.33),\n        A.VerticalFlip(p=0.33),\n        AlbuStrongAugment(),\n        MinMaxNormalization(),\n    ]\n)\n\ninst_transforms = ApplyEach([CellposeTransform(deduplicate=False)], as_list=True)\n\n# init val dataset and dataloader\nds_train = DatasetH5(\n    Path().home() / \"panoptils_refined\" / \"panoptils_train_p224_224.h5\",\n    img_key=\"image\",\n    inst_keys=[\"inst_mask\"],\n    mask_keys=[\"type_mask\", \"sem_mask\"],\n    transforms=img_transforms,\n    inst_transforms=inst_transforms,\n)\n\nloader_train = NodesDataLoader(\n    ds_train,\n    batch_size,\n    shuffle=False,\n    num_workers=batch_size,\n    pin_memory=True,\n    drop_last=True,\n)\n# init train dataset and dataloader\nds_val = DatasetH5(\n    Path().home() / \"panoptils_refined\" / \"panoptils_val_p224_224.h5\",\n    img_key=\"image\",\n    inst_keys=[\"inst_mask\"],\n    mask_keys=[\"type_mask\", \"sem_mask\"],\n    transforms=img_transforms,\n    inst_transforms=inst_transforms,\n)\n\nloader_val = NodesDataLoader(\n    ds_val,\n    batch_size,\n    shuffle=True,\n    num_workers=batch_size,\n    pin_memory=True,\n    drop_last=True,\n)\n\n# init training multi-task loss\n# The losses are defined for each task, and the weights are set for each task.\nlosses = {\n    \"cellpose\": JointLoss([MSELoss()]),\n    \"type_mask\": JointLoss([CELoss(apply_sd=True), DiceLoss()]),\n    \"sem_mask\": JointLoss([CELoss(apply_sd=True), DiceLoss()]),\n}\nloss_weights = {\"cellpose\": 1.0, \"type_mask\": 1.0, \"sem_mask\": 2.0}\nmultitask_loss = MultiTaskLoss(losses, loss_weights=loss_weights).to(device)\n\n# optimizer and lr scheduler\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=0.0001)\nlr_scheduler = CosineAnnealingLR(optimizer, T_max=1000, eta_min=0, last_epoch=-1)\n</pre> import torch.nn as nn from torch.optim.lr_scheduler import CosineAnnealingLR  from histolytics.torch_datasets.h5dataset import DatasetH5 from histolytics.losses import CELoss, DiceLoss, JointLoss, MultiTaskLoss from histolytics.models.cellpose_panoptic import CellposePanoptic from histolytics.transforms import (     AlbuStrongAugment,     CellposeTransform,     MinMaxNormalization,     ApplyEach, )  # panoptils (refined) classes nuclei_classes = {     \"background\": 0,     \"neoplastic\": 1,     \"stromal\": 2,     \"inflammatory\": 3,     \"epithelial\": 4,     \"other\": 5,     \"unknown\": 6, }  tissue_classes = {     \"background\": 0,     \"tumor\": 1,     \"stroma\": 2,     \"epithelium\": 3,     \"junk/debris\": 4,     \"blood\": 5,     \"other\": 6, }   # Quick wrapper for MSE loss to make it fit the JointLoss API class MSELoss(nn.Module):     def __init__(self, **kwargs) -&gt; None:         super().__init__()      def forward(         self, yhat: torch.Tensor, target: torch.Tensor, **kwargs     ) -&gt; torch.Tensor:         return F.mse_loss(yhat, target, reduction=\"mean\").float()   batch_size = 8 in_keys = (\"image\", \"sem_mask\", \"inst_mask\", \"type_mask\") device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # init  CellposePanoptic model cpose_panoptic = CellposePanoptic(     n_nuc_classes=len(nuclei_classes),     n_tissue_classes=len(tissue_classes),     enc_name=\"efficientnet_b5\",  # you can change this to any supported encoder     enc_pretrain=True, ) model = cpose_panoptic.model.to(device)  # init training transforms img_transforms = A.Compose(     [         A.HorizontalFlip(p=0.33),         A.VerticalFlip(p=0.33),         AlbuStrongAugment(),         MinMaxNormalization(),     ] )  inst_transforms = ApplyEach([CellposeTransform(deduplicate=False)], as_list=True)  # init val dataset and dataloader ds_train = DatasetH5(     Path().home() / \"panoptils_refined\" / \"panoptils_train_p224_224.h5\",     img_key=\"image\",     inst_keys=[\"inst_mask\"],     mask_keys=[\"type_mask\", \"sem_mask\"],     transforms=img_transforms,     inst_transforms=inst_transforms, )  loader_train = NodesDataLoader(     ds_train,     batch_size,     shuffle=False,     num_workers=batch_size,     pin_memory=True,     drop_last=True, ) # init train dataset and dataloader ds_val = DatasetH5(     Path().home() / \"panoptils_refined\" / \"panoptils_val_p224_224.h5\",     img_key=\"image\",     inst_keys=[\"inst_mask\"],     mask_keys=[\"type_mask\", \"sem_mask\"],     transforms=img_transforms,     inst_transforms=inst_transforms, )  loader_val = NodesDataLoader(     ds_val,     batch_size,     shuffle=True,     num_workers=batch_size,     pin_memory=True,     drop_last=True, )  # init training multi-task loss # The losses are defined for each task, and the weights are set for each task. losses = {     \"cellpose\": JointLoss([MSELoss()]),     \"type_mask\": JointLoss([CELoss(apply_sd=True), DiceLoss()]),     \"sem_mask\": JointLoss([CELoss(apply_sd=True), DiceLoss()]), } loss_weights = {\"cellpose\": 1.0, \"type_mask\": 1.0, \"sem_mask\": 2.0} multitask_loss = MultiTaskLoss(losses, loss_weights=loss_weights).to(device)  # optimizer and lr scheduler optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.0001) lr_scheduler = CosineAnnealingLR(optimizer, T_max=1000, eta_min=0, last_epoch=-1) In\u00a0[\u00a0]: Copied! <pre>n_epochs = 1\nprogress_freq = 100\n\ntotal = int(np.ceil(len(ds_train) / batch_size))\nsave_dir = Path.home() / \"panoptils_refined\"\n\n\nprint(\"Start training, num epochs:\", n_epochs, file=sys.stderr)\nfor epoch in range(n_epochs):\n    # Train loop\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    start = time.perf_counter()\n    for i, batch in enumerate(loader_train):\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        soft_masks = model(batch.pop(\"image\"))\n        targets = {\n            \"cellpose\": batch[\"inst_mask_cellpose\"],\n            \"type_mask\": batch[\"type_mask\"],\n            \"sem_mask\": batch[\"sem_mask\"],\n        }\n        yhats = {\n            \"cellpose\": soft_masks[\"nuc\"].aux_map,\n            \"type_mask\": soft_masks[\"nuc\"].type_map,\n            \"sem_mask\": soft_masks[\"tissue\"].type_map,\n        }\n        loss = multitask_loss(yhats, targets)\n\n        # skip batches where loss goes to nan\n        if torch.isnan(loss):\n            print(\"NaN loss encountered, skipping batch\")\n            continue\n\n        loss.backward()\n        optimizer.step()\n\n        # compute total loss\n        total_loss += loss.item()\n        num_batches += 1\n        avg_loss = total_loss / num_batches\n\n        if i % progress_freq == 0:\n            iter_time = time.perf_counter() - start\n            mins = int(iter_time // 60)\n            secs = int(iter_time % 60)\n            throughput = i * batch_size / iter_time\n            estimated_time_left = (total - i) * batch_size / (throughput + 0.001)\n            est_mins = int(estimated_time_left // 60)\n            est_secs = int(estimated_time_left % 60)\n            tmsg = f\"b {i}/{total}, {mins}min {secs}s/{est_mins}min {est_secs}s, avg throughput: {round(throughput, 2)} img/sec \"\n            lmsg = f\"avg loss: {round(avg_loss, 4)}\"\n            print(f\"epoch {epoch}, {tmsg}, {lmsg}\", file=sys.stderr)\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    val_batches = 0\n    with torch.no_grad():\n        for val_batch in loader_val:\n            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n            val_soft_masks = model(val_batch.pop(\"image\"))\n            val_targets = {\n                \"cellpose\": val_batch[\"inst_mask_cellpose\"],\n                \"type_mask\": val_batch[\"type_mask\"],\n                \"sem_mask\": val_batch[\"sem_mask\"],\n            }\n            val_yhats = {\n                \"cellpose\": val_soft_masks[\"nuc\"].aux_map,\n                \"type_mask\": val_soft_masks[\"nuc\"].type_map,\n                \"sem_mask\": val_soft_masks[\"tissue\"].type_map,\n            }\n            vloss = multitask_loss(val_yhats, val_targets)\n            if not torch.isnan(vloss):\n                val_loss += vloss.item()\n                val_batches += 1\n\n    if val_batches &gt; 0:\n        avg_val_loss = val_loss / val_batches\n        print(\n            f\"epoch {epoch}, validation avg loss: {round(avg_val_loss, 4)}\",\n            file=sys.stderr,\n        )\n\n    # save the model weights after epoch\n    checkpoint_path = (\n        Path(save_dir) / f\"effnet_cpose_panop_weights_epoch_{epoch}.safetensors\"\n    )\n    save_model(model, checkpoint_path.as_posix())\n\n    lr_scheduler.step()\n</pre> n_epochs = 1 progress_freq = 100  total = int(np.ceil(len(ds_train) / batch_size)) save_dir = Path.home() / \"panoptils_refined\"   print(\"Start training, num epochs:\", n_epochs, file=sys.stderr) for epoch in range(n_epochs):     # Train loop     model.train()     total_loss = 0     num_batches = 0     start = time.perf_counter()     for i, batch in enumerate(loader_train):         optimizer.zero_grad()         batch = {k: v.to(device) for k, v in batch.items()}         soft_masks = model(batch.pop(\"image\"))         targets = {             \"cellpose\": batch[\"inst_mask_cellpose\"],             \"type_mask\": batch[\"type_mask\"],             \"sem_mask\": batch[\"sem_mask\"],         }         yhats = {             \"cellpose\": soft_masks[\"nuc\"].aux_map,             \"type_mask\": soft_masks[\"nuc\"].type_map,             \"sem_mask\": soft_masks[\"tissue\"].type_map,         }         loss = multitask_loss(yhats, targets)          # skip batches where loss goes to nan         if torch.isnan(loss):             print(\"NaN loss encountered, skipping batch\")             continue          loss.backward()         optimizer.step()          # compute total loss         total_loss += loss.item()         num_batches += 1         avg_loss = total_loss / num_batches          if i % progress_freq == 0:             iter_time = time.perf_counter() - start             mins = int(iter_time // 60)             secs = int(iter_time % 60)             throughput = i * batch_size / iter_time             estimated_time_left = (total - i) * batch_size / (throughput + 0.001)             est_mins = int(estimated_time_left // 60)             est_secs = int(estimated_time_left % 60)             tmsg = f\"b {i}/{total}, {mins}min {secs}s/{est_mins}min {est_secs}s, avg throughput: {round(throughput, 2)} img/sec \"             lmsg = f\"avg loss: {round(avg_loss, 4)}\"             print(f\"epoch {epoch}, {tmsg}, {lmsg}\", file=sys.stderr)      # Validation loop     model.eval()     val_loss = 0     val_batches = 0     with torch.no_grad():         for val_batch in loader_val:             val_batch = {k: v.to(device) for k, v in val_batch.items()}             val_soft_masks = model(val_batch.pop(\"image\"))             val_targets = {                 \"cellpose\": val_batch[\"inst_mask_cellpose\"],                 \"type_mask\": val_batch[\"type_mask\"],                 \"sem_mask\": val_batch[\"sem_mask\"],             }             val_yhats = {                 \"cellpose\": val_soft_masks[\"nuc\"].aux_map,                 \"type_mask\": val_soft_masks[\"nuc\"].type_map,                 \"sem_mask\": val_soft_masks[\"tissue\"].type_map,             }             vloss = multitask_loss(val_yhats, val_targets)             if not torch.isnan(vloss):                 val_loss += vloss.item()                 val_batches += 1      if val_batches &gt; 0:         avg_val_loss = val_loss / val_batches         print(             f\"epoch {epoch}, validation avg loss: {round(avg_val_loss, 4)}\",             file=sys.stderr,         )      # save the model weights after epoch     checkpoint_path = (         Path(save_dir) / f\"effnet_cpose_panop_weights_epoch_{epoch}.safetensors\"     )     save_model(model, checkpoint_path.as_posix())      lr_scheduler.step() <pre>Start training, num epochs: 1\n</pre> <pre>epoch 0, b 0/3479, 0min 4s/463866min 40s, avg throughput: 0.0 img/sec , avg loss: 16.8737\nepoch 0, b 100/3479, 0min 46s/25min 57s, avg throughput: 17.35 img/sec , avg loss: 6.1997\nepoch 0, b 200/3479, 2min 14s/36min 51s, avg throughput: 11.86 img/sec , avg loss: 4.9896\nepoch 0, b 300/3479, 3min 2s/32min 13s, avg throughput: 13.15 img/sec , avg loss: 4.4239\nepoch 0, b 400/3479, 3min 44s/28min 49s, avg throughput: 14.24 img/sec , avg loss: 4.3223\nepoch 0, b 500/3479, 4min 25s/26min 22s, avg throughput: 15.06 img/sec , avg loss: 4.1799\nepoch 0, b 600/3479, 5min 9s/24min 44s, avg throughput: 15.52 img/sec , avg loss: 3.9944\nepoch 0, b 700/3479, 5min 56s/23min 34s, avg throughput: 15.72 img/sec , avg loss: 3.9385\nepoch 0, b 800/3479, 6min 47s/22min 45s, avg throughput: 15.69 img/sec , avg loss: 3.8031\nepoch 0, b 900/3479, 7min 30s/21min 29s, avg throughput: 16.0 img/sec , avg loss: 3.7824\nepoch 0, b 1000/3479, 8min 12s/20min 20s, avg throughput: 16.25 img/sec , avg loss: 3.8252\nepoch 0, b 1100/3479, 9min 3s/19min 34s, avg throughput: 16.2 img/sec , avg loss: 3.7848\nepoch 0, b 1200/3479, 9min 50s/18min 42s, avg throughput: 16.25 img/sec , avg loss: 3.7513\nepoch 0, b 1300/3479, 10min 33s/17min 41s, avg throughput: 16.42 img/sec , avg loss: 3.7238\nepoch 0, b 1400/3479, 11min 17s/16min 46s, avg throughput: 16.52 img/sec , avg loss: 3.6894\nepoch 0, b 1500/3479, 12min 14s/16min 9s, avg throughput: 16.33 img/sec , avg loss: 3.7113\nepoch 0, b 1600/3479, 12min 59s/15min 15s, avg throughput: 16.42 img/sec , avg loss: 3.7266\nepoch 0, b 1700/3479, 13min 41s/14min 19s, avg throughput: 16.55 img/sec , avg loss: 3.705\nepoch 0, b 1800/3479, 14min 33s/13min 34s, avg throughput: 16.49 img/sec , avg loss: 3.7005\nepoch 0, b 1900/3479, 15min 21s/12min 45s, avg throughput: 16.5 img/sec , avg loss: 3.7086\nepoch 0, b 2000/3479, 16min 10s/11min 57s, avg throughput: 16.48 img/sec , avg loss: 3.6494\nepoch 0, b 2100/3479, 16min 54s/11min 5s, avg throughput: 16.56 img/sec , avg loss: 3.657\nepoch 0, b 2200/3479, 17min 39s/10min 15s, avg throughput: 16.62 img/sec , avg loss: 3.6609\nepoch 0, b 2300/3479, 18min 27s/9min 27s, avg throughput: 16.62 img/sec , avg loss: 3.6656\nepoch 0, b 2400/3479, 19min 23s/8min 42s, avg throughput: 16.51 img/sec , avg loss: 3.6294\nepoch 0, b 2500/3479, 20min 6s/7min 52s, avg throughput: 16.57 img/sec , avg loss: 3.6137\nepoch 0, b 2600/3479, 20min 47s/7min 1s, avg throughput: 16.67 img/sec , avg loss: 3.5887\nepoch 0, b 2700/3479, 21min 46s/6min 17s, avg throughput: 16.53 img/sec , avg loss: 3.5784\nepoch 0, b 2800/3479, 22min 36s/5min 29s, avg throughput: 16.51 img/sec , avg loss: 3.5418\nepoch 0, b 2900/3479, 23min 20s/4min 39s, avg throughput: 16.56 img/sec , avg loss: 3.5319\nepoch 0, b 3000/3479, 24min 5s/3min 50s, avg throughput: 16.6 img/sec , avg loss: 3.4948\n</pre> In\u00a0[17]: Copied! <pre>from albumentations import Resize, Compose\nfrom histolytics.models.cellpose_panoptic import CellposePanoptic\nfrom histolytics.transforms import MinMaxNormalization\nfrom matplotlib import pyplot as plt\nfrom skimage.color import label2rgb\nfrom pathlib import Path\n\n\nmodel = CellposePanoptic.from_pretrained(\n    Path().home()\n    / \"panoptils_refined\"\n    / \"effnet_cpose_panop_weights_epoch_0.safetensors\"\n)\n\n\nmodel.set_inference_mode()\n\n# Resize to multiple of 32 of your own choosing\ntransform = Compose([Resize(1024, 1024), MinMaxNormalization()])\n\nix = 20\nim = png_bytes_to_array(df_val.iloc[ix][\"image\"])\nim = transform(image=im)[\"image\"]\n\nprob = model.predict(im)\nout = model.post_process(prob)\n\n# Validation 'GT' masks\ngt_inst = png_bytes_to_array(df_val.iloc[ix][\"inst\"])\ngt_type = png_bytes_to_array(df_val.iloc[ix][\"type\"])\ngt_sem = png_bytes_to_array(df_val.iloc[ix][\"sem\"])\n\nfig, ax = plt.subplots(2, 4, figsize=(24, 12))\nax = ax.flatten()\nax[0].imshow(im)\nax[0].set_title(\"Input Image\")\nax[1].imshow(label2rgb(out[\"nuc\"][0][0], bg_label=0))\nax[1].set_title(\"Predicted Nuclei Instance Mask\")\nax[2].imshow(label2rgb(out[\"nuc\"][0][1], bg_label=0))  # type_map\nax[2].set_title(\"Predicted Nuclei Type mask\")\nax[3].imshow(label2rgb(out[\"tissue\"][0], bg_label=0))  # tissue_map\nax[3].set_title(\"Predicted Tissue mask\")\nax[4].imshow(im)\nax[4].set_title(\"Input Image\")\nax[5].imshow(label2rgb(gt_inst, bg_label=0))\nax[5].set_title(\"GT Nuclei Instance Mask\")\nax[6].imshow(label2rgb(gt_type, bg_label=0))\nax[6].set_title(\"GT Nuclei Type Mask\")\nax[7].imshow(label2rgb(gt_sem, bg_label=0))\nax[7].set_title(\"GT Tissue Mask\")\n</pre> from albumentations import Resize, Compose from histolytics.models.cellpose_panoptic import CellposePanoptic from histolytics.transforms import MinMaxNormalization from matplotlib import pyplot as plt from skimage.color import label2rgb from pathlib import Path   model = CellposePanoptic.from_pretrained(     Path().home()     / \"panoptils_refined\"     / \"effnet_cpose_panop_weights_epoch_0.safetensors\" )   model.set_inference_mode()  # Resize to multiple of 32 of your own choosing transform = Compose([Resize(1024, 1024), MinMaxNormalization()])  ix = 20 im = png_bytes_to_array(df_val.iloc[ix][\"image\"]) im = transform(image=im)[\"image\"]  prob = model.predict(im) out = model.post_process(prob)  # Validation 'GT' masks gt_inst = png_bytes_to_array(df_val.iloc[ix][\"inst\"]) gt_type = png_bytes_to_array(df_val.iloc[ix][\"type\"]) gt_sem = png_bytes_to_array(df_val.iloc[ix][\"sem\"])  fig, ax = plt.subplots(2, 4, figsize=(24, 12)) ax = ax.flatten() ax[0].imshow(im) ax[0].set_title(\"Input Image\") ax[1].imshow(label2rgb(out[\"nuc\"][0][0], bg_label=0)) ax[1].set_title(\"Predicted Nuclei Instance Mask\") ax[2].imshow(label2rgb(out[\"nuc\"][0][1], bg_label=0))  # type_map ax[2].set_title(\"Predicted Nuclei Type mask\") ax[3].imshow(label2rgb(out[\"tissue\"][0], bg_label=0))  # tissue_map ax[3].set_title(\"Predicted Tissue mask\") ax[4].imshow(im) ax[4].set_title(\"Input Image\") ax[5].imshow(label2rgb(gt_inst, bg_label=0)) ax[5].set_title(\"GT Nuclei Instance Mask\") ax[6].imshow(label2rgb(gt_type, bg_label=0)) ax[6].set_title(\"GT Nuclei Type Mask\") ax[7].imshow(label2rgb(gt_sem, bg_label=0)) ax[7].set_title(\"GT Tissue Mask\") Out[17]: <pre>Text(0.5, 1.0, 'GT Tissue Mask')</pre> <p>After only one epoch of training we can already see that the model is able to predict the nuclei instance segmentation, nuclei type, and tissue type reasonably reasonably well. In real life you would train the model for more epochs to get better results than in this example.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/seg/finetuning/#panoptic-segmentation-model-finetuning","title":"Panoptic Segmentation Model Finetuning\u00b6","text":"<p>In this workflow, we will finetune a panoptic segmentation model using the refined Panoptils dataset that is available in histolytics-hub. See the original dataset in https://sites.google.com/view/panoptils/</p>"},{"location":"user_guide/seg/finetuning/#load-the-panoptils-dataset-from-the-hugging-face-hub","title":"Load the Panoptils dataset from the Hugging Face Hub\u00b6","text":"<p>The code below demonstrates how to load and process the Panoptils dataset from the Hugging Face Hub. This dataset is stored in a compressed, byte-encoded format that requires proper decoding before use.</p>"},{"location":"user_guide/seg/finetuning/#understanding-the-data-format","title":"Understanding the data format\u00b6","text":"<p>The Panoptils dataset is stored in the Hugging Face Hub as a parquet file containing byte-encoded PNG images. This format offers efficient storage and distribution but requires a decoding step before the data can be visualized or used for model training.</p> <p>Each record in the dataset contains multiple byte-encoded fields:</p> <ul> <li>image: The original histology image</li> <li>inst: Instance segmentation mask where each object has a unique ID</li> <li>type: Cell type classification mask</li> <li>sem: Semantic segmentation mask for tissue regions</li> </ul> <p>The data classes are:</p> <pre><code>nuclei_classes = {\n    \"background\": 0,\n    \"neoplastic\": 1,\n    \"stromal\": 2,\n    \"inflammatory\": 3,\n    \"epithelial\": 4,\n    \"other\": 5,\n    \"unknown\": 6,\n}\n\ntissue_classes = {\n    \"background\": 0,\n    \"tumor\": 1,\n    \"stroma\": 2,\n    \"epithelium\": 3,\n    \"junk/debris\": 4,\n    \"blood\": 5,\n    \"other\": 6,\n}\n</code></pre>"},{"location":"user_guide/seg/finetuning/#split-the-panoptils-data-into-training-and-validation-sets","title":"Split the panoptils data into training and validation sets\u00b6","text":"<p>The cell below creates a single train-validation split (80%-20%) while ensuring all patches from the same slide stay together to avoid data leaking.</p>"},{"location":"user_guide/seg/finetuning/#create-h5-files-for-training-and-validation","title":"Create H5 files for training and validation\u00b6","text":"<p>Working directly with the byte-encoded PNG images requires continuous on-the-fly decoding during training. By pre-decoding images and masks into H5 format, we eliminate this repetitive computational overhead, significantly accelerating data loading.</p>"},{"location":"user_guide/seg/finetuning/#visualize-patches-from-h5-train-dataset","title":"Visualize patches from H5 train dataset\u00b6","text":""},{"location":"user_guide/seg/finetuning/#create-the-dataloader-class-for-training-and-validation","title":"Create the Dataloader class for training and validation\u00b6","text":"<p>Instead of the common torch <code>DataLoader</code>, we will be using <code>torchdata.nodes</code> to create our dataloader API, which offers significant performance advantages. The NodesDataLoader implementation in the next cell creates an efficient data loading pipeline for our training and validation purposes.</p>"},{"location":"user_guide/seg/finetuning/#initialize-the-model-augmentations-datasets-dataloaders-optimizer-and-loss-functions","title":"Initialize the model, augmentations, datasets, dataloaders, optimizer, and loss functions\u00b6","text":"<p>Here we will initialize the model, augmentations, datasets, dataloaders, optimizer, and loss functions.</p> <p>Model</p> <ul> <li>We will be using the <code>CellposePanoptic</code> model with EfficientNet-B5 backbone from the pytorch-image-models library.</li> </ul> <p>Transformations</p> <ul> <li>During training we will apply random horizontal and vertical flips, and the StronAugment pipeline</li> <li>The nuclei instance masks are transformed with the <code>CellposeTransform</code> that convert the masks into regressable heat diffusion flow maps. See the Cellpose paper</li> </ul> <p>Losses for different outputs</p> <ul> <li>We will use the <code>MSELoss</code> for the regression loss of the heat diffusion flow maps.</li> <li>For the semantic nuclei type loss, we will use a <code>JointLoss</code>, consisting of <code>CELoss</code>, and <code>DiceLoss</code>. Additionally we will use <code>apply_sd=True</code> in the CELoss that activates spectral decoupling regularization when computing the loss. See this paper for more details.</li> <li>For the semantic tissue type loss, we will use the same <code>JointLoss</code> as above.</li> <li>All these losses will be combined into a single <code>MultiTaskLoss</code> that will be used to train the model.</li> </ul>"},{"location":"user_guide/seg/finetuning/#start-training-the-model","title":"Start training the model\u00b6","text":"<p>We will train/finetune the model with a simple train loop. For the sake of the example, we will train only for 1 epoch.</p>"},{"location":"user_guide/seg/finetuning/#run-inference-on-a-validation-image","title":"Run inference on a validation image\u00b6","text":""},{"location":"user_guide/seg/getting_started_seg/","title":"Getting started","text":""},{"location":"user_guide/seg/getting_started_seg/#1-load-a-pre-trained-model","title":"1. Load a pre-trained model","text":"<p>Pre-trained weights can be found on the histolytics model hub or downloaded automatically when calling <code>from_pretrained</code>. Make sure you have an internet connection for the first use.</p> <p>Available segmentation model architectures are:</p> <ul> <li><code>CellposePanoptic</code></li> <li><code>HoverNetPanoptic</code></li> <li><code>StarDistPanoptic</code></li> <li><code>CellVitPanoptic</code></li> <li><code>CPPNetPanoptic</code></li> </ul> <pre><code>from histolytics.models.cellpose_panoptic import CellposePanoptic\n# from histolytics.models.hovernet_panoptic import HoverNetPanoptic\n# from histolytics.models.stardist_panoptic import StarDistPanoptic\n\n\nmodel = CellposePanoptic.from_pretrained(\"hgsc_v1_efficientnet_b5\")\n# model = HoverNetPanoptic.from_pretrained(\"hgsc_v1_efficientnet_b5\")\n# model = StarDistPanoptic.from_pretrained(\"hgsc_v1_efficientnet_b5\")\n</code></pre>"},{"location":"user_guide/seg/getting_started_seg/#2-run-inference-for-one-image","title":"2. Run inference for one image","text":"<pre><code>from albumentations import Resize, Compose\nfrom histolytics.utils import FileHandler\nfrom histolytics.transforms import MinMaxNormalization\n\nmodel.set_inference_mode()\n\n# Resize to multiple of 32 of your own choosing\ntransform = Compose([Resize(1024, 1024), MinMaxNormalization()])\n\nim = FileHandler.read_img(IMG_PATH)\nim = transform(image=im)[\"image\"]\n\nprob = model.predict(im)\nout = model.post_process(prob)\n# out = {\"nuc\": [(nuc instances (H, W), nuc types (H, W))], \"cyto\": None, \"tissue\": None}\n</code></pre>"},{"location":"user_guide/seg/getting_started_seg/#21-run-inference-for-image-batch","title":"2.1 Run inference for image batch","text":"<pre><code>import torch\nfrom histolytics.utils import FileHandler\n\nmodel.set_inference_mode()\n\n# dont use random matrices IRL\nbatch = torch.rand(8, 3, 1024, 1024)\n\nprob = model.predict(im)\nout = model.post_process(prob)\n# out = {\n#  \"nuc\": [\n#    (nuc instances (H, W), nuc types (H, W)),\n#    (nuc instances (H, W), nuc types (H, W)),\n#    .\n#    .\n#    .\n#    (nuc instances (H, W), nuc types (H, W))\n#  ],\n#  \"tissue\": [\n#    (nuc instances (H, W), nuc types (H, W)),\n#    (nuc instances (H, W), nuc types (H, W)),\n#    .\n#    .\n#    .\n#    (nuc instances (H, W), nuc types (H, W))\n#  ],\n#  \"cyto\": None,\n#}\n</code></pre>"},{"location":"user_guide/seg/getting_started_seg/#3-visualize-output","title":"3. Visualize output","text":"<p><pre><code>from matplotlib import pyplot as plt\nfrom skimage.color import label2rgb\n\nfig, ax = plt.subplots(1, 4, figsize=(24, 6))\nax[0].imshow(im)\nax[1].imshow(label2rgb(out[\"nuc\"][0][0], bg_label=0)) # inst_map\nax[2].imshow(label2rgb(out[\"nuc\"][0][1], bg_label=0)) # type_map\nax[3].imshow(label2rgb(out[\"tissue\"][0], bg_label=0)) # tissue_map\n</code></pre> </p>"},{"location":"user_guide/seg/panoptic_segmentation/","title":"WSI Segmentation","text":"<p>This workflow demonstrates how to perform panoptic segmentation on WSIs using a panoptic Cellpose model, trained on high-grade serous carcinoma (HGSC) H&amp;E images (20x resolution). The model weights are available from the histolytics-hub on Hugging Face.</p> <p>Panoptic segmentation enables simultaneous segmentation and classification of both nuclei and tissue regions within WSIs, enabling spatially aware downstream analyses.</p> <p>For more details of the available pre-trained models, visit the histolytics-hub on Hugging Face.</p> <p>In this workflow, we will:</p> <ul> <li>Load a pre-trained panoptic Cellpose model.</li> <li>Prepare and process WSIs for segmentation.</li> <li>Run large-scale inference and save the results to .parquet files</li> <li>Visualize results.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># !pip install albumentations\n# !pip install cucim-cu12\n</pre> # !pip install albumentations # !pip install cucim-cu12 In\u00a0[3]: Copied! <pre>from platform import python_version\n\nimport torch\nimport albumentations as A\nimport cellseg_models_pytorch\n\nprint(\"torch version:\", torch.__version__)\nprint(\"albumentations version:\", A.__version__)\nprint(\"cellseg_models_pytorch version:\", cellseg_models_pytorch.__version__)\nprint(\"python version:\", python_version())\n</pre> from platform import python_version  import torch import albumentations as A import cellseg_models_pytorch  print(\"torch version:\", torch.__version__) print(\"albumentations version:\", A.__version__) print(\"cellseg_models_pytorch version:\", cellseg_models_pytorch.__version__) print(\"python version:\", python_version()) <pre>torch version: 2.7.0+cu126\nalbumentations version: 2.0.6\ncellseg_models_pytorch version: 0.1.26\npython version: 3.12.3\n</pre> In\u00a0[4]: Copied! <pre>from histolytics.models.cellpose_panoptic import CellposePanoptic\n\nmodel = CellposePanoptic.from_pretrained(weights=\"hgsc_v1_efficientnet_b5\")\nmodel.set_inference_mode(mixed_precision=False)\n</pre> from histolytics.models.cellpose_panoptic import CellposePanoptic  model = CellposePanoptic.from_pretrained(weights=\"hgsc_v1_efficientnet_b5\") model.set_inference_mode(mixed_precision=False) In\u00a0[\u00a0]: Copied! <pre>from histolytics.wsi.slide_reader import SlideReader\nfrom histolytics.wsi.utils import get_sub_grids\nfrom pathlib import Path\n\nsl_path = Path().home() / \"sample.tiff\"  # Replace with your WSI path\n\n# Initialize SlideReader with CUCIM backend\nreader = SlideReader(sl_path, backend=\"CUCIM\")\n\n# Get the tissue mask and coordinates\nthresh, tissue_mask = reader.get_tissue_mask(level=-2)\ncoordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)\n\n# Get the contiguous sub-grids\nsub_grids = get_sub_grids(coordinates)\nfiltered_sub_grids = [grid for grid in sub_grids if len(grid) &gt; 100]\n\n# pick the first sub-grid\nsub_coords = filtered_sub_grids[0]\n\n# visualize the thumbnail and the sub-coordinates\n# thumbnail = reader.read_level(-2)\n# reader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=1)\n</pre> from histolytics.wsi.slide_reader import SlideReader from histolytics.wsi.utils import get_sub_grids from pathlib import Path  sl_path = Path().home() / \"sample.tiff\"  # Replace with your WSI path  # Initialize SlideReader with CUCIM backend reader = SlideReader(sl_path, backend=\"CUCIM\")  # Get the tissue mask and coordinates thresh, tissue_mask = reader.get_tissue_mask(level=-2) coordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)  # Get the contiguous sub-grids sub_grids = get_sub_grids(coordinates) filtered_sub_grids = [grid for grid in sub_grids if len(grid) &gt; 100]  # pick the first sub-grid sub_coords = filtered_sub_grids[0]  # visualize the thumbnail and the sub-coordinates # thumbnail = reader.read_level(-2) # reader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=1) In\u00a0[\u00a0]: Copied! <pre>len(sub_coords)  # 2620 1024x1024 tiles\n</pre> len(sub_coords)  # 2620 1024x1024 tiles Out[\u00a0]: <pre>2620</pre> In\u00a0[\u00a0]: Copied! <pre>import albumentations as A\nfrom histolytics.transforms import MinMaxNormalization\nfrom histolytics.wsi.wsi_segmenter import WsiPanopticSegmenter\n\n# We will use the MinMaxNormalization transform to normalize the input images\n# This was also used in the training of the model\nimg_transforms = A.Compose([MinMaxNormalization()])\n\nwsi_segmenter = WsiPanopticSegmenter(\n    reader=reader,\n    model=model,\n    level=0,  # 20x magnification\n    coordinates=sub_coords,\n    batch_size=8,\n    transforms=img_transforms,\n)\n</pre> import albumentations as A from histolytics.transforms import MinMaxNormalization from histolytics.wsi.wsi_segmenter import WsiPanopticSegmenter  # We will use the MinMaxNormalization transform to normalize the input images # This was also used in the training of the model img_transforms = A.Compose([MinMaxNormalization()])  wsi_segmenter = WsiPanopticSegmenter(     reader=reader,     model=model,     level=0,  # 20x magnification     coordinates=sub_coords,     batch_size=8,     transforms=img_transforms, ) In\u00a0[10]: Copied! <pre>from pathlib import Path\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nsave_dir = Path().home() / \"seg_res\"\n\nwsi_segmenter.segment(\n    save_dir=save_dir,\n    use_sliding_win=False,\n    use_async_postproc=True,\n    postproc_njobs=4,\n    postproc_start_method=\"threading\",\n    class_dict_nuc=model.nuc_classes,\n    class_dict_tissue=model.tissue_classes,\n)\n</pre> from pathlib import Path import warnings  warnings.filterwarnings(\"ignore\")  save_dir = Path().home() / \"seg_res\"  wsi_segmenter.segment(     save_dir=save_dir,     use_sliding_win=False,     use_async_postproc=True,     postproc_njobs=4,     postproc_start_method=\"threading\",     class_dict_nuc=model.nuc_classes,     class_dict_tissue=model.tissue_classes, ) <pre>  0%|          | 0/328 [00:00&lt;?, ?batch/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328/328 [47:59&lt;00:00,  8.78s/batch] \n</pre> In\u00a0[12]: Copied! <pre>wsi_segmenter.merge_instances(\n    src=save_dir / \"nuc\", dst=save_dir / \"out.parquet\", simplify_level=0.1\n)\n</pre> wsi_segmenter.merge_instances(     src=save_dir / \"nuc\", dst=save_dir / \"out.parquet\", simplify_level=0.1 ) <pre>Merging objects (x-axis): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73/73 [03:23&lt;00:00,  2.79s/it]\nMerging objects (y-axis): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [03:10&lt;00:00,  2.68s/it]\n</pre> <p>NOTE: When visualizing the segmentation results with GeoPandas, the plots may appear upside down. This is because GeoPandas uses the origin at the bottom-left corner, whereas image data typically uses the top-left as the origin. The actual coordinates in the data are not flipped and this effect is only present in the visualization.</p> In\u00a0[14]: Copied! <pre>import geopandas as gpd\n\ncells = gpd.read_parquet(save_dir / \"out.parquet\")\ncells.plot(column=\"class_name\", figsize=(10, 10), legend=True)\n</pre> import geopandas as gpd  cells = gpd.read_parquet(save_dir / \"out.parquet\") cells.plot(column=\"class_name\", figsize=(10, 10), legend=True) Out[14]: <pre>&lt;Axes: &gt;</pre> In\u00a0[\u00a0]: Copied! <pre>wsi_segmenter.merge_tissues(\n    src=save_dir / \"tissue\", dst=save_dir / \"out_tissue.parquet\", simplify_level=0.5\n)\n</pre> wsi_segmenter.merge_tissues(     src=save_dir / \"tissue\", dst=save_dir / \"out_tissue.parquet\", simplify_level=0.5 ) <pre>Merging tissue columns: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 74/74 [00:43&lt;00:00,  1.68it/s]\nMerging tissues by class: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [03:06&lt;00:00, 31.06s/it]\n</pre> In\u00a0[16]: Copied! <pre>tissues = gpd.read_parquet(save_dir / \"out_tissue.parquet\")\ntissues.plot(column=\"class_name\", figsize=(10, 10), legend=True)\n</pre> tissues = gpd.read_parquet(save_dir / \"out_tissue.parquet\") tissues.plot(column=\"class_name\", figsize=(10, 10), legend=True) Out[16]: <pre>&lt;Axes: &gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/seg/panoptic_segmentation/#easy-initialization-and-model-weights-loading","title":"Easy Initialization and Model Weights Loading\u00b6","text":"<p>In Histolytics, the panoptic segmentation models can be initialized with pre-trained weights using the <code>from_pretrained</code>-method. By specifying the desired weights (e.g., <code>\"hgsc_v1_efficientnet_b5\"</code>), the model automatically downloads and loads the appropriate weights from the model hub.</p>"},{"location":"user_guide/seg/panoptic_segmentation/#slidereader-reading-whole-slide-images","title":"SlideReader: Reading Whole Slide Images\u00b6","text":"<p>The <code>SlideReader</code> object handles reading WSIs. The reader backends include:</p> <ul> <li>CUCIM: NVIDIA's accelerated backend for efficient WSI reading.</li> <li>OPENSLIDE: A widely-used open-source backend compatible with many WSI formats.</li> <li>BIOIO: A flexible backend supporting a broad range of bioimaging formats.</li> </ul> <p>After loading a slide, <code>SlideReader</code> can generate a tissue mask using Otsu-based thresholding, which automatically distinguishes tissue regions from the background. For further refining, the <code>get_sub_grids</code> function can be used to filter tile coordinates based on connected components in the tissue mask. This ensures that only contiguous tissue regions are selected for downstream processing, improving both efficiency and accuracy in large-scale WSI analysis.</p>"},{"location":"user_guide/seg/panoptic_segmentation/#wsipanopticsegmenter-running-panoptic-segmentation-on-wsis","title":"WsiPanopticSegmenter: Running Panoptic Segmentation on WSIs\u00b6","text":"<p>The <code>WsiPanopticSegmenter</code> class orchestrates the process of running panoptic segmentation on WSIs. It manages the model inference, and post-processing, making it easy to apply a panoptic segmentation model to large WSIs. It takes in a <code>SlideReader</code> object and a pre-trained panoptic segmentation model, and provides methods to run inference on the entire slide.</p>"},{"location":"user_guide/seg/panoptic_segmentation/#run-segmentation-on-a-wsi","title":"Run segmentation on a WSI\u00b6","text":"<p>Note: Running panoptic segmentation on a whole slide image (WSI) can take a some amount of time, depending on the size of the slide and available hardware resources.</p>"},{"location":"user_guide/seg/panoptic_segmentation/#merging-nuclear-and-tissue-segmentation-instances","title":"Merging Nuclear and Tissue Segmentation Instances\u00b6","text":"<p>After running <code>.segment</code>, the WSI is processed tile-by-tile, resulting in segmented nuclear and tissue instances for each tile. To reconstruct the full WSI segmentation, these tile-level results must be merged into a single, coherent set of instances for both nuclei and tissue.</p> <p>The <code>WsiPanopticSegmenter</code> provides dedicated methods for this purpose:</p> <ul> <li><code>merge_instances</code>: Merges nuclear segmentation results from all tiles into a single file.</li> <li><code>merge_tissues</code>: Merges tissue segmentation results from all tiles into a single file.</li> </ul> <p>These methods handle spatial alignment, instance ID management, and optional geometry simplification, ensuring that the final merged outputs are ready for downstream analysis and visualization. Each output is saved as a <code>.parquet</code> file for efficient storage and access.</p>"},{"location":"user_guide/seg/selecting_tiles/","title":"Tile Selection","text":"<p>In this workflow, we show how to filter WSI tiles. This is recommended before applying segmentation to the WSI, to not segment redundant regions of the slide.</p> <p>We will be using test WSIs from https://openslide.cs.cmu.edu/download/openslide-testdata/ in this workflow.</p> In\u00a0[1]: Copied! <pre>from platform import python_version\n\nprint(\"python version:\", python_version())\n</pre> from platform import python_version  print(\"python version:\", python_version()) <pre>python version: 3.12.3\n</pre> In\u00a0[2]: Copied! <pre>from histolytics.wsi.slide_reader import SlideReader\nfrom pathlib import Path\n\nin_dir = Path().home() / \"openslide_test\"\nslide = in_dir / \"CMU-1-JP2K-33005.svs\"\nreader = SlideReader(slide, backend=\"CUCIM\")\n</pre> from histolytics.wsi.slide_reader import SlideReader from pathlib import Path  in_dir = Path().home() / \"openslide_test\" slide = in_dir / \"CMU-1-JP2K-33005.svs\" reader = SlideReader(slide, backend=\"CUCIM\") In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Get the tissue mask and coordinates\nthresh, tissue_mask = reader.get_tissue_mask(level=-1)\ncoordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)\n\nplt.imshow(tissue_mask)\n</pre> import matplotlib.pyplot as plt  # Get the tissue mask and coordinates thresh, tissue_mask = reader.get_tissue_mask(level=-1) coordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)  plt.imshow(tissue_mask) Out[3]: <pre>&lt;matplotlib.image.AxesImage at 0x7eecc3590c80&gt;</pre> In\u00a0[4]: Copied! <pre>from histolytics.wsi.utils import get_sub_grids\n\n\n# Get the contiguous sub-grids\nsub_grids = get_sub_grids(coordinates)\nfiltered_sub_grids = [grid for grid in sub_grids if len(grid) &gt; 50]\n</pre> from histolytics.wsi.utils import get_sub_grids   # Get the contiguous sub-grids sub_grids = get_sub_grids(coordinates) filtered_sub_grids = [grid for grid in sub_grids if len(grid) &gt; 50] <p>Visualize the first sub-grid</p> In\u00a0[5]: Copied! <pre>from PIL import Image\n\n\n#  helper downsampling function for visualization\ndef downsample_thumbnail(annotated_thumbnail, scale_factor=0.3):\n    new_width = int(annotated_thumbnail.width * scale_factor)\n    new_height = int(annotated_thumbnail.height * scale_factor)\n    return annotated_thumbnail.resize((new_width, new_height), Image.LANCZOS)\n\n\n# pick the first sub-grid\nsub_coords = filtered_sub_grids[0]\n\nthumbnail = reader.read_level(-1)\nanno_thumb = reader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=3)\ndownsample_thumbnail(anno_thumb)\n</pre> from PIL import Image   #  helper downsampling function for visualization def downsample_thumbnail(annotated_thumbnail, scale_factor=0.3):     new_width = int(annotated_thumbnail.width * scale_factor)     new_height = int(annotated_thumbnail.height * scale_factor)     return annotated_thumbnail.resize((new_width, new_height), Image.LANCZOS)   # pick the first sub-grid sub_coords = filtered_sub_grids[0]  thumbnail = reader.read_level(-1) anno_thumb = reader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=3) downsample_thumbnail(anno_thumb) Out[5]: <p>Visualize the third sub-grid</p> In\u00a0[6]: Copied! <pre># pick the first sub-grid\nsub_coords = filtered_sub_grids[2]\n\nthumbnail = reader.read_level(-1)\nreader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=3)\n\nanno_thumb = reader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=3)\ndownsample_thumbnail(anno_thumb)\n</pre> # pick the first sub-grid sub_coords = filtered_sub_grids[2]  thumbnail = reader.read_level(-1) reader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=3)  anno_thumb = reader.get_annotated_thumbnail(thumbnail, sub_coords, linewidth=3) downsample_thumbnail(anno_thumb) Out[6]: In\u00a0[7]: Copied! <pre>slide = in_dir / \"Philips-1.tiff\"\nreader = SlideReader(slide, backend=\"CUCIM\")\n\nthresh, tissue_mask = reader.get_tissue_mask(level=-2)\ncoordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)\n\nthumbnail = reader.read_level(-2)\nanno_thumb = reader.get_annotated_thumbnail(thumbnail, coordinates, linewidth=2)\nanno_thumb\n</pre> slide = in_dir / \"Philips-1.tiff\" reader = SlideReader(slide, backend=\"CUCIM\")  thresh, tissue_mask = reader.get_tissue_mask(level=-2) coordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)  thumbnail = reader.read_level(-2) anno_thumb = reader.get_annotated_thumbnail(thumbnail, coordinates, linewidth=2) anno_thumb Out[7]: In\u00a0[8]: Copied! <pre>thresh, tissue_mask = reader.get_tissue_mask(level=-2, threshold=225)\ncoordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)\n\nthumbnail = reader.read_level(-2)\nanno_thumb = reader.get_annotated_thumbnail(thumbnail, coordinates, linewidth=2)\nanno_thumb\n</pre> thresh, tissue_mask = reader.get_tissue_mask(level=-2, threshold=225) coordinates = reader.get_tile_coordinates(width=1024, tissue_mask=tissue_mask)  thumbnail = reader.read_level(-2) anno_thumb = reader.get_annotated_thumbnail(thumbnail, coordinates, linewidth=2) anno_thumb Out[8]: In\u00a0[9]: Copied! <pre>from shapely.geometry import box\n\nbbox = box(15000, 12000, 25000, 22000)\n\ncoordinates = reader.get_tile_coordinates(\n    width=1024, tissue_mask=None, annotations=bbox\n)\n</pre> from shapely.geometry import box  bbox = box(15000, 12000, 25000, 22000)  coordinates = reader.get_tile_coordinates(     width=1024, tissue_mask=None, annotations=bbox ) In\u00a0[10]: Copied! <pre>reader.get_annotated_thumbnail(thumbnail, coordinates, linewidth=3)\n</pre> reader.get_annotated_thumbnail(thumbnail, coordinates, linewidth=3) Out[10]: In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/seg/selecting_tiles/#tissue-mask-generation","title":"Tissue Mask Generation\u00b6","text":"<p>The <code>get_tissue_mask</code> method of the <code>SlideReader</code> class is used to automatically identify tissue regions within a whole slide image. This method applies Otsu thresholding to distinguish tissue from the background, producing a binary mask (<code>tissue_mask</code>). The threshold value (<code>thresh</code>) used for segmentation is also returned.</p>"},{"location":"user_guide/seg/selecting_tiles/#connected-components-based-tile-grid-selection","title":"Connected components based tile-grid selection\u00b6","text":"<p>Sometimes WSI slides contain multiple tissue regions, and we want to focus on only some of these areas for further analysis. The <code>get_sub_grids</code> function can be used to group spatially contiguous tiles into sub-grids, representing distinct tissue regions within the slide. The <code>get_sub_grids</code> function takes the tile coordinates and identifies these connected regions. To focus on substantial tissue areas, we filter out sub-grids with fewer than 50 tiles.</p> <p>The <code>filtered_sub_grids</code> variable contains a list of spatially contiguous tile groups, each representing a distinct tissue region within the slide. The sub-grids are ordered by their minimum y-coordinate, meaning the first sub-grid in the list corresponds to the tissue region closest to the top of the slide (with the origin at the top-left corner).</p>"},{"location":"user_guide/seg/selecting_tiles/#adjusting-threshold","title":"Adjusting threshold\u00b6","text":"<p>Sometimes the tissue masking with Otsu threshold has to be adjusted, for example, when the tissue contains fat tissue which will be mistaken as background if the threshold is not set correctly.</p>"},{"location":"user_guide/seg/selecting_tiles/#selecting-tiles-by-annotations","title":"Selecting tiles by annotations\u00b6","text":"<p>In cases where tissue masking is not sufficient, you can select tiles by providing pre-defined annotations which are used to filter tiles.</p>"},{"location":"user_guide/spatial/clustering/","title":"Clustering & Cluster Features","text":"In\u00a0[1]: Copied! <pre># Let's load some data to play around with\nimport matplotlib.pyplot as plt\nfrom histolytics.data import cervix_nuclei, cervix_tissue\n\ntis = cervix_tissue()\nnuc = cervix_nuclei()\n\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False)\nax.set_axis_off()\n</pre> # Let's load some data to play around with import matplotlib.pyplot as plt from histolytics.data import cervix_nuclei, cervix_tissue  tis = cervix_tissue() nuc = cervix_nuclei()  fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False) ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.spatial_clust.density_clustering import density_clustering\n\nnuc_imm = nuc[nuc[\"class_name\"] == \"inflammatory\"]\nlabels = density_clustering(nuc_imm, eps=250, min_samples=50, method=\"dbscan\")\nnuc_imm = nuc_imm.assign(labels=labels)\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", alpha=0.3)\nax = nuc.plot(ax=ax, color=\"blue\", alpha=0.3, aspect=1)\nnuc_imm.plot(\n    ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\", legend=True, categorical=True\n)\nax.set_axis_off()\nnuc_imm.head(3)\n</pre> from histolytics.spatial_clust.density_clustering import density_clustering  nuc_imm = nuc[nuc[\"class_name\"] == \"inflammatory\"] labels = density_clustering(nuc_imm, eps=250, min_samples=50, method=\"dbscan\") nuc_imm = nuc_imm.assign(labels=labels)  ax = tis.plot(figsize=(10, 10), column=\"class_name\", alpha=0.3) ax = nuc.plot(ax=ax, color=\"blue\", alpha=0.3, aspect=1) nuc_imm.plot(     ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\", legend=True, categorical=True ) ax.set_axis_off() nuc_imm.head(3) Out[2]: geometry class_name labels 14 POLYGON ((925 5211.02, 924.01 5212, 924.01 522... inflammatory -1 17 POLYGON ((899 5607.02, 898.01 5608, 898.01 561... inflammatory -1 22 POLYGON ((873 5284.02, 871.01 5286, 871.01 529... inflammatory -1 <p>The immune cell clusters are highlighted with different colors in the image.</p> In\u00a0[3]: Copied! <pre>from histolytics.spatial_clust.centrography import cluster_tendency\nfrom histolytics.spatial_clust.clust_metrics import cluster_feats\nimport pandas as pd\nimport geopandas as gpd\n\n# compute cluster centroids and features, use groupby to apply the functions\n# to each cluster separately\nclust_centroids = (\n    nuc_imm.groupby(\"labels\")\n    .apply(lambda g: cluster_tendency(g, \"mean\"), include_groups=False)\n    .reset_index(drop=False, name=\"geometry\")\n)\n\n\nclust_features = (\n    nuc_imm.groupby(\"labels\")\n    .apply(\n        lambda x: cluster_feats(x, hull_type=\"convex_hull\", normalize_orientation=True),\n        include_groups=False,\n    )\n    .reset_index(drop=False)\n)\n\n# merge the cluster centroids and features\nclust_feats = clust_centroids.merge(clust_features, on=\"labels\")\n\n# convert to GeoDataFrame\nclust_feats = gpd.GeoDataFrame(clust_feats, geometry=\"geometry\")\n\n# drop the noise cluster\nclust_feats = clust_feats[clust_feats[\"labels\"] != -1]\n\n# plot the cluster centroids and the nuclei, color by the size\nax = tis.plot(figsize=(10, 10), column=\"class_name\", alpha=0.3)\nax = nuc.plot(ax=ax, color=\"blue\", alpha=0.3, aspect=1)\nax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\")\nclust_feats.plot(ax=ax, column=\"size\", aspect=1, cmap=\"turbo\", legend=True)\nax.set_axis_off()\nclust_feats\n</pre> from histolytics.spatial_clust.centrography import cluster_tendency from histolytics.spatial_clust.clust_metrics import cluster_feats import pandas as pd import geopandas as gpd  # compute cluster centroids and features, use groupby to apply the functions # to each cluster separately clust_centroids = (     nuc_imm.groupby(\"labels\")     .apply(lambda g: cluster_tendency(g, \"mean\"), include_groups=False)     .reset_index(drop=False, name=\"geometry\") )   clust_features = (     nuc_imm.groupby(\"labels\")     .apply(         lambda x: cluster_feats(x, hull_type=\"convex_hull\", normalize_orientation=True),         include_groups=False,     )     .reset_index(drop=False) )  # merge the cluster centroids and features clust_feats = clust_centroids.merge(clust_features, on=\"labels\")  # convert to GeoDataFrame clust_feats = gpd.GeoDataFrame(clust_feats, geometry=\"geometry\")  # drop the noise cluster clust_feats = clust_feats[clust_feats[\"labels\"] != -1]  # plot the cluster centroids and the nuclei, color by the size ax = tis.plot(figsize=(10, 10), column=\"class_name\", alpha=0.3) ax = nuc.plot(ax=ax, color=\"blue\", alpha=0.3, aspect=1) ax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\") clust_feats.plot(ax=ax, column=\"size\", aspect=1, cmap=\"turbo\", legend=True) ax.set_axis_off() clust_feats Out[3]: labels geometry area dispersion size orientation 1 0 POINT (886.159 7418.012) 2.817947e+05 234.488171 82.0 5.482046 2 1 POINT (2759.247 3884.393) 1.156891e+07 1681.809966 4272.0 75.204700 3 2 POINT (2691.992 9388.149) 1.418761e+06 525.666167 412.0 21.698906 4 3 POINT (4498.243 1071.52) 6.325208e+05 311.207442 311.0 14.691102 5 4 POINT (1934.696 8181.984) 3.547952e+05 218.192396 95.0 21.848834 6 5 POINT (2761.256 8575.997) 2.762148e+05 209.376598 79.0 14.740579 In\u00a0[6]: Copied! <pre>%%timeit\ndensity_clustering(nuc, eps=250, min_samples=50, method=\"dbscan\")\n</pre> %%timeit density_clustering(nuc, eps=250, min_samples=50, method=\"dbscan\") <pre>136 ms \u00b1 5.15 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[7]: Copied! <pre>%%timeit\ndensity_clustering(nuc, eps=250, min_samples=50, method=\"optics\")\n</pre> %%timeit density_clustering(nuc, eps=250, min_samples=50, method=\"optics\") <pre>8.04 s \u00b1 147 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[8]: Copied! <pre>%%timeit\ndensity_clustering(nuc, eps=250, min_samples=50, method=\"hdbscan\")\n</pre> %%timeit density_clustering(nuc, eps=250, min_samples=50, method=\"hdbscan\") <pre>1.06 s \u00b1 15 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[10]: Copied! <pre>%%timeit\ndensity_clustering(nuc, eps=250, min_samples=50, method=\"adbscan\")\n</pre> %%timeit density_clustering(nuc, eps=250, min_samples=50, method=\"adbscan\") <pre>36.1 s \u00b1 328 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/clustering/#introduction","title":"Introduction\u00b6","text":"<p>What is Spatial Clustering? \ud83d\udd0d Spatial clustering algorithms identify regions where data points are more concentrated than in the surrounding area. Unlike other clustering methods that might group objects by their attributes (e.g., size or color), spatial clustering is concerned solely with an object's location and its relationship to its neighbors.</p> <p>Why is Clustering Nuclei Useful? \ud83e\uddec Clustering nuclei is a powerful technique for a variety of reasons in biological analysis:</p> <p>Identifying Micro-environments: Tissues are not uniform. By clustering nuclei, you can automatically identify distinct regions within a tissue slide. For example, a dense cluster of tumor cells, a lymphocyte-rich area, or a cluster of stromal cells. These regions represent different micro-environments, each with its own unique biological properties.</p> <p>Quantifying Cluster Features Clustering allows you to move beyond qualitative descriptions like \"there seems to be a lot of cells here\" and instead provide quantitative metrics. For each identified cluster, you can compute valuable features:</p> <ul> <li><p>Size and shape: How large and what shape is the cluster? Is it a small, dense collection of cells or a large, dispersed group?</p> </li> <li><p>Dispersion: How spread out are the nuclei within the cluster?</p> </li> </ul> <p>In this tutorial, we will explore how to apply spatial clustering to nuclei segmentation data. We will also show how to compute cluster features of the resulting clusters. Segmented cervix biopsy dataset is used in this tutorial.</p>"},{"location":"user_guide/spatial/clustering/#density-based-clustering","title":"Density Based Clustering\u00b6","text":"<p>In this tutorial, we will use the DBSCAN algorithm to cluster the inflammatory cells that often form spatial clusters in the tissue. The DBSCAN algorithm is a density-based clustering algorithm that groups together points that are closely packed together, marking as outliers points that lie alone in low-density regions. We will use the <code>density_clustering</code>-function which is a wrapper around different density based clustering algorithms from the sklearn and esda libraries. The available algorithms in Histolytics are <code>dbscan</code>, <code>hdbscan</code>, <code>adbscan</code>, and <code>optics</code>.</p> <p>At least, the following parameters should be considered when running density based clustering algorithms:</p> <ul> <li><code>min_samples</code>: The minimum number of samples in a neighborhood for a point to be considered as a core point.</li> <li><code>eps</code>: The maximum distance between two samples for one to be considered as in the neighborhood of the other.</li> </ul>"},{"location":"user_guide/spatial/clustering/#cluster-features","title":"Cluster Features\u00b6","text":"<p>Next, we will analyze these immune clusters by computing cluster features. We will first get the cluster tendencies i.e. we will extract the cluster centroids. Then we will calculate the cluster size, area, dispersion and orientation for each cluster, and finally we will plot some visualizations of the clusters. We will use the <code>cluster_tendency</code>-function to extract the cluster centroids, and then the <code>cluster_feats</code>-function to compute the size, area, dispersion and orientation of the clusters.</p>"},{"location":"user_guide/spatial/clustering/#benchmark","title":"Benchmark\u00b6","text":"<p>The run-time benchmarks below show that from the included density based clustering algorithms, the DBSCAN is clearly the most performant, while the HDBSCAN run-time is still competitive. This makes these algorithms well-suited for high-throughput histopathology applications where large datasets are common. The OPTICS algorithm and ADBSCAN algorithms have some scalability issues, thus, these are not recommended for WSI-level datasets. The run-time benchmark was done with the same cervical biopsy dataset with ~19K nuclei that was used in the tutorial.</p>"},{"location":"user_guide/spatial/graphs/","title":"Graphs & Graph Features","text":"In\u00a0[1]: Copied! <pre>from histolytics.spatial_graph.graph import fit_graph\nfrom histolytics.utils.gdf import set_uid\nfrom histolytics.data import hgsc_cancer_nuclei\n\nnuc = hgsc_cancer_nuclei()\nnuc = set_uid(nuc)  # Ensure the GeoDataFrame has a unique identifier\n\n# fit spatial weights using Delaunay triangulation\nw, w_gdf = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100)\nax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1)\nw_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1)\nax.set_axis_off()\nw_gdf.head(5)\n</pre> from histolytics.spatial_graph.graph import fit_graph from histolytics.utils.gdf import set_uid from histolytics.data import hgsc_cancer_nuclei  nuc = hgsc_cancer_nuclei() nuc = set_uid(nuc)  # Ensure the GeoDataFrame has a unique identifier  # fit spatial weights using Delaunay triangulation w, w_gdf = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100) ax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1) w_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1) ax.set_axis_off() w_gdf.head(5) Out[1]: index focal neighbor weight focal_centroid_x focal_centroid_y neighbor_centroid_x neighbor_centroid_y focal_class_name neighbor_class_name class_name geometry 0 0 0 1 1.0 1400.037980 1.692484 1386.458579 9.580762 connective connective connective-connective LINESTRING (1400.038 1.692, 1386.459 9.581) 1 1 0 4 1.0 1400.037980 1.692484 1306.059654 2.527988 connective connective connective-connective LINESTRING (1400.038 1.692, 1306.06 2.528) 2 6 1 4 1.0 1386.458579 9.580762 1306.059654 2.527988 connective connective connective-connective LINESTRING (1386.459 9.581, 1306.06 2.528) 3 9 2 3 1.0 1378.296689 170.695478 1318.355274 178.923534 connective connective connective-connective LINESTRING (1378.297 170.695, 1318.355 178.924) 4 13 2 23 1.0 1378.296689 170.695478 1367.454971 219.690997 connective connective connective-connective LINESTRING (1378.297 170.695, 1367.455 219.691) In\u00a0[2]: Copied! <pre># Link counts\nw_gdf.value_counts(\"class_name\")\n</pre> # Link counts w_gdf.value_counts(\"class_name\") Out[2]: <pre>class_name\ninflammatory-inflammatory    1347\nconnective-inflammatory       748\nneoplastic-neoplastic         692\nconnective-connective         582\ninflammatory-neoplastic       228\nconnective-neoplastic          81\nName: count, dtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>import networkx as nx\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Let's fit a graph only to the neoplastic neolei\nneo = nuc.loc[nuc[\"class_name\"] == \"neoplastic\"]\nw, w_gdf = fit_graph(neo, \"delaunay\", id_col=\"uid\", threshold=100)\n# Convert libpysal weights to NetworkX graph\nG = w.to_networkx()\n\n# Compute graph features\ncloseness_centrality = nx.closeness_centrality(G)\naverage_clustering = nx.average_clustering(G)\ndegree_centrality = nx.degree_centrality(G)\nbetweenness_centrality = nx.betweenness_centrality(G)\neigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n\nprint(f\"Average clustering coefficient: {average_clustering:.4f}\")\nprint(\n    f\"Mean closeness centrality: {sum(closeness_centrality.values()) / len(closeness_centrality):.4f}\"\n)\nprint(\n    f\"Mean degree centrality: {sum(degree_centrality.values()) / len(degree_centrality):.4f}\"\n)\nprint(\n    f\"Mean betweenness centrality: {sum(betweenness_centrality.values()) / len(betweenness_centrality):.4f}\"\n)\nprint(\n    f\"Mean eigenvector centrality: {sum(eigenvector_centrality.values()) / len(eigenvector_centrality):.4f}\"\n)\n\n# Add degree centrality back to the GeoDataFrame for visualiation purposes\nneo[\"degree_centrality\"] = degree_centrality.values()\n\n# Visualize degree centrality\nax = nuc.plot(figsize=(10, 10), aspect=1, legend=False)\nax = neo.plot(\n    ax=ax,\n    column=\"degree_centrality\",\n    cmap=\"viridis\",\n    figsize=(10, 10),\n    aspect=1,\n    legend=True,\n    categorical=False,\n)\nax.set_title(\"Degree Centrality\")\nax.set_axis_off()\n</pre> import networkx as nx import warnings  warnings.filterwarnings(\"ignore\")  # Let's fit a graph only to the neoplastic neolei neo = nuc.loc[nuc[\"class_name\"] == \"neoplastic\"] w, w_gdf = fit_graph(neo, \"delaunay\", id_col=\"uid\", threshold=100) # Convert libpysal weights to NetworkX graph G = w.to_networkx()  # Compute graph features closeness_centrality = nx.closeness_centrality(G) average_clustering = nx.average_clustering(G) degree_centrality = nx.degree_centrality(G) betweenness_centrality = nx.betweenness_centrality(G) eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)  print(f\"Average clustering coefficient: {average_clustering:.4f}\") print(     f\"Mean closeness centrality: {sum(closeness_centrality.values()) / len(closeness_centrality):.4f}\" ) print(     f\"Mean degree centrality: {sum(degree_centrality.values()) / len(degree_centrality):.4f}\" ) print(     f\"Mean betweenness centrality: {sum(betweenness_centrality.values()) / len(betweenness_centrality):.4f}\" ) print(     f\"Mean eigenvector centrality: {sum(eigenvector_centrality.values()) / len(eigenvector_centrality):.4f}\" )  # Add degree centrality back to the GeoDataFrame for visualiation purposes neo[\"degree_centrality\"] = degree_centrality.values()  # Visualize degree centrality ax = nuc.plot(figsize=(10, 10), aspect=1, legend=False) ax = neo.plot(     ax=ax,     column=\"degree_centrality\",     cmap=\"viridis\",     figsize=(10, 10),     aspect=1,     legend=True,     categorical=False, ) ax.set_title(\"Degree Centrality\") ax.set_axis_off() <pre>Average clustering coefficient: 0.4372\nMean closeness centrality: 0.1487\nMean degree centrality: 0.0205\nMean betweenness centrality: 0.0201\nMean eigenvector centrality: 0.0562\n</pre> In\u00a0[4]: Copied! <pre>from histolytics.data import cervix_nuclei\nfrom histolytics.utils.gdf import set_uid\n\nnuc = cervix_nuclei()\nnuc = set_uid(nuc)\nnuc\n</pre> from histolytics.data import cervix_nuclei from histolytics.utils.gdf import set_uid  nuc = cervix_nuclei() nuc = set_uid(nuc) nuc Out[4]: geometry class_name uid uid 0 POLYGON ((940.01 5570.02, 939.01 5573, 939 559... connective 0 1 POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ... connective 1 2 POLYGON ((866 5137.02, 862.77 5137.94, 860 513... squamous_epithel 2 3 POLYGON ((932 4777.02, 928 4778.02, 922.81 478... glandular_epithel 3 4 POLYGON ((904 5177.02, 902.01 5178.02, 901.01 ... connective 4 ... ... ... ... 19192 POLYGON ((3015 2614.02, 3013.01 2616, 3013.01 ... inflammatory 19192 19193 POLYGON ((3387 2618.02, 3384.01 2620.02, 3379.... connective 19193 19194 POLYGON ((3655 2622.02, 3650.86 2623.75, 3647 ... inflammatory 19194 19195 POLYGON ((3703 2627.02, 3699.9 2629.16, 3696.0... connective 19195 19196 POLYGON ((3199 2623.02, 3194.64 2626.72, 3191.... inflammatory 19196 <p>19197 rows \u00d7 3 columns</p> In\u00a0[5]: Copied! <pre>%%timeit\n# Delaunay graph\nfit_graph(nuc, method=\"delaunay\", id_col=\"uid\", threshold=50)\n</pre> %%timeit # Delaunay graph fit_graph(nuc, method=\"delaunay\", id_col=\"uid\", threshold=50) <pre>2.6 s \u00b1 40.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[6]: Copied! <pre>%%timeit\nfit_graph(nuc, method=\"knn\", k=5, id_col=\"uid\", threshold=50)\n</pre> %%timeit fit_graph(nuc, method=\"knn\", k=5, id_col=\"uid\", threshold=50) <pre>2.24 s \u00b1 35.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[7]: Copied! <pre>%%timeit\n# distband (run-time also depends on the distance threshold)\nfit_graph(nuc, method=\"distband\", id_col=\"uid\", threshold=50)\n</pre> %%timeit # distband (run-time also depends on the distance threshold) fit_graph(nuc, method=\"distband\", id_col=\"uid\", threshold=50) <pre>2.53 s \u00b1 51.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[8]: Copied! <pre>%%timeit\nfit_graph(nuc, method=\"gabriel\", id_col=\"uid\", threshold=50)\n</pre> %%timeit fit_graph(nuc, method=\"gabriel\", id_col=\"uid\", threshold=50) <pre>2.48 s \u00b1 75.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[9]: Copied! <pre>%%timeit\nfit_graph(nuc, method=\"voronoi\", id_col=\"uid\", threshold=50)\n</pre> %%timeit fit_graph(nuc, method=\"voronoi\", id_col=\"uid\", threshold=50) <pre>22.2 s \u00b1 349 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/graphs/#introduction","title":"Introduction\u00b6","text":"<p>What are Spatial Graphs? \ud83d\udd78\ufe0f A spatial graph is a representation of spatial data as a network, where individual objects are the nodes and the relationships between them are the links or edges. This transformation allows you to apply powerful graph algorithms to analyze the relationships and structures within your data that aren't apparent from just looking at the segmentation maps.</p> <p>In Histolytics, after you've segmented your WSI, you can create a spatial graph where:</p> <p>Nodes: Each segmented object (e.g., a cell or a nucleus) is a node in the graph. These nodes have spatial attributes like their coordinates and extracted features (e.g., area, shape, or marker intensity).</p> <p>Histolytics provides a <code>fit_graph</code> function which can be used to fit a spatial graph or a spatial weights to your input data. The actual fitting is done with the <code>libpysal</code> package and the <code>fit_graph</code>-function is basically a wrapper around different graph fitting methods with added functionality for example to threshold links that are too long. The allowed spatial weights are:</p> <ul> <li><code>knn</code>: k-nearest neighbors</li> <li><code>delaunay</code> - Delaunay triangulation</li> <li><code>distband</code> - Distance band i.e. a distance thresholded knn graph</li> <li><code>rel_nhood</code> - Relative neighborhood graph</li> <li><code>gabriel</code> - Gabriel graph</li> <li><code>voronoi</code> - Voronoi graph</li> </ul> <p>We will be using the <code>delaunay</code> method in this example. Here, we will set a distance threshold for the neighbors to be within 50 microns of the nuclei centroid. The distance unit in the example data is in pixels so 50 microns in pixels of 20x magnified segmentation mask is around 50*2 = 100 pixels.</p>"},{"location":"user_guide/spatial/graphs/#link-counts","title":"Link Counts\u00b6","text":"<p>One of the most straightforward yet powerful features you can extract from a spatial graph is the number of links of a certain type. This simple metric can provide valuable insights of the different cell types and their interactions. The cell-cell link counts are easily extracted from the <code>w_gdf</code> by using pandas:</p>"},{"location":"user_guide/spatial/graphs/#graph-features-with-networkx","title":"Graph Features with NetworkX\u00b6","text":"<p>We can compute a variety of graph features using the <code>networkX</code> library. Some common features include:</p> <ul> <li>Clustering Coefficient: Indicates the degree to which nodes tend to cluster together.</li> <li>Closeness Centrality: Measures the average length of the shortest path from a node to all other nodes in the graph.</li> <li>Degree Centrality: Measures the number of connections a node has to other nodes in the graph.</li> <li>Betweenness Centrality: Measures the extent to which a node lies on paths between other nodes.</li> <li>Eigenvector Centrality: Measures a node's influence based on the importance of its neighbors.</li> </ul> <p>In biological context, centrality scores can identify nuclei that are highly connected to their neighbors. A nucleus with a high degree centrality is a hub of some local cellular network which could indicate a site of intense biological activity. Some centrality measures, like betweenness centrality, identify nuclei that act as bridges or gatekeepers between different cellular regions. A nucleus with a high betweenness centrality lies on the shortest path between many other nuclei. Scores like eigenvector centrality not only count connections but also consider the \"importance\" of a nucleus's neighbors. A nucleus with high eigenvector centrality is connected to other highly central nuclei. This can identify cells that are part of a particularly influential network.</p>"},{"location":"user_guide/spatial/graphs/#benchmark","title":"Benchmark\u00b6","text":"<p>Different graph algorithms have differences in run-times and memory usage. It is important to consider these factors when choosing an algorithm for a specific task. In this section, we will benchmark the performance of different graph algorithms with a cervix dataset containing ~ 19K nuclei. In general all the different graph algorithms scale up to millions of cells but the run-times can go up to some minutes instead of seconds.</p>"},{"location":"user_guide/spatial/legendgram/","title":"Legendgrams","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom histolytics.data import cervix_nuclei, cervix_tissue\nfrom histolytics.spatial_ops.ops import get_interfaces\nfrom histolytics.spatial_ops.h3 import h3_grid\nfrom histolytics.spatial_agg import grid_aggregate\nfrom histolytics.utils.plot import legendgram\nfrom scipy.stats import rankdata\n\n\n# This function will compute the ratio of immune to connective cells within each grid cell\n# In general, any function that takes a GeoDataFrame and returns a scalar\n# can be used here. Typically, this will be a function that calculates\n# a count, sum, mean, or other statistic of interest out of the nuclei.\ndef immune_connective_ratio(nuclei):\n    \"\"\"Calculate the immune to connective cell ratio in a grid cell.\"\"\"\n    if \"inflammatory\" in nuclei.value_counts(\"class_name\"):\n        immune_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"inflammatory\"]\n    else:\n        immune_cnt = 0\n\n    if \"connective\" in nuclei.value_counts(\"class_name\"):\n        connective_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"connective\"]\n    else:\n        connective_cnt = 0\n\n    if connective_cnt &gt; 0:\n        return float(immune_cnt / connective_cnt)\n    else:\n        return 0\n\n\n# quantile normalization function\ndef qnorm(values: pd.Series):\n    \"\"\"Quantile normalize a pandas Series.\"\"\"\n    ranks = rankdata(values, method=\"average\")\n    quantile_normalized = (ranks - 1) / (len(ranks) - 1)\n    return quantile_normalized\n\n\n# Load data\ntis = cervix_tissue()\nnuc = cervix_nuclei()\n\n# get the stroma and lesion (cin) tissue segmentations\nstroma = tis[tis[\"class_name\"] == \"stroma\"]\ncin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n\n# get the lesion-stroma-interface\ninterface = get_interfaces(cin_tissue, stroma, buffer_dist=300)\ninterface = interface.assign(class_name=\"lesion-stroma-interface\")\n\n# fit a hexagonal grid to the interface\nh3_res10_inter = h3_grid(interface, resolution=10)\n# fit a hexagonal grid to the stroma\nh3_res10_stroma = h3_grid(stroma, resolution=10)\n\n# immune-connective-ratio at the interface\nh3_res10_inter = grid_aggregate(\n    objs=nuc,\n    grid=h3_res10_inter,\n    metric_func=immune_connective_ratio,\n    new_col_names=[\"immune_connective_ratio\"],\n    predicate=\"intersects\",\n    num_processes=2,\n)\n\n# immune-connective-ratio at the stroma\nh3_res10_stroma = grid_aggregate(\n    objs=nuc,\n    grid=h3_res10_stroma,\n    metric_func=immune_connective_ratio,\n    new_col_names=[\"immune_connective_ratio\"],\n    predicate=\"intersects\",\n    num_processes=2,\n)\n\nh3_res10_inter = h3_res10_inter.assign(\n    immune_connective_ratio_qnorm=qnorm(h3_res10_inter[\"immune_connective_ratio\"])\n)\nh3_res10_stroma = h3_res10_stroma.assign(\n    immune_connective_ratio_qnorm=qnorm(h3_res10_stroma[\"immune_connective_ratio\"])\n)\n</pre> import pandas as pd from histolytics.data import cervix_nuclei, cervix_tissue from histolytics.spatial_ops.ops import get_interfaces from histolytics.spatial_ops.h3 import h3_grid from histolytics.spatial_agg import grid_aggregate from histolytics.utils.plot import legendgram from scipy.stats import rankdata   # This function will compute the ratio of immune to connective cells within each grid cell # In general, any function that takes a GeoDataFrame and returns a scalar # can be used here. Typically, this will be a function that calculates # a count, sum, mean, or other statistic of interest out of the nuclei. def immune_connective_ratio(nuclei):     \"\"\"Calculate the immune to connective cell ratio in a grid cell.\"\"\"     if \"inflammatory\" in nuclei.value_counts(\"class_name\"):         immune_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"inflammatory\"]     else:         immune_cnt = 0      if \"connective\" in nuclei.value_counts(\"class_name\"):         connective_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"connective\"]     else:         connective_cnt = 0      if connective_cnt &gt; 0:         return float(immune_cnt / connective_cnt)     else:         return 0   # quantile normalization function def qnorm(values: pd.Series):     \"\"\"Quantile normalize a pandas Series.\"\"\"     ranks = rankdata(values, method=\"average\")     quantile_normalized = (ranks - 1) / (len(ranks) - 1)     return quantile_normalized   # Load data tis = cervix_tissue() nuc = cervix_nuclei()  # get the stroma and lesion (cin) tissue segmentations stroma = tis[tis[\"class_name\"] == \"stroma\"] cin_tissue = tis[tis[\"class_name\"] == \"cin\"]  # get the lesion-stroma-interface interface = get_interfaces(cin_tissue, stroma, buffer_dist=300) interface = interface.assign(class_name=\"lesion-stroma-interface\")  # fit a hexagonal grid to the interface h3_res10_inter = h3_grid(interface, resolution=10) # fit a hexagonal grid to the stroma h3_res10_stroma = h3_grid(stroma, resolution=10)  # immune-connective-ratio at the interface h3_res10_inter = grid_aggregate(     objs=nuc,     grid=h3_res10_inter,     metric_func=immune_connective_ratio,     new_col_names=[\"immune_connective_ratio\"],     predicate=\"intersects\",     num_processes=2, )  # immune-connective-ratio at the stroma h3_res10_stroma = grid_aggregate(     objs=nuc,     grid=h3_res10_stroma,     metric_func=immune_connective_ratio,     new_col_names=[\"immune_connective_ratio\"],     predicate=\"intersects\",     num_processes=2, )  h3_res10_inter = h3_res10_inter.assign(     immune_connective_ratio_qnorm=qnorm(h3_res10_inter[\"immune_connective_ratio\"]) ) h3_res10_stroma = h3_res10_stroma.assign(     immune_connective_ratio_qnorm=qnorm(h3_res10_stroma[\"immune_connective_ratio\"]) ) In\u00a0[2]: Copied! <pre>ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nh3_res10_inter.plot(\n    ax=ax,\n    column=\"immune_connective_ratio_qnorm\",\n    cmap=\"Reds\",\n    legend=True,\n    aspect=1,\n    facecolor=\"none\",\n)\nax.set_axis_off()\n\n# Add a legendgram to visualize the distribution of immune cell density\nax = legendgram(\n    gdf=h3_res10_inter,\n    column=\"immune_connective_ratio_qnorm\",\n    n_bins=30,\n    cmap=\"Reds\",\n    ax=ax,\n)\n</pre> ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) h3_res10_inter.plot(     ax=ax,     column=\"immune_connective_ratio_qnorm\",     cmap=\"Reds\",     legend=True,     aspect=1,     facecolor=\"none\", ) ax.set_axis_off()  # Add a legendgram to visualize the distribution of immune cell density ax = legendgram(     gdf=h3_res10_inter,     column=\"immune_connective_ratio_qnorm\",     n_bins=30,     cmap=\"Reds\",     ax=ax, ) <p>We see relatively uniform distribution of immune-connective cell ratios across the lesion-stroma-interface</p> In\u00a0[3]: Copied! <pre># We will quantile normalize the result to smooth out extremes\nh3_res10_stroma = h3_res10_stroma.assign(\n    immune_connective_ratio_qnorm=qnorm(h3_res10_stroma[\"immune_connective_ratio\"])\n)\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nh3_res10_stroma.plot(\n    ax=ax,\n    column=\"immune_connective_ratio_qnorm\",\n    cmap=\"Reds\",\n    legend=True,\n    aspect=1,\n    facecolor=\"none\",\n)\nax.set_axis_off()\n\n# Add a legendgram to visualize the distribution of immune cell density\nax = legendgram(\n    gdf=h3_res10_stroma,\n    column=\"immune_connective_ratio_qnorm\",\n    n_bins=30,\n    cmap=\"Reds\",\n    ax=ax,\n)\n</pre> # We will quantile normalize the result to smooth out extremes h3_res10_stroma = h3_res10_stroma.assign(     immune_connective_ratio_qnorm=qnorm(h3_res10_stroma[\"immune_connective_ratio\"]) )  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) h3_res10_stroma.plot(     ax=ax,     column=\"immune_connective_ratio_qnorm\",     cmap=\"Reds\",     legend=True,     aspect=1,     facecolor=\"none\", ) ax.set_axis_off()  # Add a legendgram to visualize the distribution of immune cell density ax = legendgram(     gdf=h3_res10_stroma,     column=\"immune_connective_ratio_qnorm\",     n_bins=30,     cmap=\"Reds\",     ax=ax, ) <p>The immune-connective cell ratio distribution at the stroma shows a more pronounced variation, where the distal stromal regions exhibit very low ratios indicating higher proportions of connective cells. The grid cells closer to the lesion show a more uniform distribution of immune-connective cell ratios.</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\nax = legendgram(\n    gdf=h3_res10_stroma,\n    column=\"immune_connective_ratio_qnorm\",\n    n_bins=30,\n    cmap=\"turbo\",\n)\nax\n</pre> import matplotlib.pyplot as plt  ax = legendgram(     gdf=h3_res10_stroma,     column=\"immune_connective_ratio_qnorm\",     n_bins=30,     cmap=\"turbo\", ) ax Out[6]: <pre>&lt;Axes: xlabel='Immune_Connective_Ratio_Qnorm'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/legendgram/#introduction","title":"Introduction\u00b6","text":"<p>What are Legendgrams? \ud83d\udcca A legendgram is a data visualization tool that combines a color legend with a histogram. Instead of a simple color bar showing a range of values, a legendgram adds a histogram to the side of the image. This visualization instantly shows you how your data is distributed across the different colors on your map.</p> <p>Imagine you have a spatial map showing the density of a certain cell type, with a color legend ranging from white (low density) to red (high density). A traditional legend tells you what colors represent what values, but it doesn't tell you how many areas on your map fall into each color range. A legendgram, however, would have bars on its side, immediately showing you if most of your cells are in the low-density white range, the high-density red range, or somewhere in between.</p> <p>In the last tutorial we computed grid aggregates of histological features within defined tissue regions. In this tutorial, we will add a legendgram to the visualization to learn the distribution of our custom metric at different parts of the tissue. More specifically, we will compare the immune-connective cell ratio distributions between the lesion-stroma interface and the stroma.</p>"},{"location":"user_guide/spatial/legendgram/#immune-connective-cell-ratio-distribution-at-the-lesion-stroma-interface","title":"Immune-Connective Cell Ratio Distribution at the Lesion-stroma Interface\u00b6","text":""},{"location":"user_guide/spatial/legendgram/#immune-connective-cell-ratio-distribution-at-the-stroma","title":"Immune-Connective Cell Ratio Distribution at the Stroma\u00b6","text":""},{"location":"user_guide/spatial/legendgram/#extra","title":"Extra\u00b6","text":"<p>If you don't want to add the legendgram to the Axes object and you want it as a separate figure, you can do so by just calling the <code>legendgram</code> function without specifying an <code>ax</code> parameter. Different matplotlib colormaps can be used too (e.g. viridis, plasma, inferno, turbo etc...).</p>"},{"location":"user_guide/spatial/medial_lines/","title":"Medial Lines","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom histolytics.data import cervix_tissue\n\ntis = cervix_tissue()\n\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nax.set_axis_off()\n</pre> import matplotlib.pyplot as plt from histolytics.data import cervix_tissue  tis = cervix_tissue()  fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True) ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.spatial_geom.medial_lines import medial_lines\n\ncin = tis[tis[\"class_name\"] == \"cin\"]\ncin_medial = medial_lines(\n    cin, num_points=500, simplify_level=50, parallel=True, num_processes=4\n)\n\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True)\ncin_medial.plot(ax=ax, color=\"red\", aspect=1)\nax.set_axis_off()\ncin_medial\n</pre> from histolytics.spatial_geom.medial_lines import medial_lines  cin = tis[tis[\"class_name\"] == \"cin\"] cin_medial = medial_lines(     cin, num_points=500, simplify_level=50, parallel=True, num_processes=4 )  fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True) cin_medial.plot(ax=ax, color=\"red\", aspect=1) ax.set_axis_off() cin_medial Out[2]: geometry class_name 5 MULTILINESTRING ((3389.66038 10157.23114, 3389... medial 6 MULTILINESTRING ((2001.05171 4628.92169, 2000.... medial 46 MULTILINESTRING ((3399.78589 1054.58574, 3341.... medial 48 MULTILINESTRING ((2002.72995 4066.36095, 2008.... medial <p>Medial line differs from the length property of the input polygons in that it represents the central axis of the polygons, rather than its overall perimeter length. For example, in cervical lesion biopsies, the lesions can be very elongated and irregularly shaped, making the medial line length more robust measure of the lesion's extent.</p> In\u00a0[3]: Copied! <pre>cin_medial.length, cin.length\n</pre> cin_medial.length, cin.length Out[3]: <pre>(5       379.700473\n 6       294.584983\n 46    32548.994078\n 48     1811.082166\n dtype: float64,\n 5       746.623794\n 6       488.193192\n 46    56703.570434\n 48     2334.443902\n dtype: float64)</pre> In\u00a0[4]: Copied! <pre>from histolytics.spatial_geom.line_metrics import line_metric\n\nmetrics = [\"length\", \"tortuosity\", \"average_turning_angle\"]\n\ncin_medial = line_metric(cin_medial, metrics=metrics, normalize=False)\n\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True)\ncin_medial.plot(ax=ax, cmap=\"viridis\", column=\"average_turning_angle\")\nax.set_axis_off()\ncin_medial\n</pre> from histolytics.spatial_geom.line_metrics import line_metric  metrics = [\"length\", \"tortuosity\", \"average_turning_angle\"]  cin_medial = line_metric(cin_medial, metrics=metrics, normalize=False)  fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True) cin_medial.plot(ax=ax, cmap=\"viridis\", column=\"average_turning_angle\") ax.set_axis_off() cin_medial Out[4]: geometry class_name length tortuosity average_turning_angle 5 MULTILINESTRING ((3389.66038 10157.23114, 3389... medial 379.700473 1.452964 2.777942 6 MULTILINESTRING ((2001.05171 4628.92169, 2000.... medial 294.584983 1.434600 1.856054 46 MULTILINESTRING ((3399.78589 1054.58574, 3341.... medial 32548.994078 4.449887 29.848962 48 MULTILINESTRING ((2002.72995 4066.36095, 2008.... medial 1811.082166 3.058787 2.799758 In\u00a0[5]: Copied! <pre>from histolytics.spatial_ops.ops import get_interfaces\n\nstroma = tis[tis[\"class_name\"] == \"stroma\"]\ninvasive_margin = get_interfaces(stroma, cin, buffer_dist=25, symmetric_buffer=False)\nim_medial = medial_lines(invasive_margin, simplify_level=1, num_points=1000)\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nax = invasive_margin.plot(ax=ax, color=\"blue\", lw=1, alpha=0.3)\nax = im_medial.plot(ax=ax, color=\"red\", aspect=1, lw=0.8)\nax.set_axis_off()\n</pre> from histolytics.spatial_ops.ops import get_interfaces  stroma = tis[tis[\"class_name\"] == \"stroma\"] invasive_margin = get_interfaces(stroma, cin, buffer_dist=25, symmetric_buffer=False) im_medial = medial_lines(invasive_margin, simplify_level=1, num_points=1000)  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True) ax = invasive_margin.plot(ax=ax, color=\"blue\", lw=1, alpha=0.3) ax = im_medial.plot(ax=ax, color=\"red\", aspect=1, lw=0.8) ax.set_axis_off() <p>The medial line algorithm is best suited for long contiguous polygons as it may fail for circular connected polygons as seen in the example above. If this happens, and you just need the length of the medial line, often the get_interfaces result polygons can be used as a proxy when a small enough buffer_dist is applied. The length of the and the length of the perimeter of such polygons are heavily correlated.</p> <p>If you're satisfied with the resulting medial lines of your IM, you can use them for further line analysis as showcased in the previous example but we'll leave that for another time.</p> In\u00a0[6]: Copied! <pre>import numpy as np\n\n# check correlation of medial line and invasive margin lengths\nim_medial.length, invasive_margin.length\nnp.corrcoef(im_medial.length, invasive_margin.length)\n</pre> import numpy as np  # check correlation of medial line and invasive margin lengths im_medial.length, invasive_margin.length np.corrcoef(im_medial.length, invasive_margin.length) Out[6]: <pre>array([[1.        , 0.98946081],\n       [0.98946081, 1.        ]])</pre> In\u00a0[7]: Copied! <pre>%%timeit\nmedial_lines(invasive_margin, simplify_level=1, num_points=1000)\n</pre> %%timeit medial_lines(invasive_margin, simplify_level=1, num_points=1000) <pre>488 ms \u00b1 17.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[8]: Copied! <pre>%%timeit\nmedial_lines(invasive_margin, simplify_level=50, num_points=1000)\n</pre> %%timeit medial_lines(invasive_margin, simplify_level=50, num_points=1000) <pre>231 ms \u00b1 5.83 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[9]: Copied! <pre>%%timeit\nmedial_lines(invasive_margin, simplify_level=50, num_points=500)\n</pre> %%timeit medial_lines(invasive_margin, simplify_level=50, num_points=500) <pre>117 ms \u00b1 1.23 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/medial_lines/#introduction","title":"Introduction\u00b6","text":"<p>What are Medial Lines? \ud83d\udcd0 A medial line (also known as a skeleton or medial axis) is a line that runs down the \"middle\" of a shape. Imagine a fire starting at every point on the boundary of a shape and burning inward at the same speed. The medial line is the set of points where the fire from different parts of the boundary meets. It perfectly represents the central core of a shape and is equidistant from the shape's boundaries at all points.</p> <p>How Medial Lines are Useful in Histology? \ud83d\udd2c Medial lines are a powerful tool for measuring and characterizing the extent and morphology of complex shapes in histology. They can be used to:</p> <p>Measure Tumor Extent: Medial lines provide a precise way to calculate the length or span of irregularly shaped tumors or other tissue structures. Unlike simple bounding boxes, a medial line follows the true contours of the object, giving a more accurate measure of its longitudinal extent.</p> <p>Analyze Tumor Invasive Margins: The length of the medial line at the interface between a tumor and healthy tissue can be used to quantify the extent of the invasive margin. A longer, more complex medial line at this boundary could indicate a more aggressive, invasive tumor with a convoluted growth pattern.</p> <p>Characterize Morphological Features: Medial lines can also be used to simplify the shape of convoluted structures, allowing for the calculation of features like tortuosity or branching complexity. This helps in objectively describing features that are difficult to quantify with standard metrics.</p> <p>In this tutorial, we will explore the concept of medial lines in more detail and demonstrate their application in histological analysis. We will use the cervix biopsy dataset as our example data.</p>"},{"location":"user_guide/spatial/medial_lines/#computing-medial-line","title":"Computing Medial Line\u00b6","text":"<p>Next, we'll compute the medial lines with the <code>medial_lines</code> function for the CIN lesion segmentation geometries. Parameters that should be taken into consideration when computing the medial lines include the simplification level and the number of points used for resampling the input polygons. The more there are points and the lower the simplification level, the more accurate the medial line will be, but this also increases computation time. Also, in some cases when there are too many resampled points, the resulting medial lines can be overly complex which results in breaks in lines that should be contiguous. You should play around with these parameters to find the best trade-off between accuracy and performance for your specific use case.</p>"},{"location":"user_guide/spatial/medial_lines/#line-metrics-of-the-medial-line","title":"Line Metrics of the Medial Line\u00b6","text":"<p>Similar to the stromal collagen features that were extracted in a previous tutorial , we can compute various line metrics for the medial line, such as tortuosity, average turning angle orientation. These measures can help quantify the complexity and shape of the lesion.</p>"},{"location":"user_guide/spatial/medial_lines/#length-of-tumor-invasive-margin","title":"Length of Tumor Invasive Margin\u00b6","text":"<p>Although the tumor invasive margin (IM) is not a concept in cervical lesions, it is an important factor in other types of cancers. The invasive margin often refers to the area at the edge of a tumor and stromal tissue. We'll showcase with the cervical lesion dataset, how the length of the IM would be computed using the <code>medial_lines</code> function and <code>get_interfaces</code> function that was introduced earlier in the partitioning tutorial.</p>"},{"location":"user_guide/spatial/medial_lines/#benchmark","title":"Benchmark\u00b6","text":"<p>The run-times of medial line computations depend on the input parameters such as the number of resampled points in the input polygons and the simplification level applied. Higher numbers of points and lower simplification levels will generally lead to longer computation times. However, in general, the run-times are quite reasonable and should not pose a significant bottleneck in most applications:</p>"},{"location":"user_guide/spatial/nhoods/","title":"Neighborhood Features","text":"In\u00a0[1]: Copied! <pre>from histolytics.spatial_graph.graph import fit_graph\nfrom histolytics.utils.gdf import set_uid\nfrom histolytics.data import hgsc_cancer_nuclei\n\nnuc = hgsc_cancer_nuclei()\nnuc = set_uid(nuc)  # Ensure the GeoDataFrame has a unique identifier\n\n# fit spatial weights using Delaunay triangulation\nw, w_gdf = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100)\n\nax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1)\nw_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1)\nax.set_axis_off()\nw_gdf.head(5)\n</pre> from histolytics.spatial_graph.graph import fit_graph from histolytics.utils.gdf import set_uid from histolytics.data import hgsc_cancer_nuclei  nuc = hgsc_cancer_nuclei() nuc = set_uid(nuc)  # Ensure the GeoDataFrame has a unique identifier  # fit spatial weights using Delaunay triangulation w, w_gdf = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100)  ax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1) w_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1) ax.set_axis_off() w_gdf.head(5) Out[1]: index focal neighbor weight focal_centroid_x focal_centroid_y neighbor_centroid_x neighbor_centroid_y focal_class_name neighbor_class_name class_name geometry 0 0 0 1 1.0 1400.037980 1.692484 1386.458579 9.580762 connective connective connective-connective LINESTRING (1400.038 1.692, 1386.459 9.581) 1 1 0 4 1.0 1400.037980 1.692484 1306.059654 2.527988 connective connective connective-connective LINESTRING (1400.038 1.692, 1306.06 2.528) 2 6 1 4 1.0 1386.458579 9.580762 1306.059654 2.527988 connective connective connective-connective LINESTRING (1386.459 9.581, 1306.06 2.528) 3 9 2 3 1.0 1378.296689 170.695478 1318.355274 178.923534 connective connective connective-connective LINESTRING (1378.297 170.695, 1318.355 178.924) 4 13 2 23 1.0 1378.296689 170.695478 1367.454971 219.690997 connective connective connective-connective LINESTRING (1378.297 170.695, 1367.455 219.691) In\u00a0[2]: Copied! <pre>from histolytics.spatial_agg.local_values import local_vals\nfrom libpysal.weights import w_subset\n\n# First we need to wrangle the graph data a bit so that we focus only on the nuclei\n# that are neoplastic or have neoplastic nuclei in their neighborhood.\n\n# get the indices of nuclei that have neoplastic-to-other link\nneo_link_inds = w_gdf[w_gdf[\"class_name\"].str.contains(\"neoplastic\", na=False)][\n    \"focal\"\n].unique()\n\n# Subset the spatial weights object with the indices that contain neoplastic-to-other links\nw_neo = w_subset(w, neo_link_inds)\n\n# Now we extract the neighborhood cell type classes!\nnuc = local_vals(\n    nuc, w_neo, val_col=\"class_name\", new_col_name=\"nhood_classes\", id_col=\"uid\"\n)\n\n# select all the neoplastic nuclei + the nuclei that are in their neighborhood\nnuc_of_interest = nuc[nuc[\"nhood_classes\"].notna()]\nnuc_of_interest.head(5)\n</pre> from histolytics.spatial_agg.local_values import local_vals from libpysal.weights import w_subset  # First we need to wrangle the graph data a bit so that we focus only on the nuclei # that are neoplastic or have neoplastic nuclei in their neighborhood.  # get the indices of nuclei that have neoplastic-to-other link neo_link_inds = w_gdf[w_gdf[\"class_name\"].str.contains(\"neoplastic\", na=False)][     \"focal\" ].unique()  # Subset the spatial weights object with the indices that contain neoplastic-to-other links w_neo = w_subset(w, neo_link_inds)  # Now we extract the neighborhood cell type classes! nuc = local_vals(     nuc, w_neo, val_col=\"class_name\", new_col_name=\"nhood_classes\", id_col=\"uid\" )  # select all the neoplastic nuclei + the nuclei that are in their neighborhood nuc_of_interest = nuc[nuc[\"nhood_classes\"].notna()] nuc_of_interest.head(5) Out[2]: geometry class_name uid nhood_classes uid 92 POLYGON ((1184.75 1008, 1180 1008.01, 1178.01 ... connective 92 [connective, inflammatory, neoplastic, connect... 95 POLYGON ((1171 1067.02, 1170.01 1068, 1169.01 ... connective 95 [connective, connective, connective, neoplasti... 108 POLYGON ((1178 1113.01, 1175.01 1115.01, 1172.... connective 108 [connective, connective, neoplastic, neoplasti... 109 POLYGON ((1226 1174.02, 1224.01 1176.01, 1222.... connective 109 [connective, neoplastic, connective, connective] 111 POLYGON ((1208 1211.01, 1203.83 1211.17, 1201.... connective 111 [connective, connective, connective, neoplasti... In\u00a0[3]: Copied! <pre>from histolytics.spatial_agg.local_character import local_character\n\n# Let's compute the areas of the nuclei of interest\nnuc_of_interest = nuc_of_interest.assign(area=nuc_of_interest.area)\n\nnuc_of_interest = local_character(\n    nuc_of_interest, w_neo, val_cols=[\"area\"], id_col=\"uid\", reductions=(\"mean\", \"std\")\n)\nax = nuc_of_interest.plot(\n    column=\"area_nhood_mean\", figsize=(10, 10), aspect=1, legend=True\n)\nax.set_axis_off()\nnuc_of_interest.head(5)\n</pre> from histolytics.spatial_agg.local_character import local_character  # Let's compute the areas of the nuclei of interest nuc_of_interest = nuc_of_interest.assign(area=nuc_of_interest.area)  nuc_of_interest = local_character(     nuc_of_interest, w_neo, val_cols=[\"area\"], id_col=\"uid\", reductions=(\"mean\", \"std\") ) ax = nuc_of_interest.plot(     column=\"area_nhood_mean\", figsize=(10, 10), aspect=1, legend=True ) ax.set_axis_off() nuc_of_interest.head(5) Out[3]: geometry class_name uid nhood_classes area area_nhood_mean area_nhood_std uid 92 POLYGON ((1184.75 1008, 1180 1008.01, 1178.01 ... connective 92 [connective, inflammatory, neoplastic, connect... 196.21525 309.868025 195.876801 95 POLYGON ((1171 1067.02, 1170.01 1068, 1169.01 ... connective 95 [connective, connective, connective, neoplasti... 90.93045 345.139360 183.091297 108 POLYGON ((1178 1113.01, 1175.01 1115.01, 1172.... connective 108 [connective, connective, neoplastic, neoplasti... 415.43035 478.860357 304.110999 109 POLYGON ((1226 1174.02, 1224.01 1176.01, 1222.... connective 109 [connective, neoplastic, connective, connective] 344.17040 488.513488 124.202886 111 POLYGON ((1208 1211.01, 1203.83 1211.17, 1201.... connective 111 [connective, connective, connective, neoplasti... 674.33780 522.122500 244.703536 In\u00a0[4]: Copied! <pre>from histolytics.spatial_agg.local_distances import local_distances\n\n\n# Compute local neighborhood distances\nnuc_of_interest = local_distances(\n    nuc_of_interest,\n    w_neo,\n    id_col=\"uid\",\n    reductions=[\"mean\"],\n    num_processes=6,\n    normalize=True,  # we will quantile normalize for the sake of the visualization\n)\n\n\nax = nuc_of_interest.plot(\n    column=\"nhood_dists_mean\", figsize=(10, 10), aspect=1, legend=True, cmap=\"Reds_r\"\n)\nax.set_axis_off()\nnuc_of_interest.head(5)\n</pre> from histolytics.spatial_agg.local_distances import local_distances   # Compute local neighborhood distances nuc_of_interest = local_distances(     nuc_of_interest,     w_neo,     id_col=\"uid\",     reductions=[\"mean\"],     num_processes=6,     normalize=True,  # we will quantile normalize for the sake of the visualization )   ax = nuc_of_interest.plot(     column=\"nhood_dists_mean\", figsize=(10, 10), aspect=1, legend=True, cmap=\"Reds_r\" ) ax.set_axis_off() nuc_of_interest.head(5) Out[4]: geometry class_name uid nhood_classes area area_nhood_mean area_nhood_std nhood_dists_mean uid 92 POLYGON ((1184.75 1008, 1180 1008.01, 1178.01 ... connective 92 [connective, inflammatory, neoplastic, connect... 196.21525 309.868025 195.876801 0.750656 95 POLYGON ((1171 1067.02, 1170.01 1068, 1169.01 ... connective 95 [connective, connective, connective, neoplasti... 90.93045 345.139360 183.091297 0.296588 108 POLYGON ((1178 1113.01, 1175.01 1115.01, 1172.... connective 108 [connective, connective, neoplastic, neoplasti... 415.43035 478.860357 304.110999 0.853018 109 POLYGON ((1226 1174.02, 1224.01 1176.01, 1222.... connective 109 [connective, neoplastic, connective, connective] 344.17040 488.513488 124.202886 0.860892 111 POLYGON ((1208 1211.01, 1203.83 1211.17, 1201.... connective 111 [connective, connective, connective, neoplasti... 674.33780 522.122500 244.703536 0.897638 In\u00a0[5]: Copied! <pre>from histolytics.spatial_agg.local_diversity import local_diversity\n\n# Compute local cell type diversity with shannon entropy\nnuc_of_interest = local_diversity(\n    nuc_of_interest,\n    w_neo,\n    id_col=\"uid\",\n    val_cols=[\"class_name\"],\n    metrics=[\"shannon_index\"],\n    num_processes=6,\n)\n\n# visualize nhood mean areas\nax = nuc_of_interest[nuc_of_interest[\"class_name\"] == \"neoplastic\"].plot(\n    column=\"class_name_shannon_index\",\n    aspect=1,\n    cmap=\"Reds\",\n    legend=True,\n    figsize=(10, 10),\n)\nax.set_axis_off()\n</pre> from histolytics.spatial_agg.local_diversity import local_diversity  # Compute local cell type diversity with shannon entropy nuc_of_interest = local_diversity(     nuc_of_interest,     w_neo,     id_col=\"uid\",     val_cols=[\"class_name\"],     metrics=[\"shannon_index\"],     num_processes=6, )  # visualize nhood mean areas ax = nuc_of_interest[nuc_of_interest[\"class_name\"] == \"neoplastic\"].plot(     column=\"class_name_shannon_index\",     aspect=1,     cmap=\"Reds\",     legend=True,     figsize=(10, 10), ) ax.set_axis_off() <p>As seen in the visualization, the cell-type heterogeneity is non-zero only at the borders of the tumor nest, where the neoplastic nuclei are linked with the surrounding nuclei in the stroma (e.g. inflammatory and connective nuclei).</p> In\u00a0[6]: Copied! <pre>from histolytics.data import cervix_nuclei\nfrom histolytics.utils.gdf import set_uid\n\nnuc = cervix_nuclei()\nnuc = set_uid(nuc)\nnuc = nuc.assign(area=nuc.area)\n\nw, _ = fit_graph(nuc, method=\"delaunay\", id_col=\"uid\", threshold=50)\nnuc\n</pre> from histolytics.data import cervix_nuclei from histolytics.utils.gdf import set_uid  nuc = cervix_nuclei() nuc = set_uid(nuc) nuc = nuc.assign(area=nuc.area)  w, _ = fit_graph(nuc, method=\"delaunay\", id_col=\"uid\", threshold=50) nuc Out[6]: geometry class_name uid area uid 0 POLYGON ((940.01 5570.02, 939.01 5573, 939 559... connective 0 429.58790 1 POLYGON ((906.01 5350.02, 906.01 5361, 908.01 ... connective 1 408.46570 2 POLYGON ((866 5137.02, 862.77 5137.94, 860 513... squamous_epithel 2 369.49285 3 POLYGON ((932 4777.02, 928 4778.02, 922.81 478... glandular_epithel 3 339.88560 4 POLYGON ((904 5177.02, 902.01 5178.02, 901.01 ... connective 4 330.89425 ... ... ... ... ... 19192 POLYGON ((3015 2614.02, 3013.01 2616, 3013.01 ... inflammatory 19192 297.37560 19193 POLYGON ((3387 2618.02, 3384.01 2620.02, 3379.... connective 19193 293.85250 19194 POLYGON ((3655 2622.02, 3650.86 2623.75, 3647 ... inflammatory 19194 320.30200 19195 POLYGON ((3703 2627.02, 3699.9 2629.16, 3696.0... connective 19195 300.45640 19196 POLYGON ((3199 2623.02, 3194.64 2626.72, 3191.... inflammatory 19196 268.75830 <p>19197 rows \u00d7 4 columns</p> In\u00a0[7]: Copied! <pre>%%timeit\nlocal_character(\n    nuc, w, val_cols=[\"area\"], id_col=\"uid\", reductions=(\"mean\",), num_processes=3\n)\n</pre> %%timeit local_character(     nuc, w, val_cols=[\"area\"], id_col=\"uid\", reductions=(\"mean\",), num_processes=3 ) <pre>1.89 s \u00b1 26.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[8]: Copied! <pre>%%timeit\nlocal_character(\n    nuc, w, val_cols=[\"area\"], id_col=\"uid\", reductions=(\"mean\", \"std\"), num_processes=6\n)\n</pre> %%timeit local_character(     nuc, w, val_cols=[\"area\"], id_col=\"uid\", reductions=(\"mean\", \"std\"), num_processes=6 ) <pre>2.14 s \u00b1 33.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[9]: Copied! <pre>%%timeit\nlocal_distances(\n    nuc,\n    w,\n    id_col=\"uid\",\n    reductions=[\"mean\"],\n    num_processes=3,\n)\n</pre> %%timeit local_distances(     nuc,     w,     id_col=\"uid\",     reductions=[\"mean\"],     num_processes=3, ) <pre>3.53 s \u00b1 92.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[10]: Copied! <pre>%%timeit\nlocal_distances(\n    nuc,\n    w,\n    id_col=\"uid\",\n    reductions=[\"mean\", \"std\"],\n    num_processes=6,\n)\n</pre> %%timeit local_distances(     nuc,     w,     id_col=\"uid\",     reductions=[\"mean\", \"std\"],     num_processes=6, ) <pre>3.88 s \u00b1 112 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[11]: Copied! <pre>%%timeit\nlocal_diversity(\n    nuc,\n    w,\n    id_col=\"uid\",\n    val_cols=[\"class_name\"],\n    metrics=[\"simpson_index\"],\n    num_processes=3,\n)\n</pre> %%timeit local_diversity(     nuc,     w,     id_col=\"uid\",     val_cols=[\"class_name\"],     metrics=[\"simpson_index\"],     num_processes=3, ) <pre>4.09 s \u00b1 71.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[12]: Copied! <pre>%%timeit\nlocal_diversity(\n    nuc,\n    w,\n    id_col=\"uid\",\n    val_cols=[\"class_name\"],\n    metrics=[\"simpson_index\", \"shannon_index\"],\n    num_processes=6,\n)\n</pre> %%timeit local_diversity(     nuc,     w,     id_col=\"uid\",     val_cols=[\"class_name\"],     metrics=[\"simpson_index\", \"shannon_index\"],     num_processes=6, ) <pre>4.16 s \u00b1 41 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/nhoods/#introduction","title":"Introduction\u00b6","text":"<p>What are Graph-based Neighborhoods? \ud83d\udd78\ufe0f As you've learned from the last spatial graphs tutorial, a graph is the ideal data structure for defining and analyzing neighborhoods. In this context, a spatial graph is created where each nucleus is a node. The links between these nodes (the \"edges\") represent a spatial relationship, such as physical contact or proximity. This graph structure is what allows us to precisely define a nucleus's neighborhood as the collection of its directly linked neighbors.</p> <p>What Can We Measure in a Neighborhood? \ud83d\udd2c For every single-cell neighborhood, Histolytics can compute a variety of metrics that fall into three main categories:</p> <p>Summary characteristics: These are aggregate statistics that describe the overall properties of the neighborhood. For a specific nucleus, you can compute:</p> <ul> <li>The number of specific neighbors</li> <li>mean/std/median/min/max/etc of nuclear features of all the neighboring nuclei.</li> </ul> <p>Distances: These can be used to quantify spatial arrangements of nuclei (dense packing vs. loose packing)</p> <ul> <li>The mean/std/median/min/max/etc distance to all neighboring nuclei.</li> </ul> <p>Heterogeneity of neighboring features or cell types: These are measures of diversity within the neighborhood. Available metrics include:</p> <ul> <li>Shannon entropy</li> <li>Simpson's diversity index</li> <li>Gini coefficient</li> <li>Theil index</li> </ul> <p>In this tutorial, we will explore how to compute these neighborhood metrics. We will use a small HGSC dataset as our example.</p>"},{"location":"user_guide/spatial/nhoods/#extracting-neighborhood-values","title":"Extracting Neighborhood Values\u00b6","text":"<p>At first, before computing any specific metrics, we'll showcase how to extract the neighborhood values and what they look like. This might be useful if you have some custom logic that could be applied to the neighborhood values. You can extract these values using the <code>local_vals</code>-function. We use it here to extract the cell type classes of the neighboring nuclei for each neoplastic nucleus but, in general, you can use it to extract any neighboring features. The extracted neighborhood values will be stored neatly as a new column in the dataframe.</p>"},{"location":"user_guide/spatial/nhoods/#local-neighborhood-summary-characteristics","title":"Local Neighborhood Summary Characteristics\u00b6","text":"<p>Next, we'll showcase how to compute simple summary statistics for the local neighborhoods. To do this, we will first compute the areas of the nuclei of interest and then the neighborhood mean and standard deviation of these areas. These neighborhood characteristics will be stored as distinct columns in the input dataframe.</p> <p>This is a simple demonstration on quantifying a neighborhood feature at single-cell resolution. The features could be anything from gene expression levels to morphological characteristics if such single-cell-level data would exist.</p>"},{"location":"user_guide/spatial/nhoods/#local-neighborhood-distances","title":"Local Neighborhood Distances\u00b6","text":"<p>Another aspect of neighborhood analysis is the spatial arrangement of the nuclei. For example, The degree of packing density of nuclei which can be associated to various phenomena such as different tumor growth patterns. The packing density can be assessed by examining the distances in the nuclei neighborhoods.</p> <p>To find out how densely packed the neighboring nuclei are, we will compute the mean distances between each neoplastic nucleus and its neighbors. The shorter the mean distance, the denser the neighborhoods are. This shows in the visualization below where denser regions are highlighted in red.</p>"},{"location":"user_guide/spatial/nhoods/#local-heterogeneities","title":"Local Heterogeneities\u00b6","text":"<p>Next, we will demonstrate how to compute diversity indices for the local neighborhoods of neoplastic nuclei. These indices will help us quantify the compositional variability of the neighborhoods. In principle, the heterogeneity can be computed for any type of numerical or categorical feature. In this demo, we will compute the Shannon diversity index of the cell type categories for the each local neighborhood. The other diversity indices were \"simpson_index\", \"gini_index\" and \"theil_index\"</p>"},{"location":"user_guide/spatial/nhoods/#benchmark","title":"Benchmark\u00b6","text":"<p>In this section, we will benchmark the performance of different of the local neighborhood metric functions with a cervix dataset containing ~ 19K nuclei. The local neighborhood functions have varying run-times, depending on what and how many metrics are being computed. These functions are parallelizable, so they can be scaled according to the computational resources available. However, the memory usage can also increase significantly when larger datasets are used with many cores. In general, these functions scale to datasets with +1million segmented nuclei with reasonable run-times.</p>"},{"location":"user_guide/spatial/nhoods/#neighborhood-summary-characteristics","title":"Neighborhood Summary Characteristics\u00b6","text":""},{"location":"user_guide/spatial/nhoods/#neighborhood-distances","title":"Neighborhood Distances\u00b6","text":""},{"location":"user_guide/spatial/nhoods/#neighborhood-diversities","title":"Neighborhood Diversities\u00b6","text":""},{"location":"user_guide/spatial/nuclear_features/","title":"Nuclear Features","text":"In\u00a0[1]: Copied! <pre>from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n\n# Load some example data\nnuc = hgsc_cancer_nuclei()  # Nuclei vector segmentation\nhe = hgsc_cancer_he()  # Corresponding H&amp;E image\n\nax = nuc.plot(figsize=(10, 10), column=\"class_name\")\nax.set_axis_off()\nnuc.head(5)\n</pre> from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei  # Load some example data nuc = hgsc_cancer_nuclei()  # Nuclei vector segmentation he = hgsc_cancer_he()  # Corresponding H&amp;E image  ax = nuc.plot(figsize=(10, 10), column=\"class_name\") ax.set_axis_off() nuc.head(5) Out[1]: geometry class_name 0 POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ... connective 1 POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ... connective 2 POLYGON ((1382.99 156.01, 1380 156.01, 1376.01... connective 3 POLYGON ((1321 170.01, 1317.01 174.01, 1312.01... connective 4 POLYGON ((1297.01 0, 1299.01 2.99, 1302 5.99, ... connective In\u00a0[2]: Copied! <pre>from histolytics.spatial_geom.shape_metrics import shape_metric\n\nmetrics = [\n    \"major_axis_len\",\n    \"minor_axis_len\",\n    \"major_axis_angle\",\n    \"minor_axis_angle\",\n    \"compactness\",\n    \"circularity\",\n    \"convexity\",\n    \"solidity\",\n    \"elongation\",\n    \"eccentricity\",\n    \"fractal_dimension\",\n    \"shape_index\",\n    \"rectangularity\",\n    \"squareness\",\n    \"equivalent_rectangular_index\",\n    \"area\",\n    \"perimeter\",\n]\n\nnuc = shape_metric(nuc, metrics=metrics, num_processes=3)\nnuc.head(3)\n</pre> from histolytics.spatial_geom.shape_metrics import shape_metric  metrics = [     \"major_axis_len\",     \"minor_axis_len\",     \"major_axis_angle\",     \"minor_axis_angle\",     \"compactness\",     \"circularity\",     \"convexity\",     \"solidity\",     \"elongation\",     \"eccentricity\",     \"fractal_dimension\",     \"shape_index\",     \"rectangularity\",     \"squareness\",     \"equivalent_rectangular_index\",     \"area\",     \"perimeter\", ]  nuc = shape_metric(nuc, metrics=metrics, num_processes=3) nuc.head(3) Out[2]: geometry class_name area perimeter major_axis_len minor_axis_len major_axis_angle minor_axis_angle compactness circularity convexity solidity elongation eccentricity fractal_dimension shape_index rectangularity squareness equivalent_rectangular_index 0 POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ... connective 36.06900 27.986316 12.220000 3.990000 0.000000 90.000000 0.578699 0.578699 1.000000 1.000000 0.326514 9.451924e-01 1.085180 0.554563 0.739759 0.736822 0.742468 1 POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ... connective 182.46535 49.205771 14.980000 14.980000 0.000000 90.000000 0.947018 0.947018 1.000000 1.000000 1.000000 5.161914e-08 0.964059 0.856464 0.813124 1.205781 0.740496 2 POLYGON ((1382.99 156.01, 1380 156.01, 1376.01... connective 231.95650 69.421231 30.729437 9.590079 78.690068 11.309932 0.604828 0.608743 0.996779 0.972191 0.366122 9.500554e-01 1.047966 0.559090 0.787100 0.770091 0.763769 In\u00a0[3]: Copied! <pre>from histolytics.utils.raster import gdf2inst\nfrom histolytics.nuc_feats.intensity import (\n    rgb_intensity_feats,\n    grayscale_intensity_feats,\n)\n\n\n# convert the nuclei gdf into raster mask\ninst_mask = gdf2inst(nuc, width=he.shape[1], height=he.shape[0])\n\nmetrics = [\n    \"min\",\n    \"max\",\n    \"mean\",\n    \"median\",\n    \"std\",\n    \"quantiles\",\n    \"meanmediandiff\",\n    \"mad\",\n    \"iqr\",\n    \"skewness\",\n    \"kurtosis\",\n    \"histenergy\",\n]\n\ngr_intensity_feats = grayscale_intensity_feats(\n    he, inst_mask, metrics=metrics, device=\"cuda\"\n)\ngr_intensity_feats.head(3)\n</pre> from histolytics.utils.raster import gdf2inst from histolytics.nuc_feats.intensity import (     rgb_intensity_feats,     grayscale_intensity_feats, )   # convert the nuclei gdf into raster mask inst_mask = gdf2inst(nuc, width=he.shape[1], height=he.shape[0])  metrics = [     \"min\",     \"max\",     \"mean\",     \"median\",     \"std\",     \"quantiles\",     \"meanmediandiff\",     \"mad\",     \"iqr\",     \"skewness\",     \"kurtosis\",     \"histenergy\", ]  gr_intensity_feats = grayscale_intensity_feats(     he, inst_mask, metrics=metrics, device=\"cuda\" ) gr_intensity_feats.head(3) Out[3]: min max mean median std quantile_0.25 quantile_0.5 quantile_0.75 meanmediandiff mad iqr skewness kurtosis histenergy 1 0.032768 0.207166 0.088276 0.067679 0.046387 0.052129 0.067679 0.125847 0.020597 0.024256 0.073719 0.850163 -0.428480 0.262632 2 0.085619 0.317818 0.187696 0.177222 0.058691 0.138690 0.177222 0.237239 0.010474 0.044008 0.098549 0.352018 -0.973655 0.158222 3 0.119424 0.391479 0.207000 0.194400 0.055537 0.163815 0.194400 0.247490 0.012600 0.037783 0.083675 0.798506 0.408790 0.168847 In\u00a0[4]: Copied! <pre># compute the same features for every rgb channel separately\nrgb_intensity_feats = rgb_intensity_feats(he, inst_mask, metrics=metrics, device=\"cuda\")\nrgb_intensity_feats.head(3)\n</pre> # compute the same features for every rgb channel separately rgb_intensity_feats = rgb_intensity_feats(he, inst_mask, metrics=metrics, device=\"cuda\") rgb_intensity_feats.head(3) Out[4]: R_min R_max R_mean R_median R_std R_quantile_0.25 R_quantile_0.5 R_quantile_0.75 R_meanmediandiff R_mad ... B_std B_quantile_0.25 B_quantile_0.5 B_quantile_0.75 B_meanmediandiff B_mad B_iqr B_skewness B_kurtosis B_histenergy 1 0.081897 0.379310 0.199215 0.193966 0.071971 0.142241 0.193966 0.260776 0.005249 0.056034 ... 0.098219 0.297414 0.375000 0.448276 0.002288 0.077586 0.150862 0.134831 -0.777452 0.085671 2 0.232759 0.474138 0.350049 0.340517 0.063733 0.295259 0.340517 0.400862 0.009532 0.051724 ... 0.049941 0.525862 0.560345 0.599138 0.000703 0.038793 0.073276 0.013264 -0.852830 0.161521 3 0.288793 0.538793 0.368869 0.357759 0.051170 0.335129 0.357759 0.400862 0.011110 0.034483 ... 0.070186 0.491379 0.521552 0.575431 0.016387 0.038793 0.084052 0.918229 0.485612 0.136296 <p>3 rows \u00d7 42 columns</p> In\u00a0[5]: Copied! <pre>from histolytics.nuc_feats.chromatin import chromatin_feats\n\nmetrics = [\n    \"chrom_area\",\n    \"chrom_nuc_prop\",\n    \"n_chrom_clumps\",\n    \"chrom_boundary_prop\",\n    \"manders_coloc_coeff\",\n]\n\n# compute chromatin distribution features\nchrom_feats = chromatin_feats(he, inst_mask, metrics=metrics, device=\"cuda\")\nchrom_feats.head(3)\n</pre> from histolytics.nuc_feats.chromatin import chromatin_feats  metrics = [     \"chrom_area\",     \"chrom_nuc_prop\",     \"n_chrom_clumps\",     \"chrom_boundary_prop\",     \"manders_coloc_coeff\", ]  # compute chromatin distribution features chrom_feats = chromatin_feats(he, inst_mask, metrics=metrics, device=\"cuda\") chrom_feats.head(3) Out[5]: chrom_area chrom_nuc_prop n_chrom_clumps chrom_boundary_prop manders_coloc_coeff 1 139 0.776536 1.0 0.471545 0.657315 2 41 0.180617 1.0 0.016484 0.130406 3 5 0.043103 3.0 0.027273 0.030922 In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nfrom skimage.measure import label\nfrom histolytics.nuc_feats.chromatin import extract_chromatin_clumps\nfrom histolytics.utils.plot import draw_thing_contours\n\n# extract the chromatin clumps\nchrom_mask = extract_chromatin_clumps(he, inst_mask, device=\"cuda\")\n\n# Let's crop a region from the center to see the chromatin clumps better\nh, w = chrom_mask.shape\ncrop_size = 512\nstart_y = (h - crop_size) // 2\nstart_x = (w - crop_size) // 2\n\nchrom_mask_crop = chrom_mask[\n    start_y : start_y + crop_size, start_x : start_x + crop_size\n]\nhe_crop = he[start_y : start_y + crop_size, start_x : start_x + crop_size]\n\n# Let's plot the chromatin clumps\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\ncontours = draw_thing_contours(he_crop, label(chrom_mask_crop), chrom_mask_crop)\nax[0].imshow(he_crop)\nax[0].set_axis_off()\nax[1].imshow(contours)\nax[1].set_axis_off()\n</pre> import matplotlib.pyplot as plt from skimage.measure import label from histolytics.nuc_feats.chromatin import extract_chromatin_clumps from histolytics.utils.plot import draw_thing_contours  # extract the chromatin clumps chrom_mask = extract_chromatin_clumps(he, inst_mask, device=\"cuda\")  # Let's crop a region from the center to see the chromatin clumps better h, w = chrom_mask.shape crop_size = 512 start_y = (h - crop_size) // 2 start_x = (w - crop_size) // 2  chrom_mask_crop = chrom_mask[     start_y : start_y + crop_size, start_x : start_x + crop_size ] he_crop = he[start_y : start_y + crop_size, start_x : start_x + crop_size]  # Let's plot the chromatin clumps fig, ax = plt.subplots(1, 2, figsize=(16, 8)) contours = draw_thing_contours(he_crop, label(chrom_mask_crop), chrom_mask_crop) ax[0].imshow(he_crop) ax[0].set_axis_off() ax[1].imshow(contours) ax[1].set_axis_off() In\u00a0[7]: Copied! <pre>from histolytics.nuc_feats.texture import textural_feats\n\nmetrics = [\"contrast\", \"dissimilarity\"]\ndistances = (1,)\nangles = (0,)\n\ntex_feats = textural_feats(\n    he,\n    inst_mask,\n    distances=distances,\n    metrics=metrics,\n    angles=angles,\n    device=\"cuda\",\n)\n\ntex_feats.head(3)\n</pre> from histolytics.nuc_feats.texture import textural_feats  metrics = [\"contrast\", \"dissimilarity\"] distances = (1,) angles = (0,)  tex_feats = textural_feats(     he,     inst_mask,     distances=distances,     metrics=metrics,     angles=angles,     device=\"cuda\", )  tex_feats.head(3) Out[7]: contrast_d-1_a-0.00 dissimilarity_d-1_a-0.00 1 193.428571 6.628571 2 603.786667 11.600000 3 711.378788 11.934343"},{"location":"user_guide/spatial/nuclear_features/#introduction","title":"Introduction\u00b6","text":"<p>What are nuclear features? \ud83d\udd2c Nuclear features are quantitative measurements of individual nuclei that provide a deeper understanding of their biology and pathology. Histolytics organizes these features into four main categories: shape (morphometrics), intensity, chromatin distribution, and textural features. These features can be used to quantify nuclear pleomorphism, a term used in histopathology to describe a significant variation in the size, shape, and staining of nuclei.</p> <p>Shape (Morphometrics): These features describe a nucleus's physical form and dimensions. Analyzing these can help you identify changes in cell state, as a cell undergoing transformation or experiencing stress often changes its nuclear shape and size. These metrics are efficiently computed directly from the vectorized segmentation masks.</p> <p>Intensity Features: These metrics, based on the pixel values within a nucleus, are crucial for quantifying staining patterns which can indicate differences in a cell's state.</p> <p>Chromatin Distribution Features: This category quantifies the spatial organization of chromatin within nuclei, which is a key indicator of cellular state. A change in chromatin distribution often signals a shift in cell function or an impending disease process.</p> <p>Textural Features: These features describe the texture of the nuclear interior, capturing subtle patterns that are difficult for the human eye to detect.</p> <p>We will demonstrate how to compute these features, starting by computing the morphometric features:</p>"},{"location":"user_guide/spatial/nuclear_features/#morphometrics","title":"Morphometrics\u00b6","text":"<p>To compute the nuclear morphometrics, you can use the <code>shape_metric</code>-function. The allowed morphometrics to compute are: \"area\", \"perimeter\", \"major_axis_len\", \"minor_axis_len\", \"major_axis_angle\", \"minor_axis_angle\", \"compactness\", \"circularity\", \"convexity\", \"solidity\", \"elongation\", \"eccentricity\", \"fractal_dimension\", \"sphericity\", \"shape_index\", \"rectangularity\", \"squareness\", \"equivalent_rectangular_index\",</p> <p>In biological context, possibly the most interesting morphometrics are the area, eccentricity/elongation, compactness and in some cases the fractal dimension. These are good descriptors of shape pleomorphism that try to describe the variability in nuclear shape and size. Despite of this, the choice of features to compute will ultimately depend on the specific biological question being addressed. Since we are not addressing any specific question here, we'll compute all the possible morphometrics for demonstration purposes:</p>"},{"location":"user_guide/spatial/nuclear_features/#intensity-features","title":"Intensity Features\u00b6","text":"<p>While morphometric features can be computed directly from the vectorized segmentation maps, the rest of the feature categories (intensity, chromatin, and textural) require the underlying pixel data from the H&amp;E image.</p> <p>To compute these features you need to rasterize the input segmentation masks. Histolytics uses this raster mask to isolate the corresponding pixels from the H&amp;E image and computes the desired features on those isolated pixels.</p> <p>In this section, we will cover the intensity features that can be extracted from the H&amp;E image and corresponding raster label mask. These features are extracted with functions: <code>grayscale_intensity_feats</code> and <code>rgb_intensity_feats</code>. NOTE that you can use gpu acceleration to compute these features by providing parameter <code>device=\"cuda\"</code>.</p> <p>The allowed intensity metrics per nuclei are: \"min\", \"max\", \"mean\", \"median\", \"std\", \"quantiles\", \"meanmediandiff\", \"mad\", \"iqr\", \"skewness\", \"kurtosis\", \"histenergy\"</p> <p>In biological context, possibly the most interesting intensity features are just the mean/median and standard deviation of the intensity values within each nuclear region. These features can provide insights into the heterogeneity and overall intensity distribution within the nuclei. These features can be useful for example when quantifying the level of chromatism (hyper/hypo) that has direct implications for tumor biology. But again, the choice of features to compute will ultimately depend on the specific biological question being addressed. So for the sake of the example, we'll compute all the possible features:</p>"},{"location":"user_guide/spatial/nuclear_features/","title":"\u00b6","text":""},{"location":"user_guide/spatial/nuclear_features/#chromatin-distribution-features","title":"Chromatin Distribution Features\u00b6","text":"<p>Next up, we'll extract chromatin distribution features from the H&amp;E image and corresponding raster label mask. These are extracted with the <code>chromatin_feats</code>-function and can provide insights into the spatial organization and distribution of chromatin within the nuclei. For example, is the chromatin dispersed at the boundaries or covering the full nuclei.</p> <p>The allowed features are:</p> <ul> <li>\"chrom_area\" - The chromatin clump area</li> <li>\"chrom_nuc_prop\" - The proportion of chromatin area to full nucleus area</li> <li>\"n_chrom_clumps\" - The number of chromatin clumps</li> <li>\"chrom_boundary_prop\" - The proportion of chromatin clump area to the nucleus boundary area</li> <li>\"manders_coloc_coeff\" - The Manders' colocalization coefficient for the nucleus and chromatin clump intensity values</li> </ul> <p>In biological context, all these features can provide valuable information about the chromatin organization within the nuclei and may be relevant for understanding various biological processes. For example, detecting Orphan Annie eye nuclei (chromatin dispersed at the nucleus boundaries whereas rest of the nucleus is optically clear), or hyper/hyochromatism.</p>"},{"location":"user_guide/spatial/nuclear_features/#visualizing-chromatin-clump-segmentations","title":"Visualizing Chromatin Clump Segmentations\u00b6","text":"<p>Internally, the <code>chromatin_feats</code>-function segments the chromatin clumps within the nuclei using the <code>histolytics.nuc_feats.chromatin.extract_chromatin_clumps</code> function. To see what the chromatin clump segmentations look like, we can visualize them followingly:</p>"},{"location":"user_guide/spatial/nuclear_features/#textural-features","title":"Textural Features\u00b6","text":"<p>The textural features are based on the Gray Level Co-occurrence Matrix (GLCM), which is a statistical method of examining texture that considers the spatial relationship of pixels. The following textural features are extracted:</p> <ul> <li>Contrast: Measures the intensity contrast between a pixel and its neighbor over the whole image.</li> <li>Dissimilarity: Measures the variation in the gray level co-occurrence matrix.</li> <li>Homogeneity: Measures the closeness of the distribution of elements in the GLCM to the GLCM diagonal.</li> <li>Energy: Measures the uniformity of the GLCM.</li> <li>Entropy: Measures the amount of information that is missing from the image.</li> </ul> <p>In a biological context, these features can reveal textural patterns of the chromatin staining.</p> <p>For example, the contrast and dissimilarity can measure the coarseness and heterogeneity of the nuclear texture. High values can indicate a coarse, clumpy chromatin pattern.</p> <p>Homogeneity and energy can measure the uniformity of the nuclear texture. High values suggest a smooth and orderly chromatin distribution, while low values point to a more disordered or disorganized pattern.</p> <p>The entropy, on the other hand, quantifies the randomness and complexity of the nuclear texture. High entropy is strongly associated with chaotic chromatin patterns.</p> <p>The feature computation is based on the skimage library, you can see their documentation for more info about GLCM features.</p>"},{"location":"user_guide/spatial/partitioning/","title":"Spatial Partitioning","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom histolytics.data import cervix_nuclei, cervix_tissue\n\ntis = cervix_tissue()\nnuc = cervix_nuclei()\n\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False)\nax.set_axis_off()\n</pre> import matplotlib.pyplot as plt from histolytics.data import cervix_nuclei, cervix_tissue  tis = cervix_tissue() nuc = cervix_nuclei()  fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False) ax.set_axis_off() In\u00a0[\u00a0]: Copied! <pre>from histolytics.spatial_ops import get_interfaces\n\n# get the stroma and lesion (cin) tissue segmentations\nstroma = tis[tis[\"class_name\"] == \"stroma\"]\ncin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n\ninterface = get_interfaces(cin_tissue, stroma, buffer_dist=300)\ninterface = interface.assign(class_name=\"lesion-stroma-interface\")\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\ninterface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=True)\nax.set_axis_off()\ninterface.head()\n</pre> from histolytics.spatial_ops import get_interfaces  # get the stroma and lesion (cin) tissue segmentations stroma = tis[tis[\"class_name\"] == \"stroma\"] cin_tissue = tis[tis[\"class_name\"] == \"cin\"]  interface = get_interfaces(cin_tissue, stroma, buffer_dist=300) interface = interface.assign(class_name=\"lesion-stroma-interface\")  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) interface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=True) ax.set_axis_off() interface.head() Out[\u00a0]: class_name geometry 0 lesion-stroma-interface POLYGON ((1588.71 4829.03, 1591.18 4828.83, 15... 1 lesion-stroma-interface POLYGON ((743.07 5587.48, 743.63 5589, 744.07 ... 2 lesion-stroma-interface POLYGON ((1151 7566, 1150 7567, 1148.2 7568.2,... 3 lesion-stroma-interface POLYGON ((1891.19 8717.83, 1894.67 8716.8, 189... 4 lesion-stroma-interface POLYGON ((2226 8704.89, 2225.93 8702.52, 2224.... <p>Now it would be easy for example to query for cells residing in the interface region. For example, we could query for all the connective and inflammatory cells within the lesion-stroma interface and compute their ratio to see is the interface region immune or fibroblast hot (if above 1 there is more inflammation in the interface):</p> In\u00a0[3]: Copied! <pre>from histolytics.spatial_ops import get_objs\n\nnuc_within_interface = get_objs(interface, nuc, predicate=\"contains\")\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\ninterface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=False)\nnuc_within_interface.plot(\n    ax=ax,\n    column=\"class_name\",\n    aspect=1,\n    legend=True,\n)\nax.set_axis_off()\n\nclass_counts = nuc_within_interface[\"class_name\"].value_counts()\ninflammatory_count = class_counts.get(\"inflammatory\", 0)\nconnective_count = class_counts.get(\"connective\", 0)\n\ninflammatory_count / connective_count\n</pre> from histolytics.spatial_ops import get_objs  nuc_within_interface = get_objs(interface, nuc, predicate=\"contains\")  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) interface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=False) nuc_within_interface.plot(     ax=ax,     column=\"class_name\",     aspect=1,     legend=True, ) ax.set_axis_off()  class_counts = nuc_within_interface[\"class_name\"].value_counts() inflammatory_count = class_counts.get(\"inflammatory\", 0) connective_count = class_counts.get(\"connective\", 0)  inflammatory_count / connective_count Out[3]: <pre>2.471516213847502</pre> In\u00a0[4]: Copied! <pre>from histolytics.spatial_ops.h3 import h3_grid\n\nh3_res8 = h3_grid(stroma, resolution=8)\nh3_res9 = h3_grid(stroma, resolution=9)\nh3_res10 = h3_grid(stroma, resolution=10)\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nh3_res8.plot(ax=ax, aspect=1, legend=False, facecolor=\"none\", edgecolor=\"black\", lw=3)\nh3_res9.plot(ax=ax, aspect=1, legend=False, facecolor=\"none\", edgecolor=\"red\", lw=2)\nh3_res10.plot(\n    ax=ax, aspect=1, legend=False, facecolor=\"none\", edgecolor=\"blue\", lw=1, alpha=0.5\n)\nax.set_axis_off()\nh3_res9.head(5)\n</pre> from histolytics.spatial_ops.h3 import h3_grid  h3_res8 = h3_grid(stroma, resolution=8) h3_res9 = h3_grid(stroma, resolution=9) h3_res10 = h3_grid(stroma, resolution=10) ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True) h3_res8.plot(ax=ax, aspect=1, legend=False, facecolor=\"none\", edgecolor=\"black\", lw=3) h3_res9.plot(ax=ax, aspect=1, legend=False, facecolor=\"none\", edgecolor=\"red\", lw=2) h3_res10.plot(     ax=ax, aspect=1, legend=False, facecolor=\"none\", edgecolor=\"blue\", lw=1, alpha=0.5 ) ax.set_axis_off() h3_res9.head(5) Out[4]: geometry 8982a939503ffff POLYGON ((6672.79721 859.08743, 6647.90711 661... 8982a939877ffff POLYGON ((2556.61731 5658.46273, 2581.53692 58... 8982a939c4bffff POLYGON ((4546.44516 4059.58249, 4366.53531 39... 8982a93917bffff POLYGON ((6221.15607 3971.09325, 6041.22381 38... 8982a939a7bffff POLYGON ((4399.78122 8249.68626, 4219.8982 815... <p>We could also fit the grid to the interface region we just defined. This would allow us to analyze the spatial distribution of cell types at the interface between the lesion and stroma.</p> In\u00a0[5]: Copied! <pre>h3_res10_inter = h3_grid(interface, resolution=10)\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True)\ninterface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=False)\nh3_res10_inter.plot(\n    ax=ax, aspect=1, legend=True, facecolor=\"none\", edgecolor=\"blue\", lw=1, alpha=0.5\n)\nax.set_axis_off()\n</pre> h3_res10_inter = h3_grid(interface, resolution=10) ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True) interface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=False) h3_res10_inter.plot(     ax=ax, aspect=1, legend=True, facecolor=\"none\", edgecolor=\"blue\", lw=1, alpha=0.5 ) ax.set_axis_off() In\u00a0[6]: Copied! <pre>from histolytics.spatial_ops.rect_grid import rect_grid\n\nrect_grid_res256 = rect_grid(\n    cin_tissue, resolution=(256, 256), overlap=0, predicate=\"intersects\"\n)\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nrect_grid_res256.plot(\n    ax=ax, aspect=1, legend=True, facecolor=\"none\", edgecolor=\"black\", lw=2\n)\n\nax.set_axis_off()\nrect_grid_res256.head(5)\n</pre> from histolytics.spatial_ops.rect_grid import rect_grid  rect_grid_res256 = rect_grid(     cin_tissue, resolution=(256, 256), overlap=0, predicate=\"intersects\" )  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=True) rect_grid_res256.plot(     ax=ax, aspect=1, legend=True, facecolor=\"none\", edgecolor=\"black\", lw=2 )  ax.set_axis_off() rect_grid_res256.head(5) Out[6]: geometry 0 POLYGON ((2771 938, 3027 938, 3027 1194, 2771 ... 1 POLYGON ((3027 938, 3283 938, 3283 1194, 3027 ... 2 POLYGON ((3283 938, 3539 938, 3539 1194, 3283 ... 3 POLYGON ((3539 938, 3795 938, 3795 1194, 3539 ... 4 POLYGON ((2515 1194, 2771 1194, 2771 1450, 251... In\u00a0[\u00a0]: Copied! <pre>from histolytics.spatial_agg import grid_aggregate\nfrom scipy.stats import rankdata\n\n\n# This function will compute the ratio of immune to connective cells within each grid cell\n# In general, any function that takes a GeoDataFrame and returns a scalar\n# can be used here. Typically, this will be a function that calculates\n# a count, sum, mean, or other statistic of interest out of the nuclei.\ndef immune_connective_ratio(nuclei):\n    \"\"\"Calculate the immune to connective cell ratio in a grid cell.\"\"\"\n    if \"inflammatory\" in nuclei.value_counts(\"class_name\"):\n        immune_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"inflammatory\"]\n    else:\n        immune_cnt = 0\n\n    if \"connective\" in nuclei.value_counts(\"class_name\"):\n        connective_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"connective\"]\n    else:\n        connective_cnt = 0\n\n    if connective_cnt &gt; 0:\n        return float(immune_cnt / connective_cnt)\n    else:\n        return 0\n\n\nh3_res10_inter = grid_aggregate(\n    objs=nuc,\n    grid=h3_res10_inter,\n    metric_func=immune_connective_ratio,\n    new_col_names=[\"immune_connective_ratio\"],\n    predicate=\"intersects\",\n    num_processes=2,\n)\n\n# We will quantile normalize the result to smooth out extremes\nranks = rankdata(h3_res10_inter[\"immune_connective_ratio\"], method=\"average\")\nquantile_normalized = (ranks - 1) / (len(ranks) - 1)\nh3_res10_inter = h3_res10_inter.assign(\n    immune_connective_ratio_qnorm=quantile_normalized\n)\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nh3_res10_inter.plot(\n    ax=ax,\n    column=\"immune_connective_ratio_qnorm\",\n    cmap=\"Reds\",\n    legend=True,\n    aspect=1,\n    facecolor=\"none\",\n)\nax.set_axis_off()\nh3_res10_inter.head(5)\n</pre> from histolytics.spatial_agg import grid_aggregate from scipy.stats import rankdata   # This function will compute the ratio of immune to connective cells within each grid cell # In general, any function that takes a GeoDataFrame and returns a scalar # can be used here. Typically, this will be a function that calculates # a count, sum, mean, or other statistic of interest out of the nuclei. def immune_connective_ratio(nuclei):     \"\"\"Calculate the immune to connective cell ratio in a grid cell.\"\"\"     if \"inflammatory\" in nuclei.value_counts(\"class_name\"):         immune_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"inflammatory\"]     else:         immune_cnt = 0      if \"connective\" in nuclei.value_counts(\"class_name\"):         connective_cnt = nuclei.value_counts(\"class_name\", normalize=True)[\"connective\"]     else:         connective_cnt = 0      if connective_cnt &gt; 0:         return float(immune_cnt / connective_cnt)     else:         return 0   h3_res10_inter = grid_aggregate(     objs=nuc,     grid=h3_res10_inter,     metric_func=immune_connective_ratio,     new_col_names=[\"immune_connective_ratio\"],     predicate=\"intersects\",     num_processes=2, )  # We will quantile normalize the result to smooth out extremes ranks = rankdata(h3_res10_inter[\"immune_connective_ratio\"], method=\"average\") quantile_normalized = (ranks - 1) / (len(ranks) - 1) h3_res10_inter = h3_res10_inter.assign(     immune_connective_ratio_qnorm=quantile_normalized )  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) h3_res10_inter.plot(     ax=ax,     column=\"immune_connective_ratio_qnorm\",     cmap=\"Reds\",     legend=True,     aspect=1,     facecolor=\"none\", ) ax.set_axis_off() h3_res10_inter.head(5) Out[\u00a0]: geometry immune_connective_ratio immune_connective_ratio_qnorm 8a82a938225ffff POLYGON ((3413.63457 1373.25558, 3372.90649 14... 4.25 0.802023 8a82a939c307fff POLYGON ((2943.1294 2991.85151, 2975.94699 306... 4.75 0.835260 8a82a939b22ffff POLYGON ((772.20801 7544.76326, 812.90611 7485... 0.80 0.232659 8a82a9399c87fff POLYGON ((1355.27663 5295.12391, 1281.75405 52... 1.25 0.372832 8a82a9398baffff POLYGON ((2105.08282 4558.47091, 2072.26614 44... 4.75 0.835260 In\u00a0[\u00a0]: Copied! <pre>%%timeit\nh3_grid(stroma, resolution=9)\n</pre> %%timeit h3_grid(stroma, resolution=9) <pre>400 ms \u00b1 16.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%timeit\nh3_grid(stroma, resolution=10)\n</pre> %%timeit h3_grid(stroma, resolution=10) <pre>1.73 s \u00b1 24.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%timeit\nrect_grid(stroma, resolution=(256, 256), overlap=0, predicate=\"intersects\")\n</pre> %%timeit rect_grid(stroma, resolution=(256, 256), overlap=0, predicate=\"intersects\") <pre>62 ms \u00b1 351 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%timeit\nrect_grid(stroma, resolution=(512, 512), overlap=0, predicate=\"intersects\")\n</pre> %%timeit rect_grid(stroma, resolution=(512, 512), overlap=0, predicate=\"intersects\") <pre>18.5 ms \u00b1 287 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%timeit\n# two cores\ngrid_aggregate(\n    objs=nuc,\n    grid=h3_res10_inter,\n    metric_func=immune_connective_ratio,\n    new_col_names=[\"immune_connective_ratio\"],\n    predicate=\"intersects\",\n    num_processes=2,\n)\n</pre> %%timeit # two cores grid_aggregate(     objs=nuc,     grid=h3_res10_inter,     metric_func=immune_connective_ratio,     new_col_names=[\"immune_connective_ratio\"],     predicate=\"intersects\",     num_processes=2, ) <pre>610 ms \u00b1 20.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%timeit\n# four cores\ngrid_aggregate(\n    objs=nuc,\n    grid=h3_res10_inter,\n    metric_func=immune_connective_ratio,\n    new_col_names=[\"immune_connective_ratio\"],\n    predicate=\"intersects\",\n    num_processes=4,\n)\n</pre> %%timeit # four cores grid_aggregate(     objs=nuc,     grid=h3_res10_inter,     metric_func=immune_connective_ratio,     new_col_names=[\"immune_connective_ratio\"],     predicate=\"intersects\",     num_processes=4, ) <pre>478 ms \u00b1 16.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/partitioning/#introduction","title":"Introduction\u00b6","text":"<p>What is Spatial Partitioning? \u2702\ufe0f Spatial partitioning is the process of dividing a dataset into distinct sections based on spatial criteria. For histological WSIs, this means taking your complete segmentation map and breaking it down into smaller, more focused areas. This can be done in two main ways:</p> <p>Interface Partitioning: This method involves defining regions at the interface region of two distinct tissue types. For example, tumor-stroma-interfaces or tumor-invasive-margins are easy to partition with this approach.</p> <p>Partitioning with Uniform Grids: This method superimposes a grid over the entire tissue segmentation map, dividing it into smaller, uniform squares or hexagons. Each grid cell then becomes a self-contained unit for analysis. This approach provides a higher resolution view of the tissue, allowing you to compute statistics\u2014like cell density or marker expression\u2014within each small grid cell, rather than averaging them across a large, heterogeneous region.</p> <p>Next we'll demonstrate how to implement both types of partitioning using the Histolytics library. We will be using a segmented cervix biopsy dataset as an example.</p>"},{"location":"user_guide/spatial/partitioning/#tissue-interfaces","title":"Tissue Interfaces\u00b6","text":"<p>Histolytics provides a simple way to find interface regions between two tissues, the <code>get_interfaces</code>-function. Useful for defining regions, such as the tumor-stroma interface or the invasive margin of a tumor that both have clinical significance. We will extract the interface regions between the CIN lesion and the surrounding stroma to define the lesion-stroma-interface. We will use 300 pixels as the interface breadth. The breadth of the interface region is a user defined parameter that can be adjusted based on the specific requirements of the analysis.</p>"},{"location":"user_guide/spatial/partitioning/#grid-partitioning","title":"Grid Partitioning\u00b6","text":"<p>Next up are the grid partitions. Grid partitioning is sometimes called spatial indexing in geospatial analysis. We can create a grid over the tissue section. This allows us to analyze the spatial distribution of cell types within each grid cell at different tissues. Histolytics supports hexagonal (h3) and regular rectangular grids for partitioning. We will showcase both:</p>"},{"location":"user_guide/spatial/partitioning/#1-h3-hexagonal-grid-partitioning","title":"1. H3 Hexagonal Grid Partitioning\u00b6","text":"<p>First the hexagonal grid. We will fit the grid with three distinct resolutions. The higher the resolution, the smaller the hexagons and the more detailed the partitioning. We will fit the grids to the stromal regions of the segmented biopsy:</p>"},{"location":"user_guide/spatial/partitioning/#2-rectangular-grid-partitioning","title":"2. Rectangular Grid Partitioning\u00b6","text":"<p>Next, we will create a rectangular grid over the tissue section. Similar to the hexagonal grid, we can specify the resolution and overlap of the grid cells. The <code>resolution</code> parameter defines the size of each grid cell, while the <code>overlap</code> parameter controls the amount of overlap between adjacent cells. This time we will fit the grid to the lesion regions of the segmented biopsy using the intersects spatial predicate:</p>"},{"location":"user_guide/spatial/partitioning/#grid-aggregation","title":"Grid Aggregation\u00b6","text":"<p>Next, we will showcase how to use the grid cells for spatial aggregation tasks. In general, with Histolytics, you can flexibly create aggregation metrics to compute any biological phenomena of interest using the <code>grid_aggregate</code> function. It is designed to work with only custom aggregation functions.</p> <p>In this example, we will use the hexagonal grid that was fitted over the interface region of the stroma, and compute the inflammatory to connective cell ratio for each grid cell. This allows us to pinpoint regions with high inflammation rather than rely on a global statistic.</p>"},{"location":"user_guide/spatial/partitioning/#benchmarking","title":"Benchmarking\u00b6","text":""},{"location":"user_guide/spatial/partitioning/#h3-partitioning","title":"H3 Partitioning\u00b6","text":"<p>The partitioning of the tissue regions is generally fast to do. The higher the resolution is the more it takes to compute the grid. For example, partitioning the stroma in this segmented biopsy into a hexagonal grid at resolution 9 takes around 400ms on average, whereas with resolution 10 the run time can be over a second:</p>"},{"location":"user_guide/spatial/partitioning/#rect-grid-partitioning","title":"Rect Grid Partitioning\u00b6","text":"<p>With rectangular grid, the partitioning speed also depends on resolution and overlap parameters since they affect the number of grid cells that need to be processed. Higher resolutions and larger overlaps will generally lead to longer processing times. However, the rectangular grid fitting is generally a lot faster than hexagonal grid fitting:</p>"},{"location":"user_guide/spatial/partitioning/#grid-aggregation","title":"Grid Aggregation\u00b6","text":"<p>The grid aggregation metric efficiency depends on the number of grid cells used. The <code>grid_aggregate</code> function is parallelizable with respect to the number of grid cells so the runtimes can be reduced with respect to computational resources. For example, our example of computing the immune to connective cell ratio could be sped up by increasing the number of cores:</p>"},{"location":"user_guide/spatial/querying/","title":"Spatial Querying","text":"In\u00a0[1]: Copied! <pre># Let's load some data to play around with\nimport matplotlib.pyplot as plt\nfrom histolytics.data import cervix_nuclei, cervix_tissue\n\ntis = cervix_tissue()\nnuc = cervix_nuclei()\n\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False)\nax.set_axis_off()\n</pre> # Let's load some data to play around with import matplotlib.pyplot as plt from histolytics.data import cervix_nuclei, cervix_tissue  tis = cervix_tissue() nuc = cervix_nuclei()  fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False) ax.set_axis_off() In\u00a0[2]: Copied! <pre># Let's check what nuclei types and tissue types are present\nprint(\"Nuclei types:\")\nprint(nuc[\"class_name\"].unique())\nprint(\"Tissue types:\")\nprint(tis[\"class_name\"].unique())\n</pre> # Let's check what nuclei types and tissue types are present print(\"Nuclei types:\") print(nuc[\"class_name\"].unique()) print(\"Tissue types:\") print(tis[\"class_name\"].unique()) <pre>Nuclei types:\n['connective' 'squamous_epithel' 'glandular_epithel' 'inflammatory'\n 'neoplastic']\nTissue types:\n['stroma' 'cin' 'slime' 'blood' 'gland']\n</pre> In\u00a0[3]: Copied! <pre>from histolytics.spatial_ops import get_objs\n\n# get the CIN tissue and neoplastic nuclei\ncin_tissue = tis[tis[\"class_name\"] == \"cin\"]\nneo_nuclei = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n\n# select all the neoplastic nuclei contained within CIN tissue\nneo_within_cin = get_objs(cin_tissue, neo_nuclei, predicate=\"contains\")\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nneo_within_cin.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nax.set_axis_off()\nneo_within_cin.head()\n</pre> from histolytics.spatial_ops import get_objs  # get the CIN tissue and neoplastic nuclei cin_tissue = tis[tis[\"class_name\"] == \"cin\"] neo_nuclei = nuc[nuc[\"class_name\"] == \"neoplastic\"]  # select all the neoplastic nuclei contained within CIN tissue neo_within_cin = get_objs(cin_tissue, neo_nuclei, predicate=\"contains\")  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) neo_within_cin.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) ax.set_axis_off() neo_within_cin.head() Out[3]: geometry class_name 23 POLYGON ((944 4701.02, 939.83 4703.17, 936.01 ... neoplastic 25 POLYGON ((751.75 5549, 751.01 5549.02, 750.01 ... neoplastic 26 POLYGON ((916 4754.02, 912.01 4756.02, 909.01 ... neoplastic 47 POLYGON ((776 8326.02, 775.01 8327, 775.01 834... neoplastic 173 POLYGON ((1947 4664.02, 1946.01 4665, 1946.01 ... neoplastic In\u00a0[4]: Copied! <pre>from histolytics.spatial_ops import get_objs\n\n# get the stroma tissue and inflammatory nuclei\nstroma_tissue = tis[tis[\"class_name\"] == \"stroma\"]\ninfl_nuclei = nuc[nuc[\"class_name\"] == \"inflammatory\"]\n\n# select all the nuclei contained within stroma tissue\ninfl_within_stroma = get_objs(stroma_tissue, infl_nuclei, predicate=\"contains\")\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\ninfl_within_stroma.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nax.set_axis_off()\ninfl_within_stroma.head()\n</pre> from histolytics.spatial_ops import get_objs  # get the stroma tissue and inflammatory nuclei stroma_tissue = tis[tis[\"class_name\"] == \"stroma\"] infl_nuclei = nuc[nuc[\"class_name\"] == \"inflammatory\"]  # select all the nuclei contained within stroma tissue infl_within_stroma = get_objs(stroma_tissue, infl_nuclei, predicate=\"contains\")  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) infl_within_stroma.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) ax.set_axis_off() infl_within_stroma.head() Out[4]: geometry class_name 14 POLYGON ((925 5211.02, 924.01 5212, 924.01 522... inflammatory 17 POLYGON ((899 5607.02, 898.01 5608, 898.01 561... inflammatory 22 POLYGON ((873 5284.02, 871.01 5286, 871.01 529... inflammatory 29 POLYGON ((974.75 5432, 974.01 5432.02, 973.01 ... inflammatory 31 POLYGON ((960.75 5540, 956 5540.02, 955.01 554... inflammatory In\u00a0[5]: Copied! <pre># get the stroma tissue and neoplastic nuclei\nstroma_tissue = tis[tis[\"class_name\"] == \"stroma\"]\ninfl_nuclei = nuc[nuc[\"class_name\"] == \"neoplastic\"]\n\n# select all the nuclei contained within CIN tissue\nneo_inter_stroma = get_objs(stroma_tissue, infl_nuclei, predicate=\"intersects\")\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nneo_inter_stroma.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nax.set_axis_off()\nneo_inter_stroma.head()\n</pre> # get the stroma tissue and neoplastic nuclei stroma_tissue = tis[tis[\"class_name\"] == \"stroma\"] infl_nuclei = nuc[nuc[\"class_name\"] == \"neoplastic\"]  # select all the nuclei contained within CIN tissue neo_inter_stroma = get_objs(stroma_tissue, infl_nuclei, predicate=\"intersects\")  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) neo_inter_stroma.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) ax.set_axis_off() neo_inter_stroma.head() Out[5]: geometry class_name 25 POLYGON ((751.75 5549, 751.01 5549.02, 750.01 ... neoplastic 26 POLYGON ((916 4754.02, 912.01 4756.02, 909.01 ... neoplastic 47 POLYGON ((776 8326.02, 775.01 8327, 775.01 834... neoplastic 48 POLYGON ((702 7641.02, 697.01 7643.02, 694.01 ... neoplastic 64 POLYGON ((709.25 8292, 707.01 8293.02, 705.01 ... neoplastic In\u00a0[6]: Copied! <pre># get the CIN tissue and inflammatory nuclei\ncin_tissue = tis[tis[\"class_name\"] == \"cin\"]\ninfl_nuclei = nuc[nuc[\"class_name\"] == \"inflammatory\"]\n\n# select all the inflammatory nuclei intersecting within CIN tissue\ninfl_inter_cin = get_objs(cin_tissue, infl_nuclei, predicate=\"intersects\")\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\ninfl_inter_cin.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nax.set_axis_off()\ninfl_inter_cin.head()\n</pre> # get the CIN tissue and inflammatory nuclei cin_tissue = tis[tis[\"class_name\"] == \"cin\"] infl_nuclei = nuc[nuc[\"class_name\"] == \"inflammatory\"]  # select all the inflammatory nuclei intersecting within CIN tissue infl_inter_cin = get_objs(cin_tissue, infl_nuclei, predicate=\"intersects\")  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) infl_inter_cin.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) ax.set_axis_off() infl_inter_cin.head() Out[6]: geometry class_name 14 POLYGON ((925 5211.02, 924.01 5212, 924.01 522... inflammatory 17 POLYGON ((899 5607.02, 898.01 5608, 898.01 561... inflammatory 22 POLYGON ((873 5284.02, 871.01 5286, 871.01 529... inflammatory 29 POLYGON ((974.75 5432, 974.01 5432.02, 973.01 ... inflammatory 33 POLYGON ((987 7560.02, 985.01 7561.02, 983.01 ... inflammatory In\u00a0[7]: Copied! <pre>%%timeit\n\n# get the CIN tissue and inflammatory nuclei\ncin_tissue = tis[tis[\"class_name\"] == \"cin\"]\ninfl_nuclei = nuc[nuc[\"class_name\"] == \"inflammatory\"]\n\n# select all the inflammatory nuclei intersecting within CIN tissue\ninfl_inter_cin = get_objs(cin_tissue, infl_nuclei, predicate=\"intersects\")\n</pre> %%timeit  # get the CIN tissue and inflammatory nuclei cin_tissue = tis[tis[\"class_name\"] == \"cin\"] infl_nuclei = nuc[nuc[\"class_name\"] == \"inflammatory\"]  # select all the inflammatory nuclei intersecting within CIN tissue infl_inter_cin = get_objs(cin_tissue, infl_nuclei, predicate=\"intersects\") <pre>14.9 ms \u00b1 453 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/querying/#introduction","title":"Introduction\u00b6","text":"<p>What is Spatial Querying? \ud83e\udded Spatial querying is the process of retrieving data based on its spatial relationships. Think of a spatial query like asking a question about location. Instead of a traditional query that might ask \"what are all the cells with an area greater than X?\", a spatial query asks questions like:</p> <p>\"Which nuclei are contained within or intersect with this specific region?\"</p> <p>Essentially, you're using location to filter and select your data. This is a fundamental concept in geospatial analysis, whether you're working with satellite imagery or, in this case, histological whole-slide images (WSIs).</p> <p>Since Histolytics' panoptic segmentation outputs are typically stored in vectorized format (GeoParquet files) instead of dense raster masks, spatial querying can be performed more efficiently and flexibly at WSI-scale.</p> <p>Next we'll demonstrate the most useful queries in Histolytics. We will be using a segmented cervix biopsy dataset as an example.</p>"},{"location":"user_guide/spatial/querying/#query-1-nuclei-within-a-specific-region","title":"Query 1: Nuclei Within a Specific Region\u00b6","text":"<p>Next we'll show how to query cells/nuclei contained strictly within a specific region using spatial querying. The <code>get_objs</code> function is the workhorse to perform these queries. It uses the powerful R-Tree spatial index provided by <code>geopandas</code> to efficiently find geometries that meet our criteria.</p> <ol> <li>We will query for neoplastic cells within a neoplastic lesion (CIN) of a cervix biopsy.</li> <li>We will query inflammatory cells within the stromal region of the cervix biopsy.</li> </ol>"},{"location":"user_guide/spatial/querying/#query-2-nuclei-intersecting-with-a-specific-region","title":"Query 2: Nuclei Intersecting with a Specific Region\u00b6","text":"<p>Typically we don't want to constrain the queries to nuclei that reside strictly within a specific region but rather use the more loose 'intersects' predicate. This includes nuclei that intersect with the region's boundaries. Next we'll:</p> <ol> <li>Query for neoplastic cells within a stroma of a cervix biopsy. If the segmentation maps would be perfect, this would be an effective way to find metastasizing cells or just neoplastic cells residing at the tissue boundary.</li> <li>Query inflammatory cells within the CIN lesion region of the cervix biopsy. This is an efficient way to detect TILs (tumor-infiltrating lymphocytes).</li> </ol>"},{"location":"user_guide/spatial/querying/#benchmarking","title":"Benchmarking\u00b6","text":"<p>Instead of looping patches and finding inflammatory nuclei from each raster patch, the spatial indexing based querying can significantly speed up the process. For example, the last query we did takes less than 20ms on a regular laptop.</p>"},{"location":"user_guide/spatial/stromal_features/","title":"Stromal Features","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom histolytics.data import hgsc_stroma_he, hgsc_stroma_nuclei\n\n# Example data\nnuc = hgsc_stroma_nuclei()\nhe = hgsc_stroma_he()\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\nax[0].imshow(he)\nax[0].set_axis_off()\n# note that the gdf has origin in bottom left, thus it is flipped\nnuc.plot(ax=ax[1], figsize=(10, 10), column=\"class_name\")\nax[1].set_axis_off()\n</pre> import matplotlib.pyplot as plt from histolytics.data import hgsc_stroma_he, hgsc_stroma_nuclei  # Example data nuc = hgsc_stroma_nuclei() he = hgsc_stroma_he()  fig, ax = plt.subplots(1, 2, figsize=(12, 6)) ax[0].imshow(he) ax[0].set_axis_off() # note that the gdf has origin in bottom left, thus it is flipped nuc.plot(ax=ax[1], figsize=(10, 10), column=\"class_name\") ax[1].set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.stroma_feats.collagen import fiber_feats\nfrom histolytics.utils.raster import gdf2inst\n\n# convert the nuclei gdf into raster mask for nuclei masking\ninst_mask = gdf2inst(nuc, width=he.shape[1], height=he.shape[0])\n\nmetrics = [\n    \"tortuosity\",\n    \"average_turning_angle\",\n    \"major_axis_len\",\n    \"minor_axis_len\",\n    \"major_axis_angle\",\n    \"minor_axis_angle\",\n    \"length\",\n]\n\nfeats = fiber_feats(\n    he,\n    metrics=metrics,\n    label=inst_mask,\n    device=\"cuda\",\n    num_processes=4,\n    normalize=True,\n    rm_bg=True,\n    rm_fg=False,\n    return_edges=True,\n)\n\nax = feats.plot(figsize=(10, 10), aspect=1, column=\"tortuosity\", lw=1)\nax.set_axis_off()\nfeats.head(3)\n</pre> from histolytics.stroma_feats.collagen import fiber_feats from histolytics.utils.raster import gdf2inst  # convert the nuclei gdf into raster mask for nuclei masking inst_mask = gdf2inst(nuc, width=he.shape[1], height=he.shape[0])  metrics = [     \"tortuosity\",     \"average_turning_angle\",     \"major_axis_len\",     \"minor_axis_len\",     \"major_axis_angle\",     \"minor_axis_angle\",     \"length\", ]  feats = fiber_feats(     he,     metrics=metrics,     label=inst_mask,     device=\"cuda\",     num_processes=4,     normalize=True,     rm_bg=True,     rm_fg=False,     return_edges=True, )  ax = feats.plot(figsize=(10, 10), aspect=1, column=\"tortuosity\", lw=1) ax.set_axis_off() feats.head(3) Out[2]: class_name geometry tortuosity average_turning_angle major_axis_len minor_axis_len major_axis_angle minor_axis_angle length 0 collagen LINESTRING (29.06525 26.95506, 29.03764 26.844... 0.387836 0.306846 0.481968 0.203545 0.385391 0.614914 0.411828 1 collagen MULTILINESTRING ((69.19964 89.83999, 69.01369 ... 0.667482 0.612469 0.951406 0.907396 0.688264 0.312042 0.956143 2 collagen MULTILINESTRING ((51.54728 1.36606, 51.67797 1... 0.682916 0.563570 0.590770 0.698044 0.978606 0.021546 0.643949 In\u00a0[3]: Copied! <pre>from histolytics.spatial_graph.graph import fit_graph\nfrom histolytics.spatial_agg.local_diversity import local_diversity\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n\n# fit a graph to the geometries based on distance band\n# and compute the shannon entropy of the major axis angles\nwc, wc_gdf = fit_graph(feats, \"distband\", threshold=100, id_col=\"uid\")\nfeats = local_diversity(\n    feats,\n    wc,\n    val_cols=[\"major_axis_angle\"],\n    metrics=[\"shannon_index\"],\n    scheme=\"percentiles\",  # bin the values into percentiles\n    k=5,\n    normalize=True,  # quantile normalize to smooth the extremes\n)\n\n# plot the collagen disorder\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n\nfeats.plot(\n    ax=ax, cax=cax, column=\"major_axis_angle_shannon_index\", cmap=\"viridis\", legend=True\n)\nax.set_axis_off()\n\nfeats.head(3)\n</pre> from histolytics.spatial_graph.graph import fit_graph from histolytics.spatial_agg.local_diversity import local_diversity from mpl_toolkits.axes_grid1 import make_axes_locatable   # fit a graph to the geometries based on distance band # and compute the shannon entropy of the major axis angles wc, wc_gdf = fit_graph(feats, \"distband\", threshold=100, id_col=\"uid\") feats = local_diversity(     feats,     wc,     val_cols=[\"major_axis_angle\"],     metrics=[\"shannon_index\"],     scheme=\"percentiles\",  # bin the values into percentiles     k=5,     normalize=True,  # quantile normalize to smooth the extremes )  # plot the collagen disorder fig, ax = plt.subplots(1, 1, figsize=(10, 10))  divider = make_axes_locatable(ax) cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)  feats.plot(     ax=ax, cax=cax, column=\"major_axis_angle_shannon_index\", cmap=\"viridis\", legend=True ) ax.set_axis_off()  feats.head(3) Out[3]: class_name geometry tortuosity average_turning_angle major_axis_len minor_axis_len major_axis_angle minor_axis_angle length uid major_axis_angle_shannon_index uid 0 collagen LINESTRING (29.06525 26.95506, 29.03764 26.844... 0.387836 0.306846 0.481968 0.203545 0.385391 0.614914 0.411828 0 0.189945 1 collagen MULTILINESTRING ((69.19964 89.83999, 69.01369 ... 0.667482 0.612469 0.951406 0.907396 0.688264 0.312042 0.956143 1 0.130960 2 collagen MULTILINESTRING ((51.54728 1.36606, 51.67797 1... 0.682916 0.563570 0.590770 0.698044 0.978606 0.021546 0.643949 2 0.134780 In\u00a0[4]: Copied! <pre>from histolytics.stroma_feats.intensity import stromal_intensity_feats\n\nmetrics = [\n    \"max\",\n    \"min\",\n    \"mean\",\n    \"median\",\n    \"std\",\n    \"quantiles\",\n    \"meanmediandiff\",\n    \"mad\",\n    \"iqr\",\n    \"skewness\",\n    \"kurtosis\",\n    \"histenergy\",\n    \"histentropy\",\n]\n\nintensity_feats = stromal_intensity_feats(\n    he,\n    label=inst_mask,  # nuclei labels can be used for masking them out\n    metrics=metrics,\n    device=\"cpu\",\n)\n\nintensity_feats\n</pre> from histolytics.stroma_feats.intensity import stromal_intensity_feats  metrics = [     \"max\",     \"min\",     \"mean\",     \"median\",     \"std\",     \"quantiles\",     \"meanmediandiff\",     \"mad\",     \"iqr\",     \"skewness\",     \"kurtosis\",     \"histenergy\",     \"histentropy\", ]  intensity_feats = stromal_intensity_feats(     he,     label=inst_mask,  # nuclei labels can be used for masking them out     metrics=metrics,     device=\"cpu\", )  intensity_feats Out[4]: <pre>hematoxylin_area        5.455880e+05\nhematoxylin_R_max       9.067904e-01\nhematoxylin_R_min       7.134423e-08\nhematoxylin_R_mean      7.750884e-01\nhematoxylin_R_median    8.070443e-01\n                            ...     \neosin_B_iqr             4.969945e-03\neosin_B_skewness        7.534861e-03\neosin_B_kurtosis        1.605189e-02\neosin_B_histenergy     -1.495228e+01\neosin_B_histentropy     3.424770e+02\nLength: 80, dtype: float32</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/stromal_features/#introduction","title":"Introduction\u00b6","text":"<p>What are stromal features? \ud83d\udd2c Stromal features are quantitative measurements of the stromal extracellular matrix (ECM) which is the connective tissue that supports the functional cells of an organ. Analyzing the stroma is critical because it's not just passive filler; it actively interacts with and influences the behavior of cells. The stroma, is composed of various components, including fibroblasts, immune cells, and the extracellular matrix (ECM). Histolytics focuses on characterizing this ECM, particularly the arrangement of collagen fibers and the staining patterns of the stroma.</p> <p>Histolytics allows you to compute two main types of features to characterize the stroma:</p> <p>Collagen Fiber Features: These features are derived from the collagen fibers that make up a significant portion of the ECM. By analyzing the structure and arrangement of these fibers, you can quantify subtle changes in the micro-environment. Changes in these features can be linked to disease progression, as tumors often remodel the surrounding collagen to facilitate invasion.</p> <p>Hematoxylin and Eosin (H&amp;E) Intensity Features: Histolytics can measure the intensity of the H&amp;E stain components within the stromal ECM. These intensity measures provide a simple way to quantify the composition and state of the stroma. By analyzing these features, you can gain a deeper understanding of the stromal composition and how it changes in different biological and pathological conditions. For example, desmoplastic reactions in the stroma can lead to changes in the H&amp;E staining patterns which can be detected through these features.</p>"},{"location":"user_guide/spatial/stromal_features/#collagen-features","title":"Collagen Features\u00b6","text":"<p>The collagen features are computed from the histology image using the <code>fiber_feats</code> function. The corresponding nuclei mask can be used to mask out the nuclear objects which typically improves collagen fiber extraction. The collagen features include metrics such as tortuosity, average turning angle, major and minor axis lengths, and angles, as well as fiber length. These features describe the size, orientation, and level of 'squiggliness' of collagen fibers in the tissue.</p> <p>NOTE: that parts of the <code>fiber_feats</code> function can be gpu accelerated and also parallelized using the <code>device</code> and <code>num_processes</code> argument to speed up the computations. The gpu acceleration is used when segmenting the collagen fibers and the parallelization is used when computing the features. You can also use the <code>normalize</code> argument to quantile normalize the features which is typically a good idea to smooth out the extremes. The <code>rm_fg</code> and <code>rm_bg</code> arguments can be used to remove foreground (dark stain) and background pixels (light stain) from the feature computation. These arguments can help improve the quality of the collagen fiber segmentation.</p>"},{"location":"user_guide/spatial/stromal_features/#collagen-orientation-disorder","title":"Collagen Orientation Disorder\u00b6","text":"<p>As a quick example, we show how to compute the level of collagen orientation disorder. A metric that has been shown to be prognostic in various studies. This will utilize the <code>local_diversity</code>-function that was showcased in the Neighborhood Features tutorial. First, a graph is fitted to the collagen fiber geometries based on a distance band to define local collagen neighborhoods. Then we compute the Shannon entropy of the major axis angles for every collagen neighborhood. The result is the collagen orientation disorder score for every collagen fiber.</p>"},{"location":"user_guide/spatial/stromal_features/#hematoxylin-and-eosin-he-intensity-features","title":"Hematoxylin and Eosin (H&amp;E) Intensity Features\u00b6","text":"<p>Next, we'll compute intensity based features of the stroma using the <code>stromal_intensity_feats</code> function. This function computes the image intensity features of the H&amp;E stain components within the stromal ECM. Before the features are extracted from the input H&amp;E image, a HED decomposition is run on the image to separate the stain components and the intensity features are computed separately for the eosin and hematoxylin components. Similarly to the nuclear intensity features, the metrics that can be computed from the intensity values are: \"min\", \"max\", \"mean\", \"median\", \"std\", \"quantiles\", \"meanmediandiff\", \"mad\", \"iqr\", \"skewness\", \"kurtosis\", \"histenergy\". Moreover, the <code>stromal_intensity_feats</code> function calculates the estimated area of the hematoxylin and eosin stains. These features work are able to capture differences of the H&amp;E stains in the stroma , especially when applied at the WSI-level (See workflow).</p> <p>Note that the <code>stromal_intensity_feats</code> function returns a pd.Series containing the computed intensity features. I.e. one feature value per image is returned. Previously, we had computed features per segmented objects and not at patch-level.</p> <p>Note also that the function can be GPU-accelerated if needed. For example, the HED-decomposition can benefit from GPU acceleration significantly.</p>"},{"location":"user_guide/spatial/vector_to_raster/","title":"Rasterizing Vector Data","text":"In\u00a0[\u00a0]: Copied! <pre># Let's load some vector data and the corresponding H&amp;E image\nfrom histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei\n\nhe = hgsc_cancer_he()  # Load the HE image\nnuc = hgsc_cancer_nuclei()  # Corresponding nuclei vector segmentation\n\nax = nuc.plot(figsize=(10, 10), column=\"class_name\")\nax.set_axis_off()\n</pre> # Let's load some vector data and the corresponding H&amp;E image from histolytics.data import hgsc_cancer_he, hgsc_cancer_nuclei  he = hgsc_cancer_he()  # Load the HE image nuc = hgsc_cancer_nuclei()  # Corresponding nuclei vector segmentation  ax = nuc.plot(figsize=(10, 10), column=\"class_name\") ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.utils.raster import gdf2inst, gdf2sem\nimport matplotlib.pyplot as plt\nfrom skimage.color import label2rgb\n\n\ncls_dict = {\n    \"neoplastic\": 1,\n    \"connective\": 2,\n    \"inflammatory\": 3,\n}\n\n# convert the nuclei gdf into raster mask\ninst_mask = gdf2inst(nuc, width=he.shape[1], height=he.shape[0])\ntype_mask = gdf2sem(nuc, class_dict=cls_dict, width=he.shape[1], height=he.shape[0])\n\nfig, ax = plt.subplots(1, 3, figsize=(18, 6))\nax[0].imshow(he)\nax[0].set_axis_off()\nax[1].imshow(label2rgb(inst_mask, bg_label=0, alpha=0.5))\nax[1].set_axis_off()\nax[2].imshow(label2rgb(type_mask, bg_label=0, alpha=0.5))\nax[2].set_axis_off()\nplt.show()\n</pre> from histolytics.utils.raster import gdf2inst, gdf2sem import matplotlib.pyplot as plt from skimage.color import label2rgb   cls_dict = {     \"neoplastic\": 1,     \"connective\": 2,     \"inflammatory\": 3, }  # convert the nuclei gdf into raster mask inst_mask = gdf2inst(nuc, width=he.shape[1], height=he.shape[0]) type_mask = gdf2sem(nuc, class_dict=cls_dict, width=he.shape[1], height=he.shape[0])  fig, ax = plt.subplots(1, 3, figsize=(18, 6)) ax[0].imshow(he) ax[0].set_axis_off() ax[1].imshow(label2rgb(inst_mask, bg_label=0, alpha=0.5)) ax[1].set_axis_off() ax[2].imshow(label2rgb(type_mask, bg_label=0, alpha=0.5)) ax[2].set_axis_off() plt.show() In\u00a0[3]: Copied! <pre>import scipy.ndimage as ndi\nfrom skimage.color import rgb2gray\nimport numpy as np\n\n\n# select only neoplastic nuclei\nneo = nuc[nuc[\"class_name\"] == \"neoplastic\"]\nneo_inst_mask = gdf2inst(neo, width=he.shape[1], height=he.shape[0])\n\n# Compute mean grayscale intensity of neoplastic nuclei:\nmean_intensity = ndi.mean(\n    rgb2gray(he), labels=neo_inst_mask, index=np.unique(neo_inst_mask)[1:]\n)\nneo = neo.assign(mean_intensity=mean_intensity)\n\n# Set the mean_intensity values from neo to nuc\nnuc.loc[neo.index, \"mean_intensity\"] = neo[\"mean_intensity\"]\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\nax[0].imshow(he)\nax[0].set_axis_off()\nax[1].imshow(label2rgb(neo_inst_mask, bg_label=0, alpha=0.5))\nax[1].set_axis_off()\nnuc\n</pre> import scipy.ndimage as ndi from skimage.color import rgb2gray import numpy as np   # select only neoplastic nuclei neo = nuc[nuc[\"class_name\"] == \"neoplastic\"] neo_inst_mask = gdf2inst(neo, width=he.shape[1], height=he.shape[0])  # Compute mean grayscale intensity of neoplastic nuclei: mean_intensity = ndi.mean(     rgb2gray(he), labels=neo_inst_mask, index=np.unique(neo_inst_mask)[1:] ) neo = neo.assign(mean_intensity=mean_intensity)  # Set the mean_intensity values from neo to nuc nuc.loc[neo.index, \"mean_intensity\"] = neo[\"mean_intensity\"]  fig, ax = plt.subplots(1, 2, figsize=(12, 6)) ax[0].imshow(he) ax[0].set_axis_off() ax[1].imshow(label2rgb(neo_inst_mask, bg_label=0, alpha=0.5)) ax[1].set_axis_off() nuc Out[3]: geometry class_name mean_intensity 0 POLYGON ((1394.01 0, 1395.01 1.99, 1398 3.99, ... connective NaN 1 POLYGON ((1391 2.01, 1387 2.01, 1384.01 3.01, ... connective NaN 2 POLYGON ((1382.99 156.01, 1380 156.01, 1376.01... connective NaN 3 POLYGON ((1321 170.01, 1317.01 174.01, 1312.01... connective NaN 4 POLYGON ((1297.01 0, 1299.01 2.99, 1302 5.99, ... connective NaN ... ... ... ... 1290 POLYGON ((258 495, 258 496, 255 496, 255 497, ... inflammatory NaN 1291 POLYGON ((855.25 359, 851 360.01, 849.01 361.0... connective NaN 1292 POLYGON ((841 405, 841 406, 840 406, 840 407, ... neoplastic 0.393750 1293 POLYGON ((954 506, 954 507, 952 507, 952 508, ... neoplastic 0.313959 1294 POLYGON ((772 473, 772 474, 771 474, 771 475, ... connective NaN <p>1295 rows \u00d7 3 columns</p> In\u00a0[4]: Copied! <pre>from histolytics.utils.raster import inst2gdf\n\n# NOTE: when vectorizing, the class dict needs to have integer keys unlike when rasterizing.\ncls_dict = {\n    1: \"neoplastic\",\n    2: \"connective\",\n    3: \"inflammatory\",\n}\n\nnuc_vectorized = inst2gdf(inst_mask, type_map=type_mask, class_dict=cls_dict)\nnuc_vectorized.sort_values(by=\"uid\", inplace=True)\nax = nuc_vectorized.plot(figsize=(10, 10), column=\"class_name\", aspect=1)\nax.set_axis_off()\nnuc_vectorized\n</pre> from histolytics.utils.raster import inst2gdf  # NOTE: when vectorizing, the class dict needs to have integer keys unlike when rasterizing. cls_dict = {     1: \"neoplastic\",     2: \"connective\",     3: \"inflammatory\", }  nuc_vectorized = inst2gdf(inst_mask, type_map=type_mask, class_dict=cls_dict) nuc_vectorized.sort_values(by=\"uid\", inplace=True) ax = nuc_vectorized.plot(figsize=(10, 10), column=\"class_name\", aspect=1) ax.set_axis_off() nuc_vectorized Out[4]: uid class_name geometry 290 0 connective POLYGON ((1394.4 0.6, 1394.6 0.9, 1394.9 1.2, ... 296 1 connective POLYGON ((1385 2.6, 1384.6 2.9, 1384.1 3.2, 13... 334 2 connective POLYGON ((1379.6 156.6, 1379.4 156.9, 1379.1 1... 336 3 connective POLYGON ((1320.6 170.6, 1320.4 170.9, 1320.1 1... 291 4 connective POLYGON ((1297.4 0.6, 1297.6 0.9, 1297.9 1.2, ... ... ... ... ... 959 1290 inflammatory POLYGON ((256.8 495.6, 256.2 495.9, 255.4 496.... 380 1291 connective POLYGON ((852 359.6, 851.6 359.9, 851.1 360.2,... 37 1292 neoplastic POLYGON ((840.6 405.6, 840.4 405.9, 840.1 406.... 66 1293 neoplastic POLYGON ((953.3 506.6, 953 506.9, 952.6 507.2,... 404 1294 connective POLYGON ((771.8 474, 771.8 474.5, 771.9 475, 7... <p>1293 rows \u00d7 3 columns</p> In\u00a0[5]: Copied! <pre>from histolytics.utils.raster import sem2gdf\n\n# NOTE: when vectorizing, the class dict needs to have integer keys unlike when rasterizing.\ncls_dict = {\n    1: \"neoplastic\",\n    2: \"connective\",\n    3: \"inflammatory\",\n}\n\nnuc_type_vectorized = sem2gdf(type_mask, class_dict=cls_dict)\nax = nuc_type_vectorized.plot(figsize=(10, 10), column=\"class_name\", aspect=1)\nax.set_axis_off()\nnuc_type_vectorized\n</pre> from histolytics.utils.raster import sem2gdf  # NOTE: when vectorizing, the class dict needs to have integer keys unlike when rasterizing. cls_dict = {     1: \"neoplastic\",     2: \"connective\",     3: \"inflammatory\", }  nuc_type_vectorized = sem2gdf(type_mask, class_dict=cls_dict) ax = nuc_type_vectorized.plot(figsize=(10, 10), column=\"class_name\", aspect=1) ax.set_axis_off() nuc_type_vectorized Out[5]: uid class_name geometry 0 3 inflammatory POLYGON ((850.6 0.6, 851 0.9, 852 1.2, 853 1.4... 1 3 inflammatory POLYGON ((1195.4 0.6, 1195.6 0.9, 1196.3 1.2, ... 2 2 connective POLYGON ((405.5 0.6, 405.8 0.9, 406.7 1.2, 407... 3 2 connective POLYGON ((818.4 1, 818.6 1.4, 819.2 1.8, 819.8... 4 2 connective POLYGON ((1394.4 0.6, 1394.6 0.9, 1394.9 1.2, ... ... ... ... ... 1224 2 connective POLYGON ((946.6 1496.6, 946.4 1497, 947 1497.4... 1225 2 connective POLYGON ((1437.6 1492.6, 1437.4 1492.9, 1437.1... 1226 3 inflammatory POLYGON ((293 1487.6, 292.6 1487.9, 292.1 1488... 1227 2 connective POLYGON ((886.6 1494.6, 886.4 1494.9, 886.1 14... 1228 2 connective POLYGON ((1486.3 1490.6, 1486 1490.9, 1485.6 1... <p>1229 rows \u00d7 3 columns</p> In\u00a0[20]: Copied! <pre>from shapely.geometry import box\n\n\ndef crop_gdf(gdf, crop_size=1000):\n    # Get the bounds of the H&amp;E image\n    h, w = he.shape[:2]\n\n    # Calculate center crop coordinates\n    x_center, y_center = w // 2, h // 2\n    x_min = x_center - crop_size // 2\n    y_min = y_center - crop_size // 2\n    x_max = x_min + crop_size\n    y_max = y_min + crop_size\n\n    # Create bounding box for center crop\n    crop_box = box(x_min, y_min, x_max, y_max)\n\n    # Clip the nuclei to the center crop\n    nuc_cropped = gdf.clip(crop_box)\n    nuc_cropped = nuc_cropped.reset_index(drop=True)\n\n    return nuc_cropped\n\n\nnuc_1000x1000 = crop_gdf(nuc, 1000)\nnuc_500x500 = crop_gdf(nuc, 500)\n</pre> from shapely.geometry import box   def crop_gdf(gdf, crop_size=1000):     # Get the bounds of the H&amp;E image     h, w = he.shape[:2]      # Calculate center crop coordinates     x_center, y_center = w // 2, h // 2     x_min = x_center - crop_size // 2     y_min = y_center - crop_size // 2     x_max = x_min + crop_size     y_max = y_min + crop_size      # Create bounding box for center crop     crop_box = box(x_min, y_min, x_max, y_max)      # Clip the nuclei to the center crop     nuc_cropped = gdf.clip(crop_box)     nuc_cropped = nuc_cropped.reset_index(drop=True)      return nuc_cropped   nuc_1000x1000 = crop_gdf(nuc, 1000) nuc_500x500 = crop_gdf(nuc, 500) In\u00a0[21]: Copied! <pre>%%timeit\ngdf2inst(nuc, width=he.shape[1], height=he.shape[0])\n</pre> %%timeit gdf2inst(nuc, width=he.shape[1], height=he.shape[0]) <pre>146 ms \u00b1 1.73 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[22]: Copied! <pre>%%timeit\ngdf2inst(nuc_1000x1000, width=1000, height=1000)\n</pre> %%timeit gdf2inst(nuc_1000x1000, width=1000, height=1000) <pre>65.4 ms \u00b1 1.12 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[23]: Copied! <pre>%%timeit\ngdf2inst(nuc_500x500, width=500, height=500)\n</pre> %%timeit gdf2inst(nuc_500x500, width=500, height=500) <pre>17 ms \u00b1 185 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>def center_crop_raster(raster_mask, crop_size):\n    h, w = raster_mask.shape[:2]\n\n    crop_h = crop_w = crop_size\n\n    # Calculate center crop coordinates\n    y_center, x_center = h // 2, w // 2\n    y_min = max(0, y_center - crop_h // 2)\n    x_min = max(0, x_center - crop_w // 2)\n    y_max = min(h, y_min + crop_h)\n    x_max = min(w, x_min + crop_w)\n\n    cropped_mask = raster_mask[y_min:y_max, x_min:x_max]\n\n    return cropped_mask\n\n\ninst_mask_1000x1000 = center_crop_raster(inst_mask, 1000)\ninst_mask_500x500 = center_crop_raster(inst_mask, 500)\ntype_mask_1000x1000 = center_crop_raster(type_mask, 1000)\ntype_mask_500x500 = center_crop_raster(type_mask, 500)\n</pre> def center_crop_raster(raster_mask, crop_size):     h, w = raster_mask.shape[:2]      crop_h = crop_w = crop_size      # Calculate center crop coordinates     y_center, x_center = h // 2, w // 2     y_min = max(0, y_center - crop_h // 2)     x_min = max(0, x_center - crop_w // 2)     y_max = min(h, y_min + crop_h)     x_max = min(w, x_min + crop_w)      cropped_mask = raster_mask[y_min:y_max, x_min:x_max]      return cropped_mask   inst_mask_1000x1000 = center_crop_raster(inst_mask, 1000) inst_mask_500x500 = center_crop_raster(inst_mask, 500) type_mask_1000x1000 = center_crop_raster(type_mask, 1000) type_mask_500x500 = center_crop_raster(type_mask, 500) In\u00a0[37]: Copied! <pre>%%timeit\ninst2gdf(inst_mask, type_map=type_mask, class_dict=cls_dict)\n</pre> %%timeit inst2gdf(inst_mask, type_map=type_mask, class_dict=cls_dict) <pre>388 ms \u00b1 19.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[38]: Copied! <pre>%%timeit\ninst2gdf(inst_mask_1000x1000, type_map=type_mask_1000x1000, class_dict=cls_dict)\n</pre> %%timeit inst2gdf(inst_mask_1000x1000, type_map=type_mask_1000x1000, class_dict=cls_dict) <pre>175 ms \u00b1 2.75 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[39]: Copied! <pre>%%timeit\ninst2gdf(inst_mask_500x500, type_map=type_mask_500x500, class_dict=cls_dict)\n</pre> %%timeit inst2gdf(inst_mask_500x500, type_map=type_mask_500x500, class_dict=cls_dict) <pre>48.4 ms \u00b1 550 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/spatial/vector_to_raster/#introduction","title":"Introduction\u00b6","text":"<p>What is Rasterization? \ud83d\uddbc\ufe0f Rasterization is the process of converting geometric data (like the polygons representing your segmented nuclei) into a pixel-based grid. Your original Whole-Slide Image (WSI) is already a raster image \u2014 a large grid of pixels, each with a color value. Vectorized segmentation maps, on the other hand, define shapes using geometry objects such as polygons, points and lines.</p> <p>Why is this important? Many analytical tasks, particularly those involving image intensity features, require the data to be aligned in the same format. For example, if you want to compute the average intensity of a stain within a specific nucleus, you need a pixel-by-pixel mask of that nucleus to apply to the corresponding region in the original WSI. Since your original image is a raster, your segmentation mask must also be a raster.</p> <p>Histolytics supports the rasterization of vector segmentation maps, but it doesn't just rasterize; it also provides the ability to vectorize raster maps. This flexibility is a core strength of the package.</p> <p>This bi-directional capability gives you the freedom to seamlessly move between the two data formats, enabling a wide range of analytical workflows. In this tutorial, we will explore how to rasterize vector segmentation maps and vectorize raster maps using Histolytics.</p> <p>It should be noted, that rasterization can be applied only to a reasonable sized vector data. For example, full WSI-sized vector segmentation maps cannot be rasterized into a HxW sized raster mask, thus, patch-based rasterization is the recommended approach when working with large images.</p> <p>In this tutorial, we will showcase how vector data can be rasterized and vectorized back.</p>"},{"location":"user_guide/spatial/vector_to_raster/#rasterizing-a-vector-map","title":"Rasterizing a Vector Map\u00b6","text":"<p>Histolytics contains two functions to convert vector segmentation maps to raster format and vice versa. These functions are:</p> <ol> <li><code>gdf2inst</code>: Converts a GeoDataFrame of instance segmentation masks to a rasterized instance mask.</li> <li><code>gdf2sem</code>: Converts a GeoDataFrame of semantic segmentation masks to a rasterized semantic mask.</li> </ol> <p>If we want to preserve the instance information during rasterization, we should use <code>gdf2inst</code>. If we are only interested in the class labels, <code>gdf2sem</code> is the appropriate choice. In the next cell, we show how to use these functions to convert our vector segmentation maps to raster format. When using these functions, it's important to specify the target width and height of the rasterized output, which should match the dimensions of the original H&amp;E image.</p>"},{"location":"user_guide/spatial/vector_to_raster/#converting-a-subset-of-nuclei-to-raster","title":"Converting a Subset of Nuclei to Raster\u00b6","text":"<p>When converting to instance raster masks, the label ids are preserved from the original GeoDataFrame, allowing the tracking of individual instances at WSI-level. For example, if you want to compute intensity based features at WSI-level, you can convert the instance masks back to the original vector format once you are done without losing the correct nuclei instance ids. Or you can compute intensity based features on a subset of cells and then merge the resulting features back to the original GeoDataFrame, like this:</p>"},{"location":"user_guide/spatial/vector_to_raster/#vectorizing-a-raster-instance-mask","title":"Vectorizing a Raster Instance Mask\u00b6","text":"<p>To vectorize a raster instance mask, you can use the <code>inst2gdf</code> function. This function takes a raster mask as an input and returns a GeoDataFrame.</p> <p>Note 1: To preserve the nuclei type classes, the <code>inst2gdf</code> function takes in also the type_mask or the rasterized semantic nuclei type segmentation mask. If this is not passed, no nuclei type class information is added to the resulting GeoDataFrame.</p> <p>Note 2: The <code>inst2gdf</code> applies a smoothing to smooth the nuclei borders after vectorization, since the vectorization process can create jagged edges. This can be turned off by setting the <code>smooth_func</code> parameter to None. The resulting GeoDataFrame will not be an exact copy of the original (original -&gt; rasterized mask -&gt; vectorized) GeoDataFrame, but the differences should be minimal.</p>"},{"location":"user_guide/spatial/vector_to_raster/#vectorizing-a-raster-semantic-mask","title":"Vectorizing a Raster Semantic Mask\u00b6","text":"<p>To vectorize a raster semantic mask, you can use the <code>sem2gdf</code> function. This function takes a raster mask as an input and returns a GeoDataFrame. Typically you should use this function only to vectorize tissue segmentations rather than semantic nuclei type masks, but we'll showcase the usage using the semantic nuclei type mask. In the result, the overlapping nuclei will be merged due to the reason that <code>sem2gdf</code> does not handle instance level information, only class level information.</p>"},{"location":"user_guide/spatial/vector_to_raster/#benchmark","title":"Benchmark\u00b6","text":"<p>Rasterizing and vectorizing is not free in terms of computation but still rather efficient. However, the run-times can get unreasonable when input sizes get too large or the number of nuclei is in the tens of thousands.</p> <p>Next we'll benchmark rasterization and vectorization using different image sizes to understand their performance characteristics. Rasterizing and vectorizing instance segmentations is typically more computationally intensive so we'll focus only on those.</p>"},{"location":"user_guide/spatial/vector_to_raster/#rasterization-benchmark","title":"Rasterization Benchmark\u00b6","text":""},{"location":"user_guide/spatial/vector_to_raster/#benchmarking-vectorization","title":"Benchmarking Vectorization\u00b6","text":"<p>The vectorization process is a bit heavier than rasterization but we can still keep the run-times doable by again restricting the input sizes. We'll benchmark the vectorization process using the same image sizes as previously.</p>"},{"location":"user_guide/workflows/TIL_workflow/","title":"Quantifying tumor-infiltrating lymphocytes (TILs)","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom histolytics.data import cervix_nuclei, cervix_tissue\n\n# Let's load example data\ntis = cervix_tissue()\nnuc = cervix_nuclei()\n\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False)\nax.set_axis_off()\n</pre> import matplotlib.pyplot as plt from histolytics.data import cervix_nuclei, cervix_tissue  # Let's load example data tis = cervix_tissue() nuc = cervix_nuclei()  fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.5, legend=True) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=False) ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.spatial_ops import get_objs\n\n# get the CIN (lesion) tissue\ncin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n\n# select all the nuclei contained within CIN tissue\nnuc_within_cin = get_objs(cin_tissue, nuc, predicate=\"contains\")\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nnuc_within_cin.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nax.set_axis_off()\n</pre> from histolytics.spatial_ops import get_objs  # get the CIN (lesion) tissue cin_tissue = tis[tis[\"class_name\"] == \"cin\"]  # select all the nuclei contained within CIN tissue nuc_within_cin = get_objs(cin_tissue, nuc, predicate=\"contains\")  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) nuc_within_cin.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) ax.set_axis_off()  <p>Next we can count the intratumoral TILs which can be a clinically relevant feature to assess.</p> In\u00a0[3]: Copied! <pre>nuc_within_cin.value_counts(\"class_name\")\n</pre> nuc_within_cin.value_counts(\"class_name\") Out[3]: <pre>class_name\nneoplastic           3501\ninflammatory          540\nconnective              3\nglandular_epithel       1\nsquamous_epithel        1\nName: count, dtype: int64</pre> <p>Visualizing the TILs:</p> In\u00a0[4]: Copied! <pre>TILS = nuc_within_cin.loc[nuc_within_cin[\"class_name\"] == \"inflammatory\"]\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nTILS.plot(\n    ax=ax,\n    column=\"class_name\",\n    aspect=1,\n    legend=True,\n)\nax.set_axis_off()\n</pre> TILS = nuc_within_cin.loc[nuc_within_cin[\"class_name\"] == \"inflammatory\"]  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) TILS.plot(     ax=ax,     column=\"class_name\",     aspect=1,     legend=True, ) ax.set_axis_off() In\u00a0[5]: Copied! <pre>from histolytics.spatial_ops import get_interfaces\n\nstroma = tis[tis[\"class_name\"] == \"stroma\"]\ncin_tissue = tis[tis[\"class_name\"] == \"cin\"]\n\n# Partition interface\ninterface = get_interfaces(cin_tissue, stroma, buffer_dist=300)\ninterface = interface.assign(class_name=\"lesion-stroma-interface\")\n\n# get the nuclei within the interface\nnuc_within_interface = get_objs(interface, nuc, predicate=\"contains\")\n\n# plot\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\ninterface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=True)\nnuc_within_interface.plot(\n    ax=ax,\n    column=\"class_name\",\n    aspect=1,\n    legend=False,\n)\nax.set_axis_off()\n\n# Get the absolute inflammatory counts or the fractions by setting `normalize=True`\nnuc_within_interface.value_counts(\"class_name\", normalize=False)\n</pre> from histolytics.spatial_ops import get_interfaces  stroma = tis[tis[\"class_name\"] == \"stroma\"] cin_tissue = tis[tis[\"class_name\"] == \"cin\"]  # Partition interface interface = get_interfaces(cin_tissue, stroma, buffer_dist=300) interface = interface.assign(class_name=\"lesion-stroma-interface\")  # get the nuclei within the interface nuc_within_interface = get_objs(interface, nuc, predicate=\"contains\")  # plot ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) interface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=True) nuc_within_interface.plot(     ax=ax,     column=\"class_name\",     aspect=1,     legend=False, ) ax.set_axis_off()  # Get the absolute inflammatory counts or the fractions by setting `normalize=True` nuc_within_interface.value_counts(\"class_name\", normalize=False) Out[5]: <pre>class_name\ninflammatory         2820\nconnective           1141\nglandular_epithel      38\nneoplastic              7\nsquamous_epithel        1\nName: count, dtype: int64</pre> <p>Now select again the immune nuclei within the interface and plot them.</p> In\u00a0[6]: Copied! <pre>TILS = nuc_within_interface.loc[nuc_within_interface[\"class_name\"] == \"inflammatory\"]\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\ninterface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=True)\nTILS.plot(\n    ax=ax,\n    column=\"class_name\",\n    aspect=1,\n    legend=True,\n)\nax.set_axis_off()\n</pre> TILS = nuc_within_interface.loc[nuc_within_interface[\"class_name\"] == \"inflammatory\"]  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) interface.plot(ax=ax, column=\"class_name\", aspect=1, alpha=0.3, legend=True) TILS.plot(     ax=ax,     column=\"class_name\",     aspect=1,     legend=True, ) ax.set_axis_off() <p>As we see, there is a heavy concentration of TILs at the interface region. This can be a sign of an activated immune response to the CIN lesion.</p> In\u00a0[7]: Copied! <pre>from histolytics.spatial_ops.h3 import h3_grid\n\nh3 = h3_grid(stroma, resolution=10)\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\n\n# Let's plot the grid to see the hexagonal partitioning\nh3.plot(\n    ax=ax, aspect=1, legend=True, facecolor=\"none\", edgecolor=\"blue\", lw=1, alpha=0.5\n)\nax.set_axis_off()\n\nh3.head(5)\n</pre> from histolytics.spatial_ops.h3 import h3_grid  h3 = h3_grid(stroma, resolution=10) ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)  # Let's plot the grid to see the hexagonal partitioning h3.plot(     ax=ax, aspect=1, legend=True, facecolor=\"none\", edgecolor=\"blue\", lw=1, alpha=0.5 ) ax.set_axis_off()  h3.head(5) Out[7]: geometry 8a82a9395007fff POLYGON ((6754.29642 740.90756, 6795.0466 681.... 8a82a939a0cffff POLYGON ((3777.49042 7514.3929, 3818.20685 745... 8a82a939a227fff POLYGON ((3270.67389 7313.52131, 3344.20357 73... 8a82a939e217fff POLYGON ((4353.07966 5392.93377, 4385.89665 54... 8a82a939a787fff POLYGON ((4415.58687 7992.8303, 4448.40275 806... <p>Next we'll compute the immune densities. We will use the fraction of the inflammatory nuclei as the density metric but you could also use other metrics such as the absolute count or absolute count divided by the area of the grid cells.</p> In\u00a0[8]: Copied! <pre>from histolytics.spatial_agg import grid_aggregate\nfrom histolytics.utils.plot import legendgram\n\n\n# This function will compute the percentage of immune nuclei within each grid cell\n# In general, any function that takes a GeoDataFrame and returns a scalar\n# can be used here. Typically, this will be a function that calculates\n# a count, sum, mean, or other statistic of interest out of the nuclei.\ndef immune_density(nuclei):\n    \"\"\"Calculate the immune cell count in a grid cell.\"\"\"\n    if \"inflammatory\" in nuclei.value_counts(\"class_name\"):\n        cnt = nuclei.value_counts(\"class_name\", normalize=False)[\"inflammatory\"]\n    else:\n        cnt = 0\n    return float(cnt)\n\n\nh3 = grid_aggregate(\n    objs=nuc,\n    grid=h3,\n    metric_func=immune_density,\n    new_col_names=[\"immune_density\"],\n    predicate=\"contains\",\n    num_processes=2,\n)\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True)\nh3.plot(\n    ax=ax,\n    column=\"immune_density\",\n    cmap=\"Reds\",\n    legend=True,\n    aspect=1,\n    facecolor=\"none\",\n)\nax.set_axis_off()\n\n# Add a legendgram to visualize the distribution of immune cell density\nax = legendgram(\n    gdf=h3,\n    column=\"immune_density\",\n    n_bins=30,\n    cmap=\"Reds\",\n    ax=ax,\n)\n</pre> from histolytics.spatial_agg import grid_aggregate from histolytics.utils.plot import legendgram   # This function will compute the percentage of immune nuclei within each grid cell # In general, any function that takes a GeoDataFrame and returns a scalar # can be used here. Typically, this will be a function that calculates # a count, sum, mean, or other statistic of interest out of the nuclei. def immune_density(nuclei):     \"\"\"Calculate the immune cell count in a grid cell.\"\"\"     if \"inflammatory\" in nuclei.value_counts(\"class_name\"):         cnt = nuclei.value_counts(\"class_name\", normalize=False)[\"inflammatory\"]     else:         cnt = 0     return float(cnt)   h3 = grid_aggregate(     objs=nuc,     grid=h3,     metric_func=immune_density,     new_col_names=[\"immune_density\"],     predicate=\"contains\",     num_processes=2, )  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) nuc.plot(ax=ax, column=\"class_name\", aspect=1, legend=True) h3.plot(     ax=ax,     column=\"immune_density\",     cmap=\"Reds\",     legend=True,     aspect=1,     facecolor=\"none\", ) ax.set_axis_off()  # Add a legendgram to visualize the distribution of immune cell density ax = legendgram(     gdf=h3,     column=\"immune_density\",     n_bins=30,     cmap=\"Reds\",     ax=ax, )  <p>Here we see that the immune dense regions are located at the interface between the CIN lesion and the stroma, indicative of immune activation. Next we'll compute the distances of the different density grid cells to the lesion to actually quantify that the immune dense regions are indeed closer to the lesion.</p> In\u00a0[9]: Copied! <pre>import pandas as pd\nimport mapclassify\n\nlesion = tis[tis[\"class_name\"] == \"cin\"]\n\ndistances = {}\nfor i, lesion in lesion.reset_index().iterrows():\n    dist = h3.distance(lesion.geometry)\n    distances[i] = dist\n\nmin_dists = pd.DataFrame(distances).min(axis=1)\nmin_dists.name = \"min_dist\"\n\n# join the distances to the grid\nh3 = h3.join(other=min_dists, how=\"left\")\n\n# Let's bin the immune density for visualization, we'll use quantile binning\nbins = mapclassify.Quantiles(h3[\"immune_density\"], k=4)\nh3[\"immune_density_level\"] = bins.yb\n\nh3\n</pre> import pandas as pd import mapclassify  lesion = tis[tis[\"class_name\"] == \"cin\"]  distances = {} for i, lesion in lesion.reset_index().iterrows():     dist = h3.distance(lesion.geometry)     distances[i] = dist  min_dists = pd.DataFrame(distances).min(axis=1) min_dists.name = \"min_dist\"  # join the distances to the grid h3 = h3.join(other=min_dists, how=\"left\")  # Let's bin the immune density for visualization, we'll use quantile binning bins = mapclassify.Quantiles(h3[\"immune_density\"], k=4) h3[\"immune_density_level\"] = bins.yb  h3 Out[9]: geometry immune_density min_dist immune_density_level 8a82a9395007fff POLYGON ((6754.29642 740.90756, 6795.0466 681.... 0.0 3210.127874 0 8a82a939a0cffff POLYGON ((3777.49042 7514.3929, 3818.20685 745... 0.0 1539.781970 0 8a82a939a227fff POLYGON ((3270.67389 7313.52131, 3344.20357 73... 0.0 1000.778838 0 8a82a939e217fff POLYGON ((4353.07966 5392.93377, 4385.89665 54... 0.0 1663.882983 0 8a82a939a787fff POLYGON ((4415.58687 7992.8303, 4448.40275 806... 0.0 1943.510700 0 ... ... ... ... ... 8a82a92b42dffff POLYGON ((2321.10708 10094.14135, 2280.40587 1... 2.0 0.000000 2 8a82a92b0927fff POLYGON ((2435.32504 10045.4735, 2508.84185 10... 0.0 0.000000 0 8a82a92b429ffff POLYGON ((2615.17352 10135.55733, 2541.65625 1... 3.0 0.000000 3 8a82a92b465ffff POLYGON ((2746.43098 10413.05959, 2713.61664 1... 10.0 0.000000 3 8a82a92b09b7fff POLYGON ((2705.72793 10472.07979, 2746.43098 1... 1.0 0.000000 1 <p>2313 rows \u00d7 4 columns</p> <p>Let's now plot the distributions of the distances to the lesion for each immune density level.</p> In\u00a0[10]: Copied! <pre># !pip install seaborn # Uncomment this line to install seaborn for plotting\n</pre> # !pip install seaborn # Uncomment this line to install seaborn for plotting In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\n\ntidy = h3.reset_index().set_index(\"immune_density_level\")\ntidy = tidy[[\"min_dist\"]]\ntidy = tidy.stack()\ntidy = tidy.reset_index()\ntidy = tidy.rename(\n    columns={\n        \"immune_density_level\": \"Immune Density Level\",\n        \"level_1\": \"Attribute\",\n        0: \"Distance to Lesion\",\n    }\n)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nax[0] = sns.kdeplot(\n    ax=ax[0],\n    data=tidy,\n    x=\"Distance to Lesion\",\n    hue=\"Immune Density Level\",\n    fill=True,\n    alpha=0.5,\n    palette=\"viridis\",\n)\n\nax[1] = sns.swarmplot(\n    ax=ax[1],\n    data=tidy,\n    y=\"Distance to Lesion\",\n    x=\"Immune Density Level\",\n    hue=\"Immune Density Level\",\n    size=1.5,\n    orient=\"v\",\n    legend=False,\n    warn_thresh=0.5,\n    palette=\"viridis\",\n)\n\nax[1] = sns.boxplot(\n    ax=ax[1],\n    data=tidy,\n    x=\"Immune Density Level\",\n    y=\"Distance to Lesion\",\n    showfliers=False,\n    color=\"black\",\n    linewidth=1.5,\n    width=0.2,\n    fill=False,\n    whis=1.0,\n    showcaps=False,\n)\n</pre> import seaborn as sns  tidy = h3.reset_index().set_index(\"immune_density_level\") tidy = tidy[[\"min_dist\"]] tidy = tidy.stack() tidy = tidy.reset_index() tidy = tidy.rename(     columns={         \"immune_density_level\": \"Immune Density Level\",         \"level_1\": \"Attribute\",         0: \"Distance to Lesion\",     } )  fig, ax = plt.subplots(1, 2, figsize=(10, 5))  ax[0] = sns.kdeplot(     ax=ax[0],     data=tidy,     x=\"Distance to Lesion\",     hue=\"Immune Density Level\",     fill=True,     alpha=0.5,     palette=\"viridis\", )  ax[1] = sns.swarmplot(     ax=ax[1],     data=tidy,     y=\"Distance to Lesion\",     x=\"Immune Density Level\",     hue=\"Immune Density Level\",     size=1.5,     orient=\"v\",     legend=False,     warn_thresh=0.5,     palette=\"viridis\", )  ax[1] = sns.boxplot(     ax=ax[1],     data=tidy,     x=\"Immune Density Level\",     y=\"Distance to Lesion\",     showfliers=False,     color=\"black\",     linewidth=1.5,     width=0.2,     fill=False,     whis=1.0,     showcaps=False, ) <p>As seen in the KDE and swarm-plots, the immune dense regions are clearly closer to the lesion compared to the less dense regions (which is also clearly visible in the spatial plots). This is a nice way to quantify the spatial distribution of immune cells in relation to the tumor (lesion) tissue in segmented histology images.</p> In\u00a0[\u00a0]: Copied! <pre>from histolytics.spatial_graph.graph import fit_graph\nfrom histolytics.utils.gdf import set_uid\n\nnuc = set_uid(nuc)  # ensure unique IDs for nuclei\nw, w_gdf = fit_graph(\n    nuc, \"delaunay\", id_col=\"uid\", threshold=100\n)  # Let's use Delaunay graph\n\n# visualize the different links on the tissue\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.3)\nw_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1, lw=0.5)\nax.set_axis_off()\nw_gdf.value_counts(\"class_name\")\n</pre> from histolytics.spatial_graph.graph import fit_graph from histolytics.utils.gdf import set_uid  nuc = set_uid(nuc)  # ensure unique IDs for nuclei w, w_gdf = fit_graph(     nuc, \"delaunay\", id_col=\"uid\", threshold=100 )  # Let's use Delaunay graph  # visualize the different links on the tissue ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.3) w_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1, lw=0.5) ax.set_axis_off() w_gdf.value_counts(\"class_name\") Out[\u00a0]: <pre>class_name\nconnective-connective                  15216\nconnective-inflammatory                12399\ninflammatory-inflammatory              11953\nneoplastic-neoplastic                   8470\ninflammatory-neoplastic                 2526\nglandular_epithel-glandular_epithel     1142\nconnective-neoplastic                    780\nglandular_epithel-inflammatory           439\nconnective-glandular_epithel             382\nglandular_epithel-neoplastic              80\nconnective-squamous_epithel                5\ninflammatory-squamous_epithel              2\nglandular_epithel-squamous_epithel         1\nneoplastic-squamous_epithel                1\nsquamous_epithel-squamous_epithel          1\nName: count, dtype: int64</pre> <p>In total, there are more than 2500 links between neoplastic and inflammatory cells. This link count can be used as a metric for immune infiltration as it is or you can derive additional features such as link density at different regions etc. Let's now select these links and visualize them.</p> In\u00a0[14]: Copied! <pre>imm_neo_links = w_gdf[w_gdf[\"class_name\"] == \"inflammatory-neoplastic\"]\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.3)\nimm_neo_links.plot(ax=ax, linewidth=0.5, column=\"class_name\", legend=True, aspect=1)\nax.set_axis_off()\n</pre> imm_neo_links = w_gdf[w_gdf[\"class_name\"] == \"inflammatory-neoplastic\"] ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.3) imm_neo_links.plot(ax=ax, linewidth=0.5, column=\"class_name\", legend=True, aspect=1) ax.set_axis_off() <p>As we see, these links between neoplastic and inflammatory cells concentrate on specific regions on the lesion. We can check which parts of the lesion are the most enriched for these links by overlaying a grid on the lesion and computing the link density. It should be noted that one inflammatory cell can be linked to multiple neoplastic cells and vice versa, so the link density will give us a sense of the overall interaction landscape.</p> In\u00a0[\u00a0]: Copied! <pre>def immune_neo_link_density(nuclei):\n    \"\"\"Calculate the immune cell count in a grid cell.\"\"\"\n    if \"inflammatory-neoplastic\" in nuclei.value_counts(\"class_name\"):\n        cnt = nuclei.value_counts(\"class_name\", normalize=False)[\n            \"inflammatory-neoplastic\"\n        ]\n    else:\n        cnt = 0\n    return float(cnt)\n\n\nlesion = tis[tis[\"class_name\"] == \"cin\"]\nh3_lesion = h3_grid(lesion, resolution=10)\n\nh3_lesion = grid_aggregate(\n    objs=w_gdf,\n    grid=h3_lesion,\n    metric_func=immune_neo_link_density,\n    new_col_names=[\"immune_link_density\"],\n    predicate=\"contains\",\n    num_processes=2,\n)\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False)\nimm_neo_links.plot(ax=ax, column=\"class_name\", aspect=1, legend=True, lw=0.3)\nh3_lesion.plot(\n    ax=ax,\n    column=\"immune_link_density\",\n    cmap=\"Reds\",\n    legend=True,\n    aspect=1,\n    facecolor=\"none\",\n)\nax.set_axis_off()\n\n# Add a legendgram to visualize the distribution of immune cell density\nax = legendgram(\n    gdf=h3_lesion,\n    column=\"immune_link_density\",\n    n_bins=15,\n    cmap=\"Reds\",\n    ax=ax,\n)\n</pre> def immune_neo_link_density(nuclei):     \"\"\"Calculate the immune cell count in a grid cell.\"\"\"     if \"inflammatory-neoplastic\" in nuclei.value_counts(\"class_name\"):         cnt = nuclei.value_counts(\"class_name\", normalize=False)[             \"inflammatory-neoplastic\"         ]     else:         cnt = 0     return float(cnt)   lesion = tis[tis[\"class_name\"] == \"cin\"] h3_lesion = h3_grid(lesion, resolution=10)  h3_lesion = grid_aggregate(     objs=w_gdf,     grid=h3_lesion,     metric_func=immune_neo_link_density,     new_col_names=[\"immune_link_density\"],     predicate=\"contains\",     num_processes=2, )  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, alpha=0.5, legend=False) imm_neo_links.plot(ax=ax, column=\"class_name\", aspect=1, legend=True, lw=0.3) h3_lesion.plot(     ax=ax,     column=\"immune_link_density\",     cmap=\"Reds\",     legend=True,     aspect=1,     facecolor=\"none\", ) ax.set_axis_off()  # Add a legendgram to visualize the distribution of immune cell density ax = legendgram(     gdf=h3_lesion,     column=\"immune_link_density\",     n_bins=15,     cmap=\"Reds\",     ax=ax, )"},{"location":"user_guide/workflows/TIL_workflow/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial demonstrates a WSI-level analysis workflow focused on tumor-infiltrating lymphocytes (TILs). We will use cervix biopsy WSI segmentation data as an example. Specifically, in this tutorial, we will show:</p> <p>Spatial Querying of TILs: Spatial querying allows you to precisely locate TILs relative to other tissue compartments. We will show how to:</p> <ol> <li><p>Query for intratumoral TILs: By performing a spatial containment query, you can identify all lymphocytes located entirely within the boundaries of a segmented tumor region.</p> </li> <li><p>Identify TILs at the tumor-stroma interface: Using tissue intersection partitioning, you can select all lymphocytes that are located within a specified distance of the tumor boundary.</p> </li> </ol> <p>Quantifying Lymphocyte \"Hotness\" and \"Coldness\": Beyond a simple count, the spatial distribution of TILs provides valuable prognostic information. A \"hot\" tumor is characterized by a high density of TILs, while a \"cold\" tumor has a low or absent immune infiltrate. We will demonstrate how to quantify this using grid based aggregation., where we partition the stromal regions into small, uniform grid cells and aggregating the immune cell densities within those.</p> <p>Quantifying Neoplastic-Lymphocyte Links: We will create a spatial graph where nodes represent individual nuclei (both tumor cells and lymphocytes). Links (edges) will be established based on a proximity rule. We'll then show how to to identify and count tumor-lymphocyte links. This gives you a direct measure of close proximity between tumor cells and lymphocytes.</p>"},{"location":"user_guide/workflows/TIL_workflow/#intratumoral-tils","title":"Intratumoral TILs\u00b6","text":"<p>Querying intratumoral TILs is straightforward. By performing a spatial containment query, you can identify all lymphocytes located entirely within the boundaries of a segmented tumor region. In this example, we are actually talking about a cervical lesion and pre-cancerous cells but the principles remain the same.</p>"},{"location":"user_guide/workflows/TIL_workflow/#tils-at-the-tumor-stroma-interface","title":"TILs at the Tumor-Stroma Interface\u00b6","text":"<p>Now we'll identify TILs that are located at the tumor-stroma interface (actually lesion-stroma-interface). This can be done by first partitioning the stromal tissue into the tumor-stroma-interface and then performing a spatial query to find all lymphocytes that are within the interface.</p>"},{"location":"user_guide/workflows/TIL_workflow/#immune-hotnesscoldness","title":"Immune Hotness/Coldness\u00b6","text":"<p>Next, we'll quantify the immune \"hotness\" and \"coldness\" of the tumor microenvironment by analyzing the spatial density distribution of the TILs. We will use the h3 spatial indexing to create a hexagonal grid over the stromal region and compute the density of TILs within each hexagon.</p>"},{"location":"user_guide/workflows/TIL_workflow/#neoplastic-lymphocyte-links","title":"Neoplastic-Lymphocyte Links\u00b6","text":"<p>Next we'll quantify direct proximity based links between neoplastic cells and lymphocytes. This will involve creating a spatial graph where nodes represent individual nuclei (both tumor cells and lymphocytes) and edges represent proximity relationships. This way we can have a more detailed view of possible interactions between neoplastic and immune cells.</p>"},{"location":"user_guide/workflows/TIL_workflow/#conclusions","title":"Conclusions\u00b6","text":"<p>In this workflow tutorial, we showed how to analyze the immuno oncological relationships between inflammatory cells and tumor cells in a segmented histology WSI using Histolytics. We demonstrated how to quantify intratumoral TILs, tumor-stroma-interface TILs, immune cell densities in the stroma and immune cell link densities within the lesion. The flexible tools in Histolytics enable easy exploration, quantification and visualization of these complex spatial relationships.</p>"},{"location":"user_guide/workflows/chromatin_patterns/","title":"Quantifying Neoplastic Nuclear Chromatin Patterns","text":"In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\n\nnuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path\ntis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, legend=True)\nax.set_axis_off()\n</pre> import geopandas as gpd  nuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path tis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, legend=True) ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.spatial_ops.rect_grid import rect_grid\n\n# fit the grid. We'll use 512, 512 sized patches, focus only on tumor\npatch_size = (512, 512)\ngr = rect_grid(tis[tis[\"class_name\"] == \"tumor\"], patch_size, 0)\nax = tis.plot(column=\"class_name\", figsize=(10, 10), alpha=0.5, legend=True)\nax = gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=0.5)\nax.set_axis_off()\n</pre> from histolytics.spatial_ops.rect_grid import rect_grid  # fit the grid. We'll use 512, 512 sized patches, focus only on tumor patch_size = (512, 512) gr = rect_grid(tis[tis[\"class_name\"] == \"tumor\"], patch_size, 0) ax = tis.plot(column=\"class_name\", figsize=(10, 10), alpha=0.5, legend=True) ax = gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=0.5) ax.set_axis_off() In\u00a0[\u00a0]: Copied! <pre>from histolytics.wsi.slide_reader import SlideReader\n\nsl_p = \"/path/to/slide.tiff\"  # &lt;- modify path\nreader = SlideReader(sl_p, backend=\"CUCIM\")\n\n# uncomment to visualize the thumbnail\n# def polygon_to_xywh(polygon):\n#     minx, miny, maxx, maxy = polygon.bounds\n#     return (minx, miny, maxx - minx, maxy - miny)\n\n# # convert to boxes to xywh-coords\n# xywh_list = [polygon_to_xywh(geom) for geom in gr.geometry]\n# thumbnail = reader.read_level(-2)\n# reader.get_annotated_thumbnail(thumbnail, xywh_list)\n</pre> from histolytics.wsi.slide_reader import SlideReader  sl_p = \"/path/to/slide.tiff\"  # &lt;- modify path reader = SlideReader(sl_p, backend=\"CUCIM\")  # uncomment to visualize the thumbnail # def polygon_to_xywh(polygon): #     minx, miny, maxx, maxy = polygon.bounds #     return (minx, miny, maxx - minx, maxy - miny)  # # convert to boxes to xywh-coords # xywh_list = [polygon_to_xywh(geom) for geom in gr.geometry] # thumbnail = reader.read_level(-2) # reader.get_annotated_thumbnail(thumbnail, xywh_list) In\u00a0[4]: Copied! <pre>from histolytics.wsi.wsi_processor import WSIGridProcessor\nfrom histolytics.nuc_feats.chromatin import chromatin_feats\nfrom functools import partial\n\n# define our pipeline\npipeline = partial(chromatin_feats, metrics=(\"chrom_area\", \"chrom_nuc_prop\"))\n\ncrop_loader = WSIGridProcessor(\n    slide_reader=reader,\n    grid=gr,\n    nuclei=nuc[nuc[\"class_name\"] == \"neoplastic\"],  # use only neoplastic nuclei\n    pipeline_func=pipeline,\n    batch_size=8,  # use\n    num_workers=8,\n    pin_memory=False,\n    shuffle=False,\n    drop_last=False,\n)\n</pre> from histolytics.wsi.wsi_processor import WSIGridProcessor from histolytics.nuc_feats.chromatin import chromatin_feats from functools import partial  # define our pipeline pipeline = partial(chromatin_feats, metrics=(\"chrom_area\", \"chrom_nuc_prop\"))  crop_loader = WSIGridProcessor(     slide_reader=reader,     grid=gr,     nuclei=nuc[nuc[\"class_name\"] == \"neoplastic\"],  # use only neoplastic nuclei     pipeline_func=pipeline,     batch_size=8,  # use     num_workers=8,     pin_memory=False,     shuffle=False,     drop_last=False, ) In\u00a0[5]: Copied! <pre>import pandas as pd\nfrom tqdm import tqdm\n\ncrop_feats = []\nnuc_feats = []\nwith crop_loader as loader:\n    with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:\n        for batch_idx, batch in enumerate(pbar):\n            crop_feats.extend(batch)  # collect the patch level dfs\n            nuc_feats.append(\n                pd.concat([b[1] for b in batch])\n                .reset_index(drop=False)\n                .rename(columns={\"index\": \"uid\"})\n            )  # concatenate the nuclear feature patch dfs\n\nnuc_feats = pd.concat(nuc_feats, axis=0)\nnuc_feats.head(10)\n</pre> import pandas as pd from tqdm import tqdm  crop_feats = [] nuc_feats = [] with crop_loader as loader:     with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:         for batch_idx, batch in enumerate(pbar):             crop_feats.extend(batch)  # collect the patch level dfs             nuc_feats.append(                 pd.concat([b[1] for b in batch])                 .reset_index(drop=False)                 .rename(columns={\"index\": \"uid\"})             )  # concatenate the nuclear feature patch dfs  nuc_feats = pd.concat(nuc_feats, axis=0) nuc_feats.head(10) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1216/1216 [02:29&lt;00:00,  8.14batch/s]\n</pre> Out[5]: uid chrom_area chrom_nuc_prop 0 232257 37 0.948718 1 232263 232 0.720497 2 232268 335 0.865633 3 232270 327 0.918539 4 232273 0 0.000000 5 232279 545 0.718997 6 232280 41 0.976190 7 232282 143 0.910828 8 232284 221 0.969298 9 232288 225 0.937500 <p>Let's next take area weighted mean of the <code>chrom_nuc_prop</code> and the total area for nuclei that were split by the different crops/patches.</p> In\u00a0[6]: Copied! <pre>def compute_area_weighted_stats(group):\n    \"\"\"Compute area-weighted statistics for split nuclei.\"\"\"\n    total_area = group[\"chrom_area\"].sum()\n\n    if total_area == 0:\n        return pd.Series({\"chrom_nuc_prop\": 0, \"chrom_area\": 0})\n\n    # Area-weighted mean\n    weighted_mean = (group[\"chrom_nuc_prop\"] * group[\"chrom_area\"]).sum() / total_area\n    return pd.Series({\"chrom_nuc_prop\": weighted_mean, \"chrom_area\": total_area})\n\n\n# Apply the function\nnuc_feats = nuc_feats.groupby(\"uid\").apply(\n    compute_area_weighted_stats, include_groups=False\n)\n\nnuc_feats.head(10)\n</pre> def compute_area_weighted_stats(group):     \"\"\"Compute area-weighted statistics for split nuclei.\"\"\"     total_area = group[\"chrom_area\"].sum()      if total_area == 0:         return pd.Series({\"chrom_nuc_prop\": 0, \"chrom_area\": 0})      # Area-weighted mean     weighted_mean = (group[\"chrom_nuc_prop\"] * group[\"chrom_area\"]).sum() / total_area     return pd.Series({\"chrom_nuc_prop\": weighted_mean, \"chrom_area\": total_area})   # Apply the function nuc_feats = nuc_feats.groupby(\"uid\").apply(     compute_area_weighted_stats, include_groups=False )  nuc_feats.head(10) Out[6]: chrom_nuc_prop chrom_area uid 1 0.407850 239.0 2 0.482718 798.0 3 0.042517 15.0 4 0.231628 199.0 5 0.606154 360.0 6 0.620008 360.0 8 0.342438 259.0 10 0.950882 328.0 11 0.760580 834.0 12 0.298109 1071.0 In\u00a0[7]: Copied! <pre>from histolytics.utils.plot import legendgram\n\nneo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"]\nneo_nuc = neo_nuc.merge(nuc_feats, left_index=True, right_index=True)\n\nax = tis.plot(\n    column=\"class_name\", figsize=(10, 10), alpha=0.1, legend=True, cmap=\"Set2\"\n)\nax = neo_nuc.plot(ax=ax, column=\"chrom_nuc_prop\", cmap=\"turbo\", legend=False)\nax.set_axis_off()\n\nax = legendgram(\n    gdf=neo_nuc,\n    column=\"chrom_nuc_prop\",\n    n_bins=50,\n    cmap=\"turbo\",\n    ax=ax,\n    loc=\"lower right\",\n)\n</pre> from histolytics.utils.plot import legendgram  neo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"] neo_nuc = neo_nuc.merge(nuc_feats, left_index=True, right_index=True)  ax = tis.plot(     column=\"class_name\", figsize=(10, 10), alpha=0.1, legend=True, cmap=\"Set2\" ) ax = neo_nuc.plot(ax=ax, column=\"chrom_nuc_prop\", cmap=\"turbo\", legend=False) ax.set_axis_off()  ax = legendgram(     gdf=neo_nuc,     column=\"chrom_nuc_prop\",     n_bins=50,     cmap=\"turbo\",     ax=ax,     loc=\"lower right\", ) <p>From the plot above we can see that there are distinct regions where the chromatin-clump-to-nuclei proportion is high (red) indicating areas of nuclei with possible hyperchromatination. These potentially hyperchromatinated nuclei tend to localize often around the borders of tumor masses whereas within the solid tumor masses, the nuclei demonstrate moderate-levels of chromatin clumping (greenish/yellowish color) indicating a more heterogeneous chromatin structure.</p> <p>Let's plot next the chromatin-to-nuclei-proportions at the patch level:</p> In\u00a0[8]: Copied! <pre># Compute weighted column means based on DataFrame size\npatch_means = []\nn_nuc = []\n\nfor idx, df in crop_feats:\n    if not df.empty:\n        means = df.mean(numeric_only=True)\n        means.name = idx\n\n        # get also the number of nuclei so we can weight the feature values\n        n_nuclei = len(df)\n        patch_means.append(means)\n        n_nuc.append(n_nuclei)\n\n# Create DataFrame with means and weights\npatch_means_df = pd.DataFrame(patch_means)\npatch_means_df.index.name = \"uid\"\n\npatch_means_df[\"n_nuclei\"] = n_nuc\n\n# Create weights for the rows, based on the number of nuclei in a patch\npatch_means_df[\"weight\"] = (patch_means_df[\"n_nuclei\"] / 50).clip(upper=1.0)\npatch_means_df[\"weighted_chrom_prop\"] = (\n    patch_means_df[\"chrom_nuc_prop\"] * patch_means_df[\"weight\"]\n)\npatch_means_df.head()\n</pre> # Compute weighted column means based on DataFrame size patch_means = [] n_nuc = []  for idx, df in crop_feats:     if not df.empty:         means = df.mean(numeric_only=True)         means.name = idx          # get also the number of nuclei so we can weight the feature values         n_nuclei = len(df)         patch_means.append(means)         n_nuc.append(n_nuclei)  # Create DataFrame with means and weights patch_means_df = pd.DataFrame(patch_means) patch_means_df.index.name = \"uid\"  patch_means_df[\"n_nuclei\"] = n_nuc  # Create weights for the rows, based on the number of nuclei in a patch patch_means_df[\"weight\"] = (patch_means_df[\"n_nuclei\"] / 50).clip(upper=1.0) patch_means_df[\"weighted_chrom_prop\"] = (     patch_means_df[\"chrom_nuc_prop\"] * patch_means_df[\"weight\"] ) patch_means_df.head() Out[8]: chrom_area chrom_nuc_prop n_nuclei weight weighted_chrom_prop uid 0 221.388889 0.831905 18 0.36 0.299486 1 256.204545 0.740770 44 0.88 0.651877 2 213.126316 0.572740 95 1.00 0.572740 3 217.245455 0.546525 110 1.00 0.546525 4 243.718447 0.518922 103 1.00 0.518922 In\u00a0[9]: Copied! <pre>grid_feats = gr.merge(patch_means_df, left_index=True, right_index=True)\ngrid_feats[\"geometry\"] = grid_feats.geometry.boundary\ngrid_feats.head()\n</pre> grid_feats = gr.merge(patch_means_df, left_index=True, right_index=True) grid_feats[\"geometry\"] = grid_feats.geometry.boundary grid_feats.head() Out[9]: geometry chrom_area chrom_nuc_prop n_nuclei weight weighted_chrom_prop 0 LINESTRING (35819 67563, 36331 67563, 36331 68... 221.388889 0.831905 18 0.36 0.299486 1 LINESTRING (36331 67563, 36843 67563, 36843 68... 256.204545 0.740770 44 0.88 0.651877 2 LINESTRING (36843 67563, 37355 67563, 37355 68... 213.126316 0.572740 95 1.00 0.572740 3 LINESTRING (37355 67563, 37867 67563, 37867 68... 217.245455 0.546525 110 1.00 0.546525 4 LINESTRING (37867 67563, 38379 67563, 38379 68... 243.718447 0.518922 103 1.00 0.518922 In\u00a0[10]: Copied! <pre>from histolytics.utils.plot import legendgram\n\nax = tis.plot(\n    column=\"class_name\", figsize=(10, 10), alpha=0.1, legend=True, cmap=\"Set2\"\n)\nax = neo_nuc.plot(ax=ax, column=\"class_name\", legend=False, alpha=0.3)\nax = grid_feats.plot(ax=ax, column=\"chrom_nuc_prop\", cmap=\"turbo\", legend=False, lw=0.5)\nax.set_axis_off()\n\nax = legendgram(\n    gdf=grid_feats,\n    column=\"chrom_nuc_prop\",\n    n_bins=50,\n    cmap=\"turbo\",\n    ax=ax,\n    loc=\"lower right\",\n)\n</pre> from histolytics.utils.plot import legendgram  ax = tis.plot(     column=\"class_name\", figsize=(10, 10), alpha=0.1, legend=True, cmap=\"Set2\" ) ax = neo_nuc.plot(ax=ax, column=\"class_name\", legend=False, alpha=0.3) ax = grid_feats.plot(ax=ax, column=\"chrom_nuc_prop\", cmap=\"turbo\", legend=False, lw=0.5) ax.set_axis_off()  ax = legendgram(     gdf=grid_feats,     column=\"chrom_nuc_prop\",     n_bins=50,     cmap=\"turbo\",     ax=ax,     loc=\"lower right\", ) <p>Here we see that most of the patches with high chromatin-to-nuclei proportions are located at the very edges of tumor regions, while patches with low proportions are found in the center. However, when we naively average the patches, we might lose a lot of information. For example, patches with only a few nuclei often stand out as having very high or very low chromatin-to-nuclei proportions. But if we consider the weighted average, taking into account the number of nuclei in each patch, we can retain more information about the distribution of chromatin states across the tissue:</p> In\u00a0[11]: Copied! <pre>from histolytics.utils.plot import legendgram\n\nax = tis.plot(\n    column=\"class_name\", figsize=(10, 10), alpha=0.1, legend=True, cmap=\"Set2\"\n)\nax = neo_nuc.plot(ax=ax, column=\"class_name\", legend=False, alpha=0.3)\nax = grid_feats.plot(\n    ax=ax, column=\"weighted_chrom_prop\", cmap=\"turbo\", legend=False, lw=0.5\n)\nax.set_axis_off()\n\nax = legendgram(\n    gdf=grid_feats,\n    column=\"weighted_chrom_prop\",\n    n_bins=50,\n    cmap=\"turbo\",\n    ax=ax,\n    loc=\"lower right\",\n)\n</pre> from histolytics.utils.plot import legendgram  ax = tis.plot(     column=\"class_name\", figsize=(10, 10), alpha=0.1, legend=True, cmap=\"Set2\" ) ax = neo_nuc.plot(ax=ax, column=\"class_name\", legend=False, alpha=0.3) ax = grid_feats.plot(     ax=ax, column=\"weighted_chrom_prop\", cmap=\"turbo\", legend=False, lw=0.5 ) ax.set_axis_off()  ax = legendgram(     gdf=grid_feats,     column=\"weighted_chrom_prop\",     n_bins=50,     cmap=\"turbo\",     ax=ax,     loc=\"lower right\", ) <p>Now this looks a bit better!</p> In\u00a0[12]: Copied! <pre>import matplotlib.pyplot as plt\n\ngrid_feats_sorted = grid_feats.sort_values(\"weighted_chrom_prop\")\ngrid_feats_sorted = grid_feats_sorted[grid_feats_sorted[\"n_nuclei\"] &gt; 60]\n\nlow_patches = grid_feats_sorted.head(6)\nhigh_patches = grid_feats_sorted.tail(6)\n</pre> import matplotlib.pyplot as plt  grid_feats_sorted = grid_feats.sort_values(\"weighted_chrom_prop\") grid_feats_sorted = grid_feats_sorted[grid_feats_sorted[\"n_nuclei\"] &gt; 60]  low_patches = grid_feats_sorted.head(6) high_patches = grid_feats_sorted.tail(6) In\u00a0[13]: Copied! <pre>def polygon_to_xywh(polygon):\n    \"\"\"Convert polygon to xywh coordinates for slide reader.\"\"\"\n    minx, miny, maxx, maxy = polygon.bounds\n    return (int(minx), int(miny), int(maxx - minx), int(maxy - miny))\n\n\ndef extract_patch_images(patches, reader):\n    \"\"\"Extract images from WSI for given patches.\"\"\"\n    images = []\n    patch_info = []\n\n    for idx, _ in patches.iterrows():\n        original_geom = gr.loc[idx, \"geometry\"]\n        x, y, w, h = polygon_to_xywh(original_geom)\n        img = reader.read_region((int(x), int(y), int(w), int(h)), 0)\n        images.append(img)\n\n    return images, patch_info\n\n\n# Extract images for low and high chromatin patches\nlow_images, low_info = extract_patch_images(low_patches, reader)\nhigh_images, high_info = extract_patch_images(high_patches, reader)\n</pre> def polygon_to_xywh(polygon):     \"\"\"Convert polygon to xywh coordinates for slide reader.\"\"\"     minx, miny, maxx, maxy = polygon.bounds     return (int(minx), int(miny), int(maxx - minx), int(maxy - miny))   def extract_patch_images(patches, reader):     \"\"\"Extract images from WSI for given patches.\"\"\"     images = []     patch_info = []      for idx, _ in patches.iterrows():         original_geom = gr.loc[idx, \"geometry\"]         x, y, w, h = polygon_to_xywh(original_geom)         img = reader.read_region((int(x), int(y), int(w), int(h)), 0)         images.append(img)      return images, patch_info   # Extract images for low and high chromatin patches low_images, low_info = extract_patch_images(low_patches, reader) high_images, high_info = extract_patch_images(high_patches, reader) In\u00a0[14]: Copied! <pre># Create visualization grid\nfig, axes = plt.subplots(4, 3, figsize=(12, 12))\n\n# Plot low chromatin patches (top 2 rows)\nfor i, img in enumerate(low_images):\n    row = i // 3\n    col = i % 3\n\n    ax = axes[row, col]\n    ax.imshow(img)\n    ax.axis(\"off\")\n\n# Plot high chromatin patches (bottom 2 rows)\nfor i, img in enumerate(high_images):\n    row = 2 + i // 3\n    col = i % 3\n\n    ax = axes[row, col]\n    ax.imshow(img)\n    ax.axis(\"off\")\n</pre> # Create visualization grid fig, axes = plt.subplots(4, 3, figsize=(12, 12))  # Plot low chromatin patches (top 2 rows) for i, img in enumerate(low_images):     row = i // 3     col = i % 3      ax = axes[row, col]     ax.imshow(img)     ax.axis(\"off\")  # Plot high chromatin patches (bottom 2 rows) for i, img in enumerate(high_images):     row = 2 + i // 3     col = i % 3      ax = axes[row, col]     ax.imshow(img)     ax.axis(\"off\")  <p>From the above plot we see that the low chromatin-clump-to-nuclei proportion patches (two first rows) show neoplastic nuclei where the chromatin is clearly clumped and distributed in a more dispersed manner. Some of the nuclei exhibit prominent nucleoli, either single or multiple (multinucleation). In contrast, the high chromatin-clump-to-nuclei proportion patches (two last rows) exhibit a more compact chromatin pattern, with less clumping observed. The broadly distributed and dark stain of the chromatin indicate hyperchromatic neoplastic nuclei.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/workflows/chromatin_patterns/#introduction","title":"Introduction\u00b6","text":"<p>Extracting and quantifying chromatin patterns from routine Hematoxylin and Eosin (H&amp;E) stained WSIs offers important insights into nuclear architecture, an important indicator of cellular state that undergoes profound alterations in various disease pathologies, particularly cancer. This analysis workflow provides an example of how Histolytics can be used to extract and quantify chromatin clumping from H&amp;E stained images. Specifically, we will focus on the computation of the chromatin clump area and the proportion of chromatin clump area to the total nuclear area since these features are easily interpretable. In general a low chromatin clump-to-nucleus area proportion indicates dispersed chromatin, while a higher proportion can reflect increased chromatin condensation or hyperchromatism\u2014hallmarks often associated with malignancy. If you are interested in other chromatin distribution related features, refer to the nuclear features tutorial.</p> <p>We will use a WSI from that we segmented in the panoptic segmentation example as our input data. We will also be focusing solely on the neoplastic nuclei in this tutorial.</p>"},{"location":"user_guide/workflows/chromatin_patterns/#determining-grid-for-feature-extraction","title":"Determining Grid for Feature Extraction\u00b6","text":"<p>Since we will extract features from the underlying WSI rgb-data, we also need to determine a grid where the WSI will be analyzed. This grid will help us to systematically extract features at the WSI-level.</p>"},{"location":"user_guide/workflows/chromatin_patterns/#initialize-wsi-reader","title":"Initialize WSI Reader\u00b6","text":"<p>Next, we'll initialize the WSI reader with the slide path and the desired backend. This Reader is used to access the underlying image data and perform operations at WSI-scale.</p>"},{"location":"user_guide/workflows/chromatin_patterns/#initialize-wsigridprocessor","title":"Initialize WSIGridProcessor\u00b6","text":"<p>In Histolytics, the <code>WSIGridProcessor</code> is designed to facilitate the processing of whole slide images (WSIs), when the underlying image data is needed for extracting features. The class is a context manager that handles the setup and teardown of the WSI processing environment. It takes in the initialized grid, the <code>SlideReader</code> instance, and the WSI-scale nuclei and tissue segmentation maps. Also, you must pass a pipeline Callable to the processor, this pipeline will be applied to each patch of the WSI as it is processed. The pipeline function should accept the arguments <code>img: np.ndarray</code>, <code>label: np.ndarray</code>, <code>mask: np.ndarray</code>, where the img is the cropped image from the WSI, label is to cropped nuclei instance label mask from the nuclei segmentation map, and mask is the binary mask indicating the region of interest from the tissue segmentation map. We will be focusing only on neoplastic nuclei so there is no need to mask out regions in the inputs.</p>"},{"location":"user_guide/workflows/chromatin_patterns/#process-the-wsi","title":"Process the WSI\u00b6","text":"<p>We will be using 8 cpus to loop over the grid cells. On a regular laptop this should take around two to three minutes to process. We will save the features at patch level and nuclei level. The returned batch from the data loader will contain a list of tuples where the tuple first element is the crop index and the second element is a DataFrame containing the nuclear-level features for that crop.</p>"},{"location":"user_guide/workflows/chromatin_patterns/#plot-the-chromatin-clump-to-nuclei-proportions","title":"Plot the Chromatin-clump-to-nuclei-proportions\u00b6","text":"<p>And then some plotting at nuclear level.</p>"},{"location":"user_guide/workflows/chromatin_patterns/#visualizing-he-patches","title":"Visualizing H&amp;E patches\u00b6","text":"<p>Let's now visualize some H&amp;E crops to see what the chromatin-clumping actually looks like. Let's first select some patches with high and low chromatin clump proportions.</p>"},{"location":"user_guide/workflows/chromatin_patterns/#conclusions","title":"Conclusions\u00b6","text":"<p>In this tutorial, we demonstrated how to extract and quantify chromatin patterns from H&amp;E stained WSIs using Histolytics. Specifically, we focused on the neoplastic nuclei, for which computed features such as the chromatin clump area and the proportion of chromatin clump area to the total nuclear area. We used these features to characterize the chromatin architecture of neoplastic nuclei and explored how nuclei with differences in their chromatin organization distribute within the slide.</p>"},{"location":"user_guide/workflows/clustering_desmoplasia/","title":"Clustering Desmoplastic Stroma","text":"In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\n\nnuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path\ntis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", legend=True)\nax.set_axis_off()\n</pre> import geopandas as gpd  nuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path tis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path  ax = tis.plot(figsize=(10, 10), column=\"class_name\", legend=True) ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.spatial_ops.rect_grid import rect_grid\n\n# fit the grid. We'll use 256, 256 sized patches, focus only on stroma\npatch_size = (256, 256)\ngr = rect_grid(tis, patch_size, 0, \"intersects\")\nax = tis.plot(column=\"class_name\", figsize=(10, 10), alpha=0.5, legend=True)\nax = gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=0.5)\nax.set_axis_off()\n</pre> from histolytics.spatial_ops.rect_grid import rect_grid  # fit the grid. We'll use 256, 256 sized patches, focus only on stroma patch_size = (256, 256) gr = rect_grid(tis, patch_size, 0, \"intersects\") ax = tis.plot(column=\"class_name\", figsize=(10, 10), alpha=0.5, legend=True) ax = gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=0.5) ax.set_axis_off() <p>Let's now filter out the other tissue section and constrain the bboxes to the stromal regions.</p> In\u00a0[3]: Copied! <pre>from histolytics.wsi.utils import get_sub_grids\nfrom histolytics.spatial_ops.ops import get_objs\n\nsub_grids = get_sub_grids(gr, min_size=1000, return_gdf=True)\ngr = get_objs(tis.loc[tis[\"class_name\"] == \"stroma\"], sub_grids[1], \"contains\")\ngr = gr.reset_index(drop=True)\n\nax = tis.plot(column=\"class_name\", aspect=1, figsize=(10, 10), legend=True)\nax = gr.boundary.plot(ax=ax, lw=0.5, color=\"red\")\nax.set_axis_off()\n</pre> from histolytics.wsi.utils import get_sub_grids from histolytics.spatial_ops.ops import get_objs  sub_grids = get_sub_grids(gr, min_size=1000, return_gdf=True) gr = get_objs(tis.loc[tis[\"class_name\"] == \"stroma\"], sub_grids[1], \"contains\") gr = gr.reset_index(drop=True)  ax = tis.plot(column=\"class_name\", aspect=1, figsize=(10, 10), legend=True) ax = gr.boundary.plot(ax=ax, lw=0.5, color=\"red\") ax.set_axis_off() In\u00a0[4]: Copied! <pre># Visualize the grid on top of the slide\n# thumbnail = reader.read_level(-2)\n# reader.get_annotated_thumbnail(thumbnail, gr, linewidth=1)\n</pre> # Visualize the grid on top of the slide # thumbnail = reader.read_level(-2) # reader.get_annotated_thumbnail(thumbnail, gr, linewidth=1) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom histolytics.wsi.wsi_processor import WSIGridProcessor\nfrom histolytics.stroma_feats.collagen import fiber_feats\nfrom histolytics.stroma_feats.intensity import stromal_intensity_feats\nfrom histolytics.wsi.slide_reader import SlideReader\n\n\ndef pipeline(img: np.ndarray, label: np.ndarray, mask: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"A pipeline for extracting features from WSI stromal patches.\"\"\"\n    collagen_feats: gpd.GeoDataFrame = fiber_feats(\n        img,\n        label=label,\n        mask=mask,\n        metrics=(\"major_axis_angle\", \"tortuosity\"),\n        rm_bg=True,\n        rm_fg=False,\n        return_edges=False,\n    )\n\n    if len(collagen_feats) &lt; 3:\n        fiber_orientation_std = 0\n        tortuosity_mean = 0\n    else:\n        fiber_orientation_std = collagen_feats[\"major_axis_angle\"].std()\n        tortuosity_mean = collagen_feats[\"tortuosity\"].mean()\n\n    collagen_feats = pd.Series(\n        {\n            \"fiber_orientation_std\": fiber_orientation_std,\n            \"tortuosity_mean\": tortuosity_mean,\n        }\n    )\n\n    stromal_feats: pd.Series = stromal_intensity_feats(\n        img, label=label, mask=mask, metrics=[\"mean\"]\n    )\n    combined_feats = pd.concat([collagen_feats, stromal_feats], axis=0)\n    return combined_feats\n\n\nsl_p = \"/path/to/slide.mrxs\"  # &lt;- modify path\nreader = SlideReader(sl_p, backend=\"OPENSLIDE\")\n\ncrop_loader = WSIGridProcessor(\n    slide_reader=reader,\n    grid=gr,\n    nuclei=nuc,\n    pipeline_func=pipeline,\n    batch_size=8,\n    num_workers=8,\n    pin_memory=False,\n    shuffle=False,\n    drop_last=False,\n)\n</pre> import numpy as np import pandas as pd from histolytics.wsi.wsi_processor import WSIGridProcessor from histolytics.stroma_feats.collagen import fiber_feats from histolytics.stroma_feats.intensity import stromal_intensity_feats from histolytics.wsi.slide_reader import SlideReader   def pipeline(img: np.ndarray, label: np.ndarray, mask: np.ndarray) -&gt; pd.DataFrame:     \"\"\"A pipeline for extracting features from WSI stromal patches.\"\"\"     collagen_feats: gpd.GeoDataFrame = fiber_feats(         img,         label=label,         mask=mask,         metrics=(\"major_axis_angle\", \"tortuosity\"),         rm_bg=True,         rm_fg=False,         return_edges=False,     )      if len(collagen_feats) &lt; 3:         fiber_orientation_std = 0         tortuosity_mean = 0     else:         fiber_orientation_std = collagen_feats[\"major_axis_angle\"].std()         tortuosity_mean = collagen_feats[\"tortuosity\"].mean()      collagen_feats = pd.Series(         {             \"fiber_orientation_std\": fiber_orientation_std,             \"tortuosity_mean\": tortuosity_mean,         }     )      stromal_feats: pd.Series = stromal_intensity_feats(         img, label=label, mask=mask, metrics=[\"mean\"]     )     combined_feats = pd.concat([collagen_feats, stromal_feats], axis=0)     return combined_feats   sl_p = \"/path/to/slide.mrxs\"  # &lt;- modify path reader = SlideReader(sl_p, backend=\"OPENSLIDE\")  crop_loader = WSIGridProcessor(     slide_reader=reader,     grid=gr,     nuclei=nuc,     pipeline_func=pipeline,     batch_size=8,     num_workers=8,     pin_memory=False,     shuffle=False,     drop_last=False, ) <p>Let's now iterate over the cropped patches and apply the feature extraction pipeline. This takes a few minutes on a regular laptop.</p> In\u00a0[6]: Copied! <pre>import pandas as pd\nfrom tqdm import tqdm\n\ncrop_feats = []\nwith crop_loader as loader:\n    with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:\n        for batch_idx, batch in enumerate(pbar):\n            crop_feats.extend(batch)  # collect the patch level dfs\n</pre> import pandas as pd from tqdm import tqdm  crop_feats = [] with crop_loader as loader:     with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:         for batch_idx, batch in enumerate(pbar):             crop_feats.extend(batch)  # collect the patch level dfs  <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 718/718 [05:07&lt;00:00,  2.33batch/s]\n</pre> In\u00a0[7]: Copied! <pre>crop_feats_df = pd.DataFrame([series for _, series in crop_feats])\ncrop_feats_df.index = [idx for idx, _ in crop_feats]\ncrop_feats_df.index.name = \"uid\"\n\n# merge the grid gdf and the feature df\ngrid_feats = gr.merge(crop_feats_df, left_index=True, right_index=True)\ngrid_feats[\"geometry\"] = grid_feats.geometry.boundary\ngrid_feats.head(3)\n</pre> crop_feats_df = pd.DataFrame([series for _, series in crop_feats]) crop_feats_df.index = [idx for idx, _ in crop_feats] crop_feats_df.index.name = \"uid\"  # merge the grid gdf and the feature df grid_feats = gr.merge(crop_feats_df, left_index=True, right_index=True) grid_feats[\"geometry\"] = grid_feats.geometry.boundary grid_feats.head(3) Out[7]: geometry fiber_orientation_std tortuosity_mean hematoxylin_area hematoxylin_R_mean hematoxylin_G_mean hematoxylin_B_mean eosin_area eosin_R_mean eosin_G_mean eosin_B_mean 0 LINESTRING (6726 67466, 6726 67722, 6470 67722... 20.107610 1.469182 1657.0 0.750142 0.734647 0.876184 2603.0 0.972066 0.681243 0.956540 1 LINESTRING (7494 67466, 7494 67722, 7238 67722... 25.209740 1.704413 13925.0 0.775036 0.760802 0.889430 42164.0 0.969307 0.648682 0.952238 2 LINESTRING (7750 67466, 7750 67722, 7494 67722... 25.403968 1.683213 23361.0 0.780649 0.766445 0.893465 31992.0 0.978101 0.733881 0.965822 <p>Let's do a quick visualization to see how a feature looks like on the segmented slide.</p> In\u00a0[8]: Copied! <pre>ax = tis.plot(column=\"class_name\", aspect=1, figsize=(10, 10))\nax = grid_feats.plot(\n    ax=ax, column=\"fiber_orientation_std\", legend=True, lw=0.5, cmap=\"jet\"\n)\nax.set_axis_off()\n</pre> ax = tis.plot(column=\"class_name\", aspect=1, figsize=(10, 10)) ax = grid_feats.plot(     ax=ax, column=\"fiber_orientation_std\", legend=True, lw=0.5, cmap=\"jet\" ) ax.set_axis_off() In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn import set_config\n\n\nset_config(transform_output=\"pandas\")  # return pandas dataframes\n\n\ndef export_legend(legend, filename=\"legend.png\", expand=[-5, -5, 5, 5]):\n    fig = legend.figure\n    fig.canvas.draw()\n    bbox = legend.get_window_extent()\n    bbox = bbox.from_extents(*(bbox.extents + np.array(expand)))\n    bbox = bbox.transformed(fig.dpi_scale_trans.inverted())\n    fig.savefig(filename, dpi=\"figure\", bbox_inches=bbox)\n\n\n# helper function to calculate Silhouette score for a range of clusters\ndef get_silhouette(range: range, clust_vars):\n    sil_score = []\n    for i in range:\n        kmeans = KMeans(n_clusters=i, random_state=42)\n        kmeans.fit(clust_vars)\n        labels = kmeans.labels_\n\n        # calculate Silhouette score\n        score = silhouette_score(\n            clust_vars, labels, metric=\"euclidean\", sample_size=1000, random_state=200\n        )\n        sil_score.append(score)\n\n    return sil_score\n\n\n# drop the geometry column for clustering\ncols = grid_feats.columns\nclust_vars = grid_feats.loc[:, ~cols.isin([\"geometry\"])]\n\n# scale the features\ncols = clust_vars.columns\nclust_vars = minmax_scale(clust_vars)\nclust_vars.columns = cols\nclust_vars = clust_vars.fillna(0)\n\nsil_score = get_silhouette(\n    range(3, 12),\n    clust_vars,\n)\n\n# select the highest silhouette score\nn_clust = range(3, 12)[np.argmax(sil_score)]\nplt.plot(range(3, 12), sil_score, label=f\"Optimal number of clusters: {n_clust}\")\n</pre> import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score from sklearn.preprocessing import minmax_scale from sklearn import set_config   set_config(transform_output=\"pandas\")  # return pandas dataframes   def export_legend(legend, filename=\"legend.png\", expand=[-5, -5, 5, 5]):     fig = legend.figure     fig.canvas.draw()     bbox = legend.get_window_extent()     bbox = bbox.from_extents(*(bbox.extents + np.array(expand)))     bbox = bbox.transformed(fig.dpi_scale_trans.inverted())     fig.savefig(filename, dpi=\"figure\", bbox_inches=bbox)   # helper function to calculate Silhouette score for a range of clusters def get_silhouette(range: range, clust_vars):     sil_score = []     for i in range:         kmeans = KMeans(n_clusters=i, random_state=42)         kmeans.fit(clust_vars)         labels = kmeans.labels_          # calculate Silhouette score         score = silhouette_score(             clust_vars, labels, metric=\"euclidean\", sample_size=1000, random_state=200         )         sil_score.append(score)      return sil_score   # drop the geometry column for clustering cols = grid_feats.columns clust_vars = grid_feats.loc[:, ~cols.isin([\"geometry\"])]  # scale the features cols = clust_vars.columns clust_vars = minmax_scale(clust_vars) clust_vars.columns = cols clust_vars = clust_vars.fillna(0)  sil_score = get_silhouette(     range(3, 12),     clust_vars, )  # select the highest silhouette score n_clust = range(3, 12)[np.argmax(sil_score)] plt.plot(range(3, 12), sil_score, label=f\"Optimal number of clusters: {n_clust}\") Out[9]: <pre>[&lt;matplotlib.lines.Line2D at 0x782bd2194560&gt;]</pre> <p>And then we'll cluster!</p> In\u00a0[10]: Copied! <pre>kmeans = KMeans(n_clusters=n_clust, random_state=42)\nkmeans.fit(clust_vars)\ngrid_feats[\"labels\"] = kmeans.labels_\ngrid_feats[\"labels\"] = grid_feats[\"labels\"] + 1\ngrid_feats[\"labels\"] = grid_feats[\"labels\"].apply(lambda x: f\"cluster {x}\")\ngrid_feats.head(3)\n</pre> kmeans = KMeans(n_clusters=n_clust, random_state=42) kmeans.fit(clust_vars) grid_feats[\"labels\"] = kmeans.labels_ grid_feats[\"labels\"] = grid_feats[\"labels\"] + 1 grid_feats[\"labels\"] = grid_feats[\"labels\"].apply(lambda x: f\"cluster {x}\") grid_feats.head(3) Out[10]: geometry fiber_orientation_std tortuosity_mean hematoxylin_area hematoxylin_R_mean hematoxylin_G_mean hematoxylin_B_mean eosin_area eosin_R_mean eosin_G_mean eosin_B_mean labels 0 LINESTRING (6726 67466, 6726 67722, 6470 67722... 20.107610 1.469182 1657.0 0.750142 0.734647 0.876184 2603.0 0.972066 0.681243 0.956540 cluster 3 1 LINESTRING (7494 67466, 7494 67722, 7238 67722... 25.209740 1.704413 13925.0 0.775036 0.760802 0.889430 42164.0 0.969307 0.648682 0.952238 cluster 1 2 LINESTRING (7750 67466, 7750 67722, 7494 67722... 25.403968 1.683213 23361.0 0.780649 0.766445 0.893465 31992.0 0.978101 0.733881 0.965822 cluster 1 In\u00a0[11]: Copied! <pre>ax = tis.plot(figsize=(10, 10), alpha=0.2, column=\"class_name\", aspect=1)\nax = grid_feats.plot(ax=ax, column=\"labels\", aspect=1, legend=True, cmap=\"Set2_r\")\nax.set_axis_off()\n</pre> ax = tis.plot(figsize=(10, 10), alpha=0.2, column=\"class_name\", aspect=1) ax = grid_feats.plot(ax=ax, column=\"labels\", aspect=1, legend=True, cmap=\"Set2_r\") ax.set_axis_off() In\u00a0[12]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef polygon_to_xywh(polygon):\n    \"\"\"Convert polygon to xywh coordinates for slide reader.\"\"\"\n    minx, miny, maxx, maxy = polygon.bounds\n    return (int(minx), int(miny), int(maxx - minx), int(maxy - miny))\n\n\ndef extract_cluster_images(cluster_patches_dict, reader):\n    \"\"\"Extract images for all clusters.\"\"\"\n    cluster_images = {}\n\n    for cluster, patches in cluster_patches_dict.items():\n        images = []\n        patch_info = []\n\n        for idx, _ in patches.iterrows():\n            original_geom = gr.loc[idx, \"geometry\"]\n            x, y, w, h = polygon_to_xywh(original_geom)\n\n            img = reader.read_region((int(x), int(y), int(w), int(h)), 0)\n            images.append(img)\n            patch_info.append({\"idx\": idx, \"cluster\": cluster, \"coords\": (x, y, w, h)})\n\n        cluster_images[cluster] = {\"images\": images, \"info\": patch_info}\n\n    return cluster_images\n\n\n# Get unique cluster labels\nunique_clusters = np.unique(grid_feats[\"labels\"])\n\n# Sample patches from each cluster (2-3 per cluster)\npatches_per_cluster = 3\ncluster_patches = {}\n\nfor cluster in unique_clusters:\n    cluster_data = grid_feats[grid_feats[\"labels\"] == cluster]\n    sampled = cluster_data.sample(n=patches_per_cluster, random_state=45)\n    cluster_patches[cluster] = sampled\n\ncluster_images = extract_cluster_images(cluster_patches, reader)\n</pre> import matplotlib.pyplot as plt import numpy as np   def polygon_to_xywh(polygon):     \"\"\"Convert polygon to xywh coordinates for slide reader.\"\"\"     minx, miny, maxx, maxy = polygon.bounds     return (int(minx), int(miny), int(maxx - minx), int(maxy - miny))   def extract_cluster_images(cluster_patches_dict, reader):     \"\"\"Extract images for all clusters.\"\"\"     cluster_images = {}      for cluster, patches in cluster_patches_dict.items():         images = []         patch_info = []          for idx, _ in patches.iterrows():             original_geom = gr.loc[idx, \"geometry\"]             x, y, w, h = polygon_to_xywh(original_geom)              img = reader.read_region((int(x), int(y), int(w), int(h)), 0)             images.append(img)             patch_info.append({\"idx\": idx, \"cluster\": cluster, \"coords\": (x, y, w, h)})          cluster_images[cluster] = {\"images\": images, \"info\": patch_info}      return cluster_images   # Get unique cluster labels unique_clusters = np.unique(grid_feats[\"labels\"])  # Sample patches from each cluster (2-3 per cluster) patches_per_cluster = 3 cluster_patches = {}  for cluster in unique_clusters:     cluster_data = grid_feats[grid_feats[\"labels\"] == cluster]     sampled = cluster_data.sample(n=patches_per_cluster, random_state=45)     cluster_patches[cluster] = sampled  cluster_images = extract_cluster_images(cluster_patches, reader) In\u00a0[13]: Copied! <pre># Visualize patches from all clusters in a grid\nn_clusters = len(cluster_images)\nmax_patches = max([len(data[\"images\"]) for data in cluster_images.values()])\n\nfig, axes = plt.subplots(n_clusters, max_patches, figsize=(10, 10))\n\nfor cluster_idx, (cluster, data) in enumerate(cluster_images.items()):\n    images = data[\"images\"]\n    info = data[\"info\"]\n\n    for patch_idx, (img, patch_info) in enumerate(zip(images, info)):\n        ax = axes[cluster_idx, patch_idx]\n        ax.imshow(img)\n        ax.set_title(f\"{cluster}\\nPatch {patch_info['idx']}\", fontsize=10)\n        ax.axis(\"off\")\n\n    for patch_idx in range(len(images), max_patches):\n        axes[cluster_idx, patch_idx].axis(\"off\")\n\nplt.tight_layout()\nplt.suptitle(\"H&amp;E Patches by Cluster\", fontsize=16, y=1.02)\nplt.show()\n</pre> # Visualize patches from all clusters in a grid n_clusters = len(cluster_images) max_patches = max([len(data[\"images\"]) for data in cluster_images.values()])  fig, axes = plt.subplots(n_clusters, max_patches, figsize=(10, 10))  for cluster_idx, (cluster, data) in enumerate(cluster_images.items()):     images = data[\"images\"]     info = data[\"info\"]      for patch_idx, (img, patch_info) in enumerate(zip(images, info)):         ax = axes[cluster_idx, patch_idx]         ax.imshow(img)         ax.set_title(f\"{cluster}\\nPatch {patch_info['idx']}\", fontsize=10)         ax.axis(\"off\")      for patch_idx in range(len(images), max_patches):         axes[cluster_idx, patch_idx].axis(\"off\")  plt.tight_layout() plt.suptitle(\"H&amp;E Patches by Cluster\", fontsize=16, y=1.02) plt.show() <p>Based on these H&amp;E patches, we could conclude that the Cluster 1 represents mature desmoplastic stroma, Cluster 2 represents immune dense stroma, and Cluster 3 represents immature desmoplastic stroma.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/workflows/clustering_desmoplasia/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial outlines a quantitative workflow for characterizing desmoplastic stroma in whole-slide images (WSIs), a key indicator in many cancer types. Desmoplasia refers to the growth of fibrous connective tissue in response to a tumor, a process where the stroma becomes stiff and remodels the tumor microenvironment. A crucial aspect of this process is distinguishing between immature and mature desmoplastic stroma.</p> <p>This guide will walk you through a series of steps to extract a set of features designed to characterize desmoplastic stroma. We will use a segmented HGSC omental slide as an example data in this workflow.</p>"},{"location":"user_guide/workflows/clustering_desmoplasia/#fit-a-rectangular-grid-to-the-tissue-segmentation-map","title":"Fit a Rectangular Grid to the Tissue Segmentation Map\u00b6","text":"<p>First, we'll create a rectangular grid over the tissue segmentation map from which we will filter only relevant parts to be analyzed. Firstly, the segmentation map contains two identical sections, we will only analyze one of them to reduce computational cost. Secondly, we are extracting stromal features so we will constrain our analysis to the stromal regions.</p>"},{"location":"user_guide/workflows/clustering_desmoplasia/#initialize-desmoplastic-stroma-feature-extraction-pipeline","title":"Initialize Desmoplastic Stroma Feature Extraction Pipeline\u00b6","text":"<p>Now, we will define our pipeline. We will be extracting features based on prior knowledge of desmoplastic stroma characteristics:</p> <p>Mature Desmoplasia should have:</p> <ul> <li>more pronounced eosin staining (pinkish) in comparison to immature desmoplasia.</li> <li>less deviation in collagen orientation.</li> </ul> <p>Immature Desmoplasia should have:</p> <ul> <li>more prominent hematoxylin staining in comparison to mature desmoplasia (blueish).</li> <li>more deviation in collagen orientation.</li> </ul> <p>To quantify these phenomena, we will extract a total of 10 features from the stromal regions:</p> <ol> <li>The standard deviation of collagen fiber orientation.</li> <li>The mean tortuosity of collagen fibers.</li> <li>Mean RGB-intensities of the stromal regions, especially the eosin and hematoxylin stains.</li> <li>The area of eosin and hematoxylin stain masks.</li> </ol>"},{"location":"user_guide/workflows/clustering_desmoplasia/#clustering-the-stromal-features","title":"Clustering the Stromal Features\u00b6","text":"<p>Next, we'll cluster the stromal features using KMeans clustering. We will first scale the features and then determine the optimal number of clusters using the silhouette score.</p>"},{"location":"user_guide/workflows/clustering_desmoplasia/#visualize-he-patches-from-different-clusters","title":"Visualize H&amp;E patches from Different Clusters\u00b6","text":"<p>Next, we'll visualize a few H&amp;E patches from each cluster to better understand their characteristics.</p>"},{"location":"user_guide/workflows/clustering_desmoplasia/#conclusions","title":"Conclusions\u00b6","text":"<p>In this workflow we demonstrated how to leverage unsupervised clustering techniques to identify distinct stromal phenotypes in histological images. By extracting and analyzing a comprehensive set of features, we were able to differentiate between mature desmoplastic stroma, immune dense stroma, and immature desmoplastic stroma. This approach can help to enhance our understanding of the tumor microenvironment: For example, these identified stromal clusters can be used to stratify features of individual stromal nuclei or to quantify the overall stromal composition of an entire slide, which can then be directly correlated with clinical outcomes. But we'll leave that for another time.</p>"},{"location":"user_guide/workflows/collagen_orientation/","title":"Collagen Orientation Disorder Quantification","text":"In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\n\nnuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path\ntis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", legend=True)\nax.set_axis_off()\n</pre> import geopandas as gpd  nuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path tis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path  ax = tis.plot(figsize=(10, 10), column=\"class_name\", legend=True) ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.spatial_ops.rect_grid import rect_grid\n\n# fit the grid. We'll use 256, 256 sized patches, focus only on stroma\npatch_size = (256, 256)\ngr = rect_grid(tis, patch_size, 0, \"intersects\")\nax = tis.plot(column=\"class_name\", figsize=(10, 10), alpha=0.5, legend=True)\nax = gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=0.5)\nax.set_axis_off()\n</pre> from histolytics.spatial_ops.rect_grid import rect_grid  # fit the grid. We'll use 256, 256 sized patches, focus only on stroma patch_size = (256, 256) gr = rect_grid(tis, patch_size, 0, \"intersects\") ax = tis.plot(column=\"class_name\", figsize=(10, 10), alpha=0.5, legend=True) ax = gr.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", lw=0.5) ax.set_axis_off() <p>Let's now filter out the other tissue section and constrain the bboxes to the stromal regions.</p> In\u00a0[3]: Copied! <pre>from histolytics.wsi.utils import get_sub_grids\nfrom histolytics.spatial_ops.ops import get_objs\n\nsub_grids = get_sub_grids(gr, min_size=1000, return_gdf=True)\ngr = get_objs(tis.loc[tis[\"class_name\"] == \"stroma\"], sub_grids[1], \"contains\")\ngr = gr.reset_index(drop=True)\n\nax = tis.plot(column=\"class_name\", aspect=1, figsize=(10, 10), legend=True)\nax = gr.boundary.plot(ax=ax, lw=0.5, color=\"red\")\nax.set_axis_off()\n</pre> from histolytics.wsi.utils import get_sub_grids from histolytics.spatial_ops.ops import get_objs  sub_grids = get_sub_grids(gr, min_size=1000, return_gdf=True) gr = get_objs(tis.loc[tis[\"class_name\"] == \"stroma\"], sub_grids[1], \"contains\") gr = gr.reset_index(drop=True)  ax = tis.plot(column=\"class_name\", aspect=1, figsize=(10, 10), legend=True) ax = gr.boundary.plot(ax=ax, lw=0.5, color=\"red\") ax.set_axis_off() In\u00a0[4]: Copied! <pre># Visualize the grid on top of the slide\n# thumbnail = reader.read_level(-2)\n# reader.get_annotated_thumbnail(thumbnail, gr, linewidth=1)\n</pre> # Visualize the grid on top of the slide # thumbnail = reader.read_level(-2) # reader.get_annotated_thumbnail(thumbnail, gr, linewidth=1) In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport scipy.ndimage as ndimage\n\nfrom histolytics.wsi.wsi_processor import WSIGridProcessor\nfrom histolytics.wsi.slide_reader import SlideReader\nfrom histolytics.stroma_feats.collagen import _fiber_midpoints, _major_axis_angle\nfrom histolytics.spatial_agg.local_diversity import local_diversity\nfrom histolytics.spatial_graph.spatial_weights import fit_distband\nfrom histolytics.utils.gdf import set_uid\nfrom histolytics.stroma_feats.collagen import extract_collagen_fibers\nfrom skimage.measure import label as label_sk\n\n\ndef pipeline(img: np.ndarray, label: np.ndarray, mask: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"A pipeline for extracting collagen orientation disorder from WSI stromal patches.\"\"\"\n    fibers = extract_collagen_fibers(img, label, rm_fg=True, rm_bg=False)\n    labeled_fibers = label_sk(fibers)\n\n    # return empty gdf if no fibers detected\n    if len(np.unique(labeled_fibers)) &lt;= 1:  # Only background (0) present\n        return gpd.GeoDataFrame(\n            columns=[\n                \"geometry\",\n                \"uid\",\n                \"major_axis_angle\",\n                \"major_axis_angle_shannon_index\",\n            ]\n        )\n\n    # get fiber indices (x, y coords for each extracted and labelled fiber)\n    fiber_indices = ndimage.value_indices(labeled_fibers, ignore_value=0)\n\n    # get fiber midpoints to fit the spatial graph\n    midpoints = _fiber_midpoints(fiber_indices)\n    point_gdf = set_uid(\n        gpd.GeoDataFrame(geometry=gpd.points_from_xy(midpoints[:, 0], midpoints[:, 1]))\n    )\n\n    # compute major axis angle\n    maa = _major_axis_angle(fiber_indices)\n    point_gdf[\"major_axis_angle\"] = maa\n\n    # fit distband graph to the midpoints of the collagen fibers with 32 micron nhood radius\n    # note the threshold value is 32*2 since it is in pixels\n    w = fit_distband(point_gdf, id_col=\"uid\", threshold=64)\n\n    # compute the local shannon index of the collagen neighborhoods\n    point_gdf = local_diversity(\n        point_gdf,\n        w,\n        val_cols=[\"major_axis_angle\"],\n        normalize=False,\n        scheme=\"quantiles\",\n        metrics=[\"shannon_index\"],\n        k=4,\n    )\n\n    return point_gdf\n\n\nsl_p = \"/path/to/slide.mrxs\"  # &lt;- modify path\nreader = SlideReader(sl_p, backend=\"OPENSLIDE\")\n\ncrop_loader = WSIGridProcessor(\n    slide_reader=reader,\n    grid=gr,\n    nuclei=nuc,\n    pipeline_func=pipeline,\n    batch_size=8,\n    num_workers=8,\n    pin_memory=False,\n    shuffle=False,\n    drop_last=False,\n)\n</pre> import numpy as np import pandas as pd import scipy.ndimage as ndimage  from histolytics.wsi.wsi_processor import WSIGridProcessor from histolytics.wsi.slide_reader import SlideReader from histolytics.stroma_feats.collagen import _fiber_midpoints, _major_axis_angle from histolytics.spatial_agg.local_diversity import local_diversity from histolytics.spatial_graph.spatial_weights import fit_distband from histolytics.utils.gdf import set_uid from histolytics.stroma_feats.collagen import extract_collagen_fibers from skimage.measure import label as label_sk   def pipeline(img: np.ndarray, label: np.ndarray, mask: np.ndarray) -&gt; pd.DataFrame:     \"\"\"A pipeline for extracting collagen orientation disorder from WSI stromal patches.\"\"\"     fibers = extract_collagen_fibers(img, label, rm_fg=True, rm_bg=False)     labeled_fibers = label_sk(fibers)      # return empty gdf if no fibers detected     if len(np.unique(labeled_fibers)) &lt;= 1:  # Only background (0) present         return gpd.GeoDataFrame(             columns=[                 \"geometry\",                 \"uid\",                 \"major_axis_angle\",                 \"major_axis_angle_shannon_index\",             ]         )      # get fiber indices (x, y coords for each extracted and labelled fiber)     fiber_indices = ndimage.value_indices(labeled_fibers, ignore_value=0)      # get fiber midpoints to fit the spatial graph     midpoints = _fiber_midpoints(fiber_indices)     point_gdf = set_uid(         gpd.GeoDataFrame(geometry=gpd.points_from_xy(midpoints[:, 0], midpoints[:, 1]))     )      # compute major axis angle     maa = _major_axis_angle(fiber_indices)     point_gdf[\"major_axis_angle\"] = maa      # fit distband graph to the midpoints of the collagen fibers with 32 micron nhood radius     # note the threshold value is 32*2 since it is in pixels     w = fit_distband(point_gdf, id_col=\"uid\", threshold=64)      # compute the local shannon index of the collagen neighborhoods     point_gdf = local_diversity(         point_gdf,         w,         val_cols=[\"major_axis_angle\"],         normalize=False,         scheme=\"quantiles\",         metrics=[\"shannon_index\"],         k=4,     )      return point_gdf   sl_p = \"/path/to/slide.mrxs\"  # &lt;- modify path reader = SlideReader(sl_p, backend=\"OPENSLIDE\")  crop_loader = WSIGridProcessor(     slide_reader=reader,     grid=gr,     nuclei=nuc,     pipeline_func=pipeline,     batch_size=8,     num_workers=8,     pin_memory=False,     shuffle=False,     drop_last=False, ) In\u00a0[6]: Copied! <pre>import pandas as pd\nfrom tqdm import tqdm\n\ncrop_feats = []\nwith crop_loader as loader:\n    with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:\n        for batch_idx, batch in enumerate(pbar):\n            crop_feats.extend(batch)  # collect the patch level dfs\n</pre> import pandas as pd from tqdm import tqdm  crop_feats = [] with crop_loader as loader:     with tqdm(loader, unit=\"batch\", total=len(loader)) as pbar:         for batch_idx, batch in enumerate(pbar):             crop_feats.extend(batch)  # collect the patch level dfs <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 718/718 [05:29&lt;00:00,  2.18batch/s]\n</pre> <p>Let's take the mean of the major axis angle Shannon index across all patches and create a patch-level DataFrame out of these.</p> In\u00a0[\u00a0]: Copied! <pre>patch_indices = []\nmean_shannon_values = []\nfor idx, gdf in crop_feats:\n    patch_indices.append(idx)\n    if gdf.empty or \"major_axis_angle_shannon_index\" not in gdf.columns:\n        mean_shannon_values.append(0.0)\n    else:\n        mean_shannon_values.append(gdf[\"major_axis_angle_shannon_index\"].mean())\n\npatch_feats = pd.DataFrame(\n    {\"mean_major_axis_angle_shannon_index\": mean_shannon_values}, index=patch_indices\n)\n\npatch_feats.index.name = \"uid\"\npatch_feats.head()\n</pre> patch_indices = [] mean_shannon_values = [] for idx, gdf in crop_feats:     patch_indices.append(idx)     if gdf.empty or \"major_axis_angle_shannon_index\" not in gdf.columns:         mean_shannon_values.append(0.0)     else:         mean_shannon_values.append(gdf[\"major_axis_angle_shannon_index\"].mean())  patch_feats = pd.DataFrame(     {\"mean_major_axis_angle_shannon_index\": mean_shannon_values}, index=patch_indices )  patch_feats.index.name = \"uid\" patch_feats.head() Out[\u00a0]: mean_major_axis_angle_shannon_index uid 0 0.621227 1 0.843264 2 1.069790 3 1.199338 4 1.043276 In\u00a0[8]: Copied! <pre>import matplotlib.pyplot as plt\nfrom histolytics.utils.plot import legendgram\n\n# Merge with the grid for spatial visualization\ngrid_feats = gr.merge(patch_feats, left_index=True, right_index=True, how=\"left\")\n\n# Fill any missing values with 0 (for patches not in crop_feats)\ngrid_feats[\"mean_major_axis_angle_shannon_index\"] = grid_feats[\n    \"mean_major_axis_angle_shannon_index\"\n].fillna(0)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 10))\ntis.plot(ax=ax, column=\"class_name\", alpha=0.3, legend=True)\ngrid_feats.plot(\n    ax=ax,\n    column=\"mean_major_axis_angle_shannon_index\",\n    cmap=\"viridis\",\n    legend=False,\n    lw=0.5,\n)\nax.set_axis_off()\n\n# add legendgram\nax = legendgram(\n    grid_feats,\n    \"mean_major_axis_angle_shannon_index\",\n    cmap=\"viridis\",\n    ax=ax,\n    loc=\"lower left\",\n)\n</pre> import matplotlib.pyplot as plt from histolytics.utils.plot import legendgram  # Merge with the grid for spatial visualization grid_feats = gr.merge(patch_feats, left_index=True, right_index=True, how=\"left\")  # Fill any missing values with 0 (for patches not in crop_feats) grid_feats[\"mean_major_axis_angle_shannon_index\"] = grid_feats[     \"mean_major_axis_angle_shannon_index\" ].fillna(0)  # Visualize fig, ax = plt.subplots(figsize=(10, 10)) tis.plot(ax=ax, column=\"class_name\", alpha=0.3, legend=True) grid_feats.plot(     ax=ax,     column=\"mean_major_axis_angle_shannon_index\",     cmap=\"viridis\",     legend=False,     lw=0.5, ) ax.set_axis_off()  # add legendgram ax = legendgram(     grid_feats,     \"mean_major_axis_angle_shannon_index\",     cmap=\"viridis\",     ax=ax,     loc=\"lower left\", ) In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Let's get sample some low and high collagen disorder cases..\n\n# Sort by Shannon index\ngrid_feats_sorted = grid_feats.sort_values(\"mean_major_axis_angle_shannon_index\")\n\n# Define thresholds for low and high cases\nn_total = len(grid_feats_sorted)\nlow_threshold = int(n_total * 0.25)  # Bottom 25%\nhigh_threshold = int(n_total * 0.75)  # Top 25%\n\n# Get low and high value patches\nlow_candidates = grid_feats_sorted.iloc[:low_threshold]\nhigh_candidates = grid_feats_sorted.iloc[high_threshold:]\n\n# Randomly sample 6 from each group\nnp.random.seed(46)\nlow_patches = low_candidates.sample(n=min(6, len(low_candidates)))\nhigh_patches = high_candidates.sample(n=min(6, len(high_candidates)))\n</pre> import matplotlib.pyplot as plt import numpy as np  # Let's get sample some low and high collagen disorder cases..  # Sort by Shannon index grid_feats_sorted = grid_feats.sort_values(\"mean_major_axis_angle_shannon_index\")  # Define thresholds for low and high cases n_total = len(grid_feats_sorted) low_threshold = int(n_total * 0.25)  # Bottom 25% high_threshold = int(n_total * 0.75)  # Top 25%  # Get low and high value patches low_candidates = grid_feats_sorted.iloc[:low_threshold] high_candidates = grid_feats_sorted.iloc[high_threshold:]  # Randomly sample 6 from each group np.random.seed(46) low_patches = low_candidates.sample(n=min(6, len(low_candidates))) high_patches = high_candidates.sample(n=min(6, len(high_candidates))) In\u00a0[10]: Copied! <pre>def polygon_to_xywh(polygon):\n    \"\"\"Convert polygon to xywh coordinates for slide reader.\"\"\"\n    minx, miny, maxx, maxy = polygon.bounds\n    return (int(minx), int(miny), int(maxx - minx), int(maxy - miny))\n\n\ndef extract_patch_images(patches, reader):\n    \"\"\"Extract images from WSI for given patches.\"\"\"\n    images = []\n    patch_info = []\n\n    for idx, _ in patches.iterrows():\n        original_geom = gr.loc[idx, \"geometry\"]\n        x, y, w, h = polygon_to_xywh(original_geom)\n        img = reader.read_region((int(x), int(y), int(w), int(h)), 0)\n        images.append(img)\n\n    return images, patch_info\n\n\n# Extract images for low and high chromatin patches\nlow_images, low_info = extract_patch_images(low_patches, reader)\nhigh_images, high_info = extract_patch_images(high_patches, reader)\n</pre> def polygon_to_xywh(polygon):     \"\"\"Convert polygon to xywh coordinates for slide reader.\"\"\"     minx, miny, maxx, maxy = polygon.bounds     return (int(minx), int(miny), int(maxx - minx), int(maxy - miny))   def extract_patch_images(patches, reader):     \"\"\"Extract images from WSI for given patches.\"\"\"     images = []     patch_info = []      for idx, _ in patches.iterrows():         original_geom = gr.loc[idx, \"geometry\"]         x, y, w, h = polygon_to_xywh(original_geom)         img = reader.read_region((int(x), int(y), int(w), int(h)), 0)         images.append(img)      return images, patch_info   # Extract images for low and high chromatin patches low_images, low_info = extract_patch_images(low_patches, reader) high_images, high_info = extract_patch_images(high_patches, reader) In\u00a0[11]: Copied! <pre>from histolytics.utils.plot import draw_thing_contours\n\nfig, axes = plt.subplots(4, 3, figsize=(12, 12))\n\n# Plot low collagen disorder patches (top 2 rows)\nfor i, img in enumerate(low_images):\n    row = i // 3\n    col = i % 3\n\n    fibers = label_sk(extract_collagen_fibers(img, rm_fg=True, rm_bg=True))\n    ax = axes[row, col]\n    ax.imshow(draw_thing_contours(img, fibers, fibers &gt; 0))\n    ax.axis(\"off\")\n\n# Plot high collagen disorder patches (bottom 2 rows)\nfor i, img in enumerate(high_images):\n    row = 2 + i // 3\n    col = i % 3\n\n    fibers = label_sk(extract_collagen_fibers(img, rm_fg=True, rm_bg=True))\n    ax = axes[row, col]\n    ax.imshow(draw_thing_contours(img, fibers, fibers &gt; 0))\n    ax.axis(\"off\")\n</pre> from histolytics.utils.plot import draw_thing_contours  fig, axes = plt.subplots(4, 3, figsize=(12, 12))  # Plot low collagen disorder patches (top 2 rows) for i, img in enumerate(low_images):     row = i // 3     col = i % 3      fibers = label_sk(extract_collagen_fibers(img, rm_fg=True, rm_bg=True))     ax = axes[row, col]     ax.imshow(draw_thing_contours(img, fibers, fibers &gt; 0))     ax.axis(\"off\")  # Plot high collagen disorder patches (bottom 2 rows) for i, img in enumerate(high_images):     row = 2 + i // 3     col = i % 3      fibers = label_sk(extract_collagen_fibers(img, rm_fg=True, rm_bg=True))     ax = axes[row, col]     ax.imshow(draw_thing_contours(img, fibers, fibers &gt; 0))     ax.axis(\"off\")  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user_guide/workflows/collagen_orientation/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial outlines a workflow to quantitatively measure collagen orientation disorder (see example) at WSI-level, a key feature of fibrosis in many solid tumors. The spatial arrangement of collagen fibers within the tumor microenvironment holds significant prognostic value: a highly aligned, parallel architecture is often associated with a less aggressive tumor phenotype, while a disorganized, chaotic pattern have been associated with more aggressive forms of cancer.</p> <p>This guide will demonstrate how to use Histolytics to extract collagen orientation disorder by measuring the fiber angle orientational entropy. We will use a segmented HGSC omental slide as an example data in this workflow.</p>"},{"location":"user_guide/workflows/collagen_orientation/#fit-a-rectangular-grid-to-the-tissue-segmentation-map","title":"Fit a Rectangular Grid to the Tissue Segmentation Map\u00b6","text":"<p>First, we'll create a rectangular grid over the tissue segmentation map from which we will filter only relevant parts to be analyzed. Firstly, the segmentation map contains two identical sections, we will only analyze one of them to reduce computational cost. Secondly, we are extracting a feature of the stroma so we will constrain our analysis to the stromal regions.</p>"},{"location":"user_guide/workflows/collagen_orientation/#initialize-the-collagen-disorder-pipeline","title":"Initialize the Collagen Disorder Pipeline\u00b6","text":"<p>To quantify the collagen orientation disorder, we will compute the mean Shannon entropy of the neighborhood major axis angles of collagen fibers within each patch. This will provide a measure of the dispersion of collagen fiber orientations, with higher values indicating greater disorder. To compute the disorder we will be using the graph fitting and neighborhood feature extraction utilities introduced previously.</p>"},{"location":"user_guide/workflows/collagen_orientation/#visualize-the-collagen-disorder-spatial-distribution","title":"Visualize the Collagen Disorder Spatial Distribution\u00b6","text":"<p>Next, we'll visualize the spatial distribution of the averaged collagen disorder values overlayed on the segmentation map.</p>"},{"location":"user_guide/workflows/collagen_orientation/#visualize-he-patches-from-low-and-high-cases","title":"Visualize H&amp;E patches from Low and High Cases\u00b6","text":"<p>Next, we'll visualize a few H&amp;E patches from low and high collagen disorder cases.</p>"},{"location":"user_guide/workflows/nuclear_morphology/","title":"Quantifying Neoplastic Nuclear Morphology","text":"In\u00a0[1]: Copied! <pre># Install seaborn for visualizing the results.\n# !pip install seaborn\n</pre> # Install seaborn for visualizing the results. # !pip install seaborn In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\n\nnuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path\ntis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, legend=True)\nax.set_axis_off()\n</pre> import geopandas as gpd  nuc = gpd.read_parquet(\"/path/to/nuclei_seg.parquet\")  # &lt;- modify path tis = gpd.read_parquet(\"/path/to/tissue_seg.parquet\")  # &lt;- modify path  ax = tis.plot(figsize=(10, 10), column=\"class_name\", aspect=1, legend=True) ax.set_axis_off() In\u00a0[3]: Copied! <pre>from histolytics.spatial_geom.shape_metrics import shape_metric\n\nmetrics = [\n    \"area\",\n    \"solidity\",\n    \"major_axis_len\",\n    \"major_axis_angle\",\n    \"minor_axis_len\",\n    \"minor_axis_angle\",\n    \"convexity\",\n    \"compactness\",\n    \"circularity\",\n    \"eccentricity\",\n    \"elongation\",\n    \"equivalent_rectangular_index\",\n    \"rectangularity\",\n    \"squareness\",\n    \"shape_index\",\n    \"fractal_dimension\",\n]\n\nneo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"]\nneo_nuc = shape_metric(neo_nuc, metrics=metrics, parallel=True, num_processes=8)\nneo_nuc.head()\n</pre> from histolytics.spatial_geom.shape_metrics import shape_metric  metrics = [     \"area\",     \"solidity\",     \"major_axis_len\",     \"major_axis_angle\",     \"minor_axis_len\",     \"minor_axis_angle\",     \"convexity\",     \"compactness\",     \"circularity\",     \"eccentricity\",     \"elongation\",     \"equivalent_rectangular_index\",     \"rectangularity\",     \"squareness\",     \"shape_index\",     \"fractal_dimension\", ]  neo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"] neo_nuc = shape_metric(neo_nuc, metrics=metrics, parallel=True, num_processes=8) neo_nuc.head() Out[3]: geometry class_name id area solidity major_axis_len major_axis_angle minor_axis_len minor_axis_angle convexity compactness circularity eccentricity elongation equivalent_rectangular_index rectangularity squareness shape_index fractal_dimension 0 POLYGON ((23532.205 71012.364, 23534.206 71014... neoplastic NaN 303.179889 0.996454 21.122759 34.167292 18.632316 55.832708 0.998349 0.928748 0.931822 0.471068 0.896783 0.707009 0.770341 1.182519 0.876445 0.970662 1 POLYGON ((23538.205 71151.421, 23534.205 71159... neoplastic NaN 595.511364 0.998364 38.856234 80.484152 20.130217 9.515848 0.999083 0.816035 0.817533 0.855339 0.572616 0.708278 0.761344 1.039008 0.703120 0.994011 2 POLYGON ((23526.205 72039.579, 23527.205 72040... neoplastic NaN 1668.573991 0.942004 62.826251 70.665622 34.472251 19.334378 0.986742 0.783256 0.804446 0.836025 0.623408 0.738001 0.770433 0.997273 0.730779 1.000368 3 POLYGON ((23528.964 72615.761, 23528.225 72618... neoplastic NaN 687.394834 0.996001 35.221928 84.881615 24.344484 5.118385 0.999539 0.901900 0.902731 0.722688 0.687848 0.735519 0.801664 1.148335 0.796325 0.978828 4 POLYGON ((23525.205 72869.579, 23527.205 72871... neoplastic NaN 985.062835 0.994368 39.643076 38.579682 31.336595 51.420318 0.998415 0.960170 0.963221 0.612503 0.951165 0.712232 0.792948 1.222527 0.889037 0.970850 In\u00a0[4]: Copied! <pre>import mapclassify\nfrom histolytics.utils.plot import legendgram\nfrom histolytics.utils.gdf import col_norm\n\n\n# helper function to replace legend items\ndef replace_legend_items(legend, mapping):\n    for txt in legend.texts:\n        for k, v in mapping.items():\n            if txt.get_text() == str(k):\n                txt.set_text(v)\n\n\ndef plot_neoplastic(neoplastic_nuc, col: str = \"area\", k: int = 5):\n    vals = col_norm(neoplastic_nuc[col], \"minmax\")\n    bins = mapclassify.Quantiles(vals, k=k)\n    neoplastic_nuc = neoplastic_nuc.assign(bin_vals=bins.yb)\n    neoplastic_nuc[f\"{col}_minmax\"] = vals\n\n    ax = tis.plot(\n        figsize=(10, 10), column=\"class_name\", aspect=1, legend=True, alpha=0.1\n    )\n    ax = neoplastic_nuc.plot(\n        ax=ax, column=\"bin_vals\", legend=True, cmap=\"viridis\", categorical=True\n    )\n\n    bin_legends = bins.get_legend_classes()\n    mapping = dict([(i, s) for i, s in enumerate(bin_legends)])\n    replace_legend_items(ax.get_legend(), mapping)\n    ax.set_axis_off()\n\n    ax.set_axis_off()\n\n    ax = legendgram(\n        gdf=neoplastic_nuc,\n        column=f\"{col}_minmax\",\n        n_bins=50,\n        breaks=bins.bins,\n        cmap=\"viridis\",\n        ax=ax,\n        loc=\"lower right\",\n    )\n\n    return ax\n\n\nplot_neoplastic(neo_nuc, col=\"area\", k=5)\n</pre> import mapclassify from histolytics.utils.plot import legendgram from histolytics.utils.gdf import col_norm   # helper function to replace legend items def replace_legend_items(legend, mapping):     for txt in legend.texts:         for k, v in mapping.items():             if txt.get_text() == str(k):                 txt.set_text(v)   def plot_neoplastic(neoplastic_nuc, col: str = \"area\", k: int = 5):     vals = col_norm(neoplastic_nuc[col], \"minmax\")     bins = mapclassify.Quantiles(vals, k=k)     neoplastic_nuc = neoplastic_nuc.assign(bin_vals=bins.yb)     neoplastic_nuc[f\"{col}_minmax\"] = vals      ax = tis.plot(         figsize=(10, 10), column=\"class_name\", aspect=1, legend=True, alpha=0.1     )     ax = neoplastic_nuc.plot(         ax=ax, column=\"bin_vals\", legend=True, cmap=\"viridis\", categorical=True     )      bin_legends = bins.get_legend_classes()     mapping = dict([(i, s) for i, s in enumerate(bin_legends)])     replace_legend_items(ax.get_legend(), mapping)     ax.set_axis_off()      ax.set_axis_off()      ax = legendgram(         gdf=neoplastic_nuc,         column=f\"{col}_minmax\",         n_bins=50,         breaks=bins.bins,         cmap=\"viridis\",         ax=ax,         loc=\"lower right\",     )      return ax   plot_neoplastic(neo_nuc, col=\"area\", k=5) Out[4]: <pre>&lt;AxesHostAxes: xlabel='Area_Minmax'&gt;</pre> <p>We can tell that the smaller neoplastic nuclei tend to localize at the border regions of the tumor masses.</p> <p>Eccentricity</p> In\u00a0[5]: Copied! <pre>plot_neoplastic(neo_nuc, col=\"eccentricity\", k=5)\n</pre> plot_neoplastic(neo_nuc, col=\"eccentricity\", k=5) Out[5]: <pre>&lt;AxesHostAxes: xlabel='Eccentricity_Minmax'&gt;</pre> <p>Fractal Dimension</p> In\u00a0[6]: Copied! <pre>plot_neoplastic(neo_nuc, col=\"fractal_dimension\", k=5)\n</pre> plot_neoplastic(neo_nuc, col=\"fractal_dimension\", k=5) Out[6]: <pre>&lt;AxesHostAxes: xlabel='Fractal_Dimension_Minmax'&gt;</pre> In\u00a0[7]: Copied! <pre>import seaborn\n\n\n# Let's first define a quick function to plot the data distributions with seaborn\ndef plot_kde(tidy_data, hue_col):\n    seaborn.set_style(\"whitegrid\")\n    seaborn.set_theme(font_scale=1.5)\n\n    # Setup the facets\n    facets = seaborn.FacetGrid(\n        data=tidy_data,\n        col=\"Attribute\",\n        hue=hue_col,\n        sharey=False,\n        sharex=False,\n        aspect=2,\n        col_wrap=2,\n    )\n\n    # Build the plot from `sns.kdeplot`\n    kde_ax = facets.map(seaborn.kdeplot, \"Values\", fill=True).add_legend()\n    return kde_ax\n</pre> import seaborn   # Let's first define a quick function to plot the data distributions with seaborn def plot_kde(tidy_data, hue_col):     seaborn.set_style(\"whitegrid\")     seaborn.set_theme(font_scale=1.5)      # Setup the facets     facets = seaborn.FacetGrid(         data=tidy_data,         col=\"Attribute\",         hue=hue_col,         sharey=False,         sharex=False,         aspect=2,         col_wrap=2,     )      # Build the plot from `sns.kdeplot`     kde_ax = facets.map(seaborn.kdeplot, \"Values\", fill=True).add_legend()     return kde_ax <p>Let's tidy up the data first</p> In\u00a0[8]: Copied! <pre>m = list(metrics)\n\nvals = col_norm(neo_nuc[\"area\"], \"minmax\")\nbins = mapclassify.Quantiles(vals, k=5)\nneo_nuc = neo_nuc.assign(bin_vals=bins.yb)\n\n\ntidy = neo_nuc.reset_index().set_index(\"bin_vals\")\ntidy = tidy[m]\ntidy = tidy.stack()\ntidy = tidy.reset_index()\ntidy = tidy.rename(\n    columns={\"bin_vals\": \"Area Bin\", \"level_1\": \"Attribute\", 0: \"Values\"}\n)\ntidy.head()\n</pre> m = list(metrics)  vals = col_norm(neo_nuc[\"area\"], \"minmax\") bins = mapclassify.Quantiles(vals, k=5) neo_nuc = neo_nuc.assign(bin_vals=bins.yb)   tidy = neo_nuc.reset_index().set_index(\"bin_vals\") tidy = tidy[m] tidy = tidy.stack() tidy = tidy.reset_index() tidy = tidy.rename(     columns={\"bin_vals\": \"Area Bin\", \"level_1\": \"Attribute\", 0: \"Values\"} ) tidy.head() Out[8]: Area Bin Attribute Values 0 1 area 303.179889 1 1 solidity 0.996454 2 1 major_axis_len 21.122759 3 1 major_axis_angle 34.167292 4 1 minor_axis_len 18.632316 In\u00a0[9]: Copied! <pre>plot_kde(tidy, hue_col=\"Area Bin\")\n</pre> plot_kde(tidy, hue_col=\"Area Bin\") Out[9]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x713627c37bc0&gt;</pre> <p>It looks like smaller nuclei tend to have higher eccentricity and lower circularity overall among the neoplastic nuclei which is an interesting finding. This indicates that the smaller neoplastic nuclei are more elongated and less circular in shape compared to larger neoplastic nuclei.</p>"},{"location":"user_guide/workflows/nuclear_morphology/#introduction","title":"Introduction\u00b6","text":"<p>In histopathology and biomedical image analysis, morphological metrics provide valuable insights into the geometry and complexity of nuclei which can be used to evaluate nuclear pleomorphism (variation in the size, shape, and staining characteristics of cell nuclei within a population of cells). Histolytics offers a comprehensive suite of morphological metrics to compute for nuclei and other segmented objects. In this workflow, we will demonstrate how to compute these at WSI-scale and visualize the spatial distributions of some of these metrics.</p> <p>We will use a WSI from that we segmented in the panoptic segmentation example as our input data. We will also be focusing solely on the neoplastic nuclei in this tutorial.</p>"},{"location":"user_guide/workflows/nuclear_morphology/#computing-morphological-metrics","title":"Computing Morphological Metrics\u00b6","text":"<p>We will compute all of the different shape morphological metrics that are available in <code>histolytics</code> and then visualize some of them. The morphological metrics can be computed with the <code>shape_metric</code>-function. The available metrics are:</p> <ul> <li>Area</li> <li>Perimeter</li> <li>Solidity</li> <li>Major Axis Length</li> <li>Minor Axis Length</li> <li>Major Axis Angle</li> <li>Minor Axis Angle</li> <li>Convexity</li> <li>Compactness</li> <li>Circularity</li> <li>Eccentricity</li> <li>Elongation</li> <li>Equivalent Recatangular Index</li> <li>Rectangularity</li> <li>Squareness</li> <li>Sphericity</li> <li>Shape Index</li> <li>Fractal Dimension</li> </ul>"},{"location":"user_guide/workflows/nuclear_morphology/#visualizing-the-metrics","title":"Visualizing the Metrics\u00b6","text":""},{"location":"user_guide/workflows/nuclear_morphology/#spatial-plots","title":"Spatial Plots\u00b6","text":"<p>We will visualize some of the shape metrics of the neoplastic cells. First: Area (in pixels)</p>"},{"location":"user_guide/workflows/nuclear_morphology/#kernel-density-estimation-kde-plots","title":"Kernel Density Estimation (KDE) Plots\u00b6","text":"<p>Next, we will look at how the distributions of the different attributes look like when we group the nuclei based on their area bins. Larger nuclei should to have different distributions compared to smaller nuclei, and this can be visualized using KDE plots.</p>"},{"location":"user_guide/workflows/nuclear_morphology/#conclusion","title":"Conclusion\u00b6","text":"<p>In this workflow tutorial we have demonstrated how to compute a wide range of morphological shape metrics for neoplastic nuclei and how to visualize the spatial distributions of selected metrics. This type of analysis can be used to quantify nuclear size and morphology and can be used to evaluate nuclear shape pleomorphism at WSI-scale.</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/","title":"TLS and Lymphoid Aggregate Analysis","text":"In\u00a0[1]: Copied! <pre>from histolytics.data import hgsc_nuclei_wsi, hgsc_tissue_wsi\n\nnuc = hgsc_nuclei_wsi()\ntis = hgsc_tissue_wsi()\n\nax = tis.plot(figsize=(10, 10), column=\"class_name\", alpha=0.2, aspect=1, legend=True)\nnuc.plot(ax=ax, column=\"class_name\", aspect=1)\nax.set_axis_off()\n</pre> from histolytics.data import hgsc_nuclei_wsi, hgsc_tissue_wsi  nuc = hgsc_nuclei_wsi() tis = hgsc_tissue_wsi()  ax = tis.plot(figsize=(10, 10), column=\"class_name\", alpha=0.2, aspect=1, legend=True) nuc.plot(ax=ax, column=\"class_name\", aspect=1) ax.set_axis_off() In\u00a0[2]: Copied! <pre>from histolytics.spatial_clust.density_clustering import density_clustering\n\nnuc_imm = nuc[nuc[\"class_name\"] == \"inflammatory\"]\nlabels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\")\nnuc_imm = nuc_imm.assign(labels=labels)\n\nax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1)\nnuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\")\nax.set_axis_off()\nnuc_imm.head(3)\n</pre> from histolytics.spatial_clust.density_clustering import density_clustering  nuc_imm = nuc[nuc[\"class_name\"] == \"inflammatory\"] labels = density_clustering(nuc_imm, eps=250, min_samples=100, method=\"dbscan\") nuc_imm = nuc_imm.assign(labels=labels)  ax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1) nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\") ax.set_axis_off() nuc_imm.head(3) Out[2]: geometry class_name labels uid 86 POLYGON ((547 37896.992, 547 37901.985, 551.99... inflammatory -1 94 POLYGON ((701 37928.983, 705.993 37933.976, 71... inflammatory -1 98 POLYGON ((625 37499.992, 625.999 37511.982, 63... inflammatory -1 <p>The immune cell clusters are highlighted with different colors in the image. These clusters represent either Lymphoid Aggregates (Laggs) or more mature tertiary lymphoid structures (TLS).</p> In\u00a0[16]: Copied! <pre>from histolytics.spatial_agg.local_distances import local_distances\nfrom histolytics.spatial_graph.graph import fit_graph\nimport pandas as pd\nfrom histolytics.spatial_clust.centrography import cluster_tendency\nimport geopandas as gpd\n\n# Get the clustered immune nuclei. Let's set an id col so we can track each cell id at global level\nclustered_nuc_imm = (\n    nuc_imm[nuc_imm[\"labels\"] != -1]\n    .reset_index(drop=False)\n    .rename(columns={\"uid\": \"id\"})\n)\n\n# let's fit a delaunay graph on each immune cluster to compute nhood distances\ncluster_graphs = []\nfor i in clustered_nuc_imm[\"labels\"].unique():\n    cluster = clustered_nuc_imm[clustered_nuc_imm[\"labels\"] == i]\n    w, w_gdf = fit_graph(cluster, \"delaunay\", threshold=250, id_col=None)\n    dists = local_distances(cluster, w, id_col=None)\n    cluster_graphs.append(dists)\n\n# get cluster centroids\nclust_centroids = (\n    nuc_imm.groupby(\"labels\")\n    .apply(lambda g: cluster_tendency(g, \"mean\"), include_groups=False)\n    .reset_index(drop=False, name=\"geometry\")\n)\n\n# get mean nhood distances per cluster\nnhood_dists = pd.concat(cluster_graphs).reset_index(drop=True)\nnhood_dists = (\n    nhood_dists.groupby(\"labels\")\n    .apply(lambda x: x[\"nhood_dists_mean\"].mean(), include_groups=False)\n    .sort_values(ascending=False)\n)\nnhood_dists.name = \"nhood_dists_mean\"\n\n# merge the cluster centroids and nhood distances\nclust_feats = clust_centroids.merge(nhood_dists, on=\"labels\")\n\n# convert to GeoDataFrame\nclust_feats = gpd.GeoDataFrame(clust_feats, geometry=\"geometry\")\n\n# plot\nax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1)\nax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\")\nclust_feats.plot(ax=ax, column=\"nhood_dists_mean\", aspect=1, cmap=\"turbo\", legend=True)\nax.set_axis_off()\n</pre> from histolytics.spatial_agg.local_distances import local_distances from histolytics.spatial_graph.graph import fit_graph import pandas as pd from histolytics.spatial_clust.centrography import cluster_tendency import geopandas as gpd  # Get the clustered immune nuclei. Let's set an id col so we can track each cell id at global level clustered_nuc_imm = (     nuc_imm[nuc_imm[\"labels\"] != -1]     .reset_index(drop=False)     .rename(columns={\"uid\": \"id\"}) )  # let's fit a delaunay graph on each immune cluster to compute nhood distances cluster_graphs = [] for i in clustered_nuc_imm[\"labels\"].unique():     cluster = clustered_nuc_imm[clustered_nuc_imm[\"labels\"] == i]     w, w_gdf = fit_graph(cluster, \"delaunay\", threshold=250, id_col=None)     dists = local_distances(cluster, w, id_col=None)     cluster_graphs.append(dists)  # get cluster centroids clust_centroids = (     nuc_imm.groupby(\"labels\")     .apply(lambda g: cluster_tendency(g, \"mean\"), include_groups=False)     .reset_index(drop=False, name=\"geometry\") )  # get mean nhood distances per cluster nhood_dists = pd.concat(cluster_graphs).reset_index(drop=True) nhood_dists = (     nhood_dists.groupby(\"labels\")     .apply(lambda x: x[\"nhood_dists_mean\"].mean(), include_groups=False)     .sort_values(ascending=False) ) nhood_dists.name = \"nhood_dists_mean\"  # merge the cluster centroids and nhood distances clust_feats = clust_centroids.merge(nhood_dists, on=\"labels\")  # convert to GeoDataFrame clust_feats = gpd.GeoDataFrame(clust_feats, geometry=\"geometry\")  # plot ax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1) ax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\") clust_feats.plot(ax=ax, column=\"nhood_dists_mean\", aspect=1, cmap=\"turbo\", legend=True) ax.set_axis_off() In\u00a0[\u00a0]: Copied! <pre>from histolytics.spatial_clust.clust_metrics import cluster_feats\nimport pandas as pd\nimport geopandas as gpd\n\n\n# compute the cluster sizes (number of cells in each cluster)\nclust_sizes = (\n    clustered_nuc_imm.groupby(\"labels\")\n    .apply(\n        lambda x: pd.Series(\n            cluster_feats(x, hull_type=\"convex_hull\", normalize_orientation=True)\n        ),\n        include_groups=False,\n    )\n    .reset_index(drop=False)\n)\n# merge the cluster centroids and features\nclust_feats = clust_feats.merge(clust_sizes, on=\"labels\")\nclust_feats.head(5)\n</pre> from histolytics.spatial_clust.clust_metrics import cluster_feats import pandas as pd import geopandas as gpd   # compute the cluster sizes (number of cells in each cluster) clust_sizes = (     clustered_nuc_imm.groupby(\"labels\")     .apply(         lambda x: pd.Series(             cluster_feats(x, hull_type=\"convex_hull\", normalize_orientation=True)         ),         include_groups=False,     )     .reset_index(drop=False) ) # merge the cluster centroids and features clust_feats = clust_feats.merge(clust_sizes, on=\"labels\") clust_feats.head(5) Out[\u00a0]: labels geometry nhood_dists_mean area dispersion size orientation 0 0 POINT (2432.115 30375.571) 44.931122 3.348646e+05 210.071704 163.0 6.179763 1 1 POINT (3508.768 49500.988) 31.615201 1.653666e+07 1853.488920 12115.0 89.351878 2 2 POINT (3647.298 44067.835) 41.627359 1.130964e+06 473.763588 494.0 40.110577 3 3 POINT (4007.447 45206.206) 37.056731 1.262858e+06 435.069616 760.0 42.858025 4 4 POINT (3669.009 53649.081) 43.285193 1.303155e+05 148.194226 83.0 83.120009 ... ... ... ... ... ... ... ... 85 85 POINT (44933.734 21806.188) 44.318124 3.499691e+05 238.108933 181.0 8.879174 86 86 POINT (44859.741 36651.431) 40.267456 1.522813e+05 157.504400 107.0 69.020158 87 87 POINT (46434.795 49237.848) 38.968171 2.432185e+05 203.606138 155.0 40.041493 88 88 POINT (46836.496 50458.293) 37.290687 3.192059e+05 232.912662 225.0 89.920400 89 89 POINT (46379.693 55183.825) 38.136915 1.521591e+05 180.376885 122.0 89.290457 <p>90 rows \u00d7 7 columns</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Add a simple classification based on size and density\ndef classify_clusters(df, area_threshold=2500000, density_threshold=None):\n    \"\"\"Simple heuristic classification\"\"\"\n    if density_threshold is None:\n        density_threshold = df[\"nhood_dists_mean\"].median()\n\n    conditions = [\n        (df[\"area\"] &gt; area_threshold) &amp; (df[\"nhood_dists_mean\"] &lt; density_threshold),\n        (df[\"area\"] &gt; area_threshold) &amp; (df[\"nhood_dists_mean\"] &gt;= density_threshold),\n        (df[\"area\"] &lt;= area_threshold) &amp; (df[\"nhood_dists_mean\"] &lt; density_threshold),\n        (df[\"area\"] &lt;= area_threshold) &amp; (df[\"nhood_dists_mean\"] &gt;= density_threshold),\n    ]\n\n    choices = [\"Potential TLS\", \"Large Aggregate\", \"Dense Aggregate\", \"Small Aggregate\"]\n\n    df[\"cluster_type\"] = np.select(conditions, choices, default=\"Unclassified\")\n    return df\n\n\n# Apply classification\nclust_feats_classified = classify_clusters(clust_feats.copy())\n\n# Plot with classification\nplt.figure(figsize=(10, 6))\nfor cluster_type in clust_feats_classified[\"cluster_type\"].unique():\n    mask = clust_feats_classified[\"cluster_type\"] == cluster_type\n    plt.scatter(\n        clust_feats_classified.loc[mask, \"area\"],\n        clust_feats_classified.loc[mask, \"nhood_dists_mean\"],\n        label=cluster_type,\n        s=100,\n        alpha=0.7,\n    )\n\nplt.xlabel(\"Cluster Area (\u03bcm\u00b2)\")\nplt.ylabel(\"Mean Neighborhood Distance\")\nplt.title(\"Immune Cluster Classification\\n(Based on Area and Density)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Add threshold lines\nplt.axvline(x=2500000, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Area Threshold\")\nplt.axhline(\n    y=clust_feats[\"nhood_dists_mean\"].median(),\n    color=\"blue\",\n    linestyle=\"--\",\n    alpha=0.5,\n    label=\"Density Threshold\",\n)\n\nplt.tight_layout()\nplt.show()\n\n# Print classification results\nprint(\"\\nCluster Classification Results:\")\nprint(clust_feats_classified[\"cluster_type\"].value_counts())\n</pre> import matplotlib.pyplot as plt import numpy as np   # Add a simple classification based on size and density def classify_clusters(df, area_threshold=2500000, density_threshold=None):     \"\"\"Simple heuristic classification\"\"\"     if density_threshold is None:         density_threshold = df[\"nhood_dists_mean\"].median()      conditions = [         (df[\"area\"] &gt; area_threshold) &amp; (df[\"nhood_dists_mean\"] &lt; density_threshold),         (df[\"area\"] &gt; area_threshold) &amp; (df[\"nhood_dists_mean\"] &gt;= density_threshold),         (df[\"area\"] &lt;= area_threshold) &amp; (df[\"nhood_dists_mean\"] &lt; density_threshold),         (df[\"area\"] &lt;= area_threshold) &amp; (df[\"nhood_dists_mean\"] &gt;= density_threshold),     ]      choices = [\"Potential TLS\", \"Large Aggregate\", \"Dense Aggregate\", \"Small Aggregate\"]      df[\"cluster_type\"] = np.select(conditions, choices, default=\"Unclassified\")     return df   # Apply classification clust_feats_classified = classify_clusters(clust_feats.copy())  # Plot with classification plt.figure(figsize=(10, 6)) for cluster_type in clust_feats_classified[\"cluster_type\"].unique():     mask = clust_feats_classified[\"cluster_type\"] == cluster_type     plt.scatter(         clust_feats_classified.loc[mask, \"area\"],         clust_feats_classified.loc[mask, \"nhood_dists_mean\"],         label=cluster_type,         s=100,         alpha=0.7,     )  plt.xlabel(\"Cluster Area (\u03bcm\u00b2)\") plt.ylabel(\"Mean Neighborhood Distance\") plt.title(\"Immune Cluster Classification\\n(Based on Area and Density)\") plt.legend() plt.grid(True, alpha=0.3)  # Add threshold lines plt.axvline(x=2500000, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Area Threshold\") plt.axhline(     y=clust_feats[\"nhood_dists_mean\"].median(),     color=\"blue\",     linestyle=\"--\",     alpha=0.5,     label=\"Density Threshold\", )  plt.tight_layout() plt.show()  # Print classification results print(\"\\nCluster Classification Results:\") print(clust_feats_classified[\"cluster_type\"].value_counts()) <pre>\nCluster Classification Results:\ncluster_type\nSmall Aggregate    45\nDense Aggregate    35\nPotential TLS      10\nName: count, dtype: int64\n</pre> <p>Let's visualize clusters and cluster centroids. We will highlight the potetial cluster type.</p> In\u00a0[\u00a0]: Copied! <pre>ax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1)\nax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\")\nclust_feats_classified.plot(\n    ax=ax, column=\"cluster_type\", aspect=1, cmap=\"tab10\", legend=True\n)\nax.set_axis_off()\n</pre> ax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1) ax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\") clust_feats_classified.plot(     ax=ax, column=\"cluster_type\", aspect=1, cmap=\"tab10\", legend=True ) ax.set_axis_off() In\u00a0[\u00a0]: Copied! <pre>from histolytics.spatial_clust.clust_metrics import cluster_dists_to_tissue\n\n# get the distances to the tissue classes\nclust_feats_classified = cluster_dists_to_tissue(clust_feats_classified, tis, \"tumor\")\nclust_feats_classified = cluster_dists_to_tissue(\n    clust_feats_classified, tis, \"omental_fat\"\n)\n\n# get the closest tissue class\nclust_feats_classified[\"closest_tissue\"] = (\n    clust_feats_classified[[\"dist_to_tumor\", \"dist_to_omental_fat\"]]\n    .idxmin(axis=1)\n    .str.replace(\"dist_to\", \"closest_to\")\n)\n\n# # get the distance to the closest tissue class\nclust_feats_classified[\"distance_to_tissue\"] = clust_feats_classified[\n    [\"dist_to_tumor\", \"dist_to_omental_fat\"]\n].min(axis=1)\n\n\nax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1)\nax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\")\nclust_feats_classified.plot(\n    ax=ax, column=\"closest_tissue\", legend=True, aspect=1, cmap=\"tab10\"\n)\nax.set_axis_off()\nclust_feats_classified.head(5)\n</pre> from histolytics.spatial_clust.clust_metrics import cluster_dists_to_tissue  # get the distances to the tissue classes clust_feats_classified = cluster_dists_to_tissue(clust_feats_classified, tis, \"tumor\") clust_feats_classified = cluster_dists_to_tissue(     clust_feats_classified, tis, \"omental_fat\" )  # get the closest tissue class clust_feats_classified[\"closest_tissue\"] = (     clust_feats_classified[[\"dist_to_tumor\", \"dist_to_omental_fat\"]]     .idxmin(axis=1)     .str.replace(\"dist_to\", \"closest_to\") )  # # get the distance to the closest tissue class clust_feats_classified[\"distance_to_tissue\"] = clust_feats_classified[     [\"dist_to_tumor\", \"dist_to_omental_fat\"] ].min(axis=1)   ax = nuc.plot(figsize=(10, 10), color=\"blue\", alpha=0.3, aspect=1) ax = nuc_imm.plot(ax=ax, column=\"labels\", aspect=1, cmap=\"tab20\") clust_feats_classified.plot(     ax=ax, column=\"closest_tissue\", legend=True, aspect=1, cmap=\"tab10\" ) ax.set_axis_off() clust_feats_classified.head(5) Out[\u00a0]: labels geometry nhood_dists_mean area dispersion size orientation cluster_type dist_to_tumor dist_to_omental_fat closest_tissue distance_to_tissue 0 0 POINT (2432.115 30375.571) 44.931122 3.348646e+05 210.071704 163.0 6.179763 Small Aggregate 470.562404 684.283897 closest_to_tumor 470.562404 1 1 POINT (3508.768 49500.988) 31.615201 1.653666e+07 1853.488920 12115.0 89.351878 Potential TLS 1109.971779 27.871655 closest_to_omental_fat 27.871655 2 2 POINT (3647.298 44067.835) 41.627359 1.130964e+06 473.763588 494.0 40.110577 Small Aggregate 392.835793 63.166603 closest_to_omental_fat 63.166603 3 3 POINT (4007.447 45206.206) 37.056731 1.262858e+06 435.069616 760.0 42.858025 Dense Aggregate 786.064771 0.000000 closest_to_omental_fat 0.000000 4 4 POINT (3669.009 53649.081) 43.285193 1.303155e+05 148.194226 83.0 83.120009 Small Aggregate 614.856948 165.602860 closest_to_omental_fat 165.602860 In\u00a0[32]: Copied! <pre>clust_feats_classified.value_counts(\"closest_tissue\")\n</pre> clust_feats_classified.value_counts(\"closest_tissue\") Out[32]: <pre>closest_tissue\nclosest_to_omental_fat    51\nclosest_to_tumor          39\nName: count, dtype: int64</pre> <p>In the example, we can see that most of the inflammatory cluster centroids reside closer to omental fat tissue than the tumor itself. However, from this example, it is hard to conclude anything from this alone. The main purpose was to demonstrate that this type of analysis can be conducted in a straightforward manner and it can be useful for understanding the spatial relationships between immune clusters and different tissue types in other contexts. Naturally, this analysis can be extended to include other tissue types as well.</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/#introduction","title":"Introduction\u00b6","text":"<p>In this workflow, we will show spatial clustering and cluster analysis techniques applied to segmented nuclei from H&amp;E WSI. Using nuclei segmentations, we identify and analyze spatial patterns of immune cell aggregates and their relationship to tissue structures. This is useful for understanding the spatial organization of immune cells in the tumor microenvironment, which can provide insights into immune response related questions such as immune activation, infiltration etc. Note that this workflow is not limited to immune cells, it can be applied to any type of spatially distributed objects, such as collagen fibers, stromal cells, etc.</p> <p>Key steps include:</p> <ul> <li>Density-based clustering to identify immune cell aggregates.</li> <li>Extraction of cluster features such as neighborhood densitiy, size, area, dispersion, and orientation.</li> <li>Visualization of clusters and their centroids.</li> <li>Classifying clusters into Lymphoid aggregates and potential TLSs based on simple heuristics.</li> <li>Analysis of cluster proximity to different tissue types.</li> </ul> <p>Let's start by loading the example data.</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/#clustering-inflammatory-cells-with-dbscan","title":"Clustering Inflammatory Cells with DBSCAN\u00b6","text":"<p>We'll set the minimum number of samples in the clusters to 100 and the maximum distance between samples to 250px (125 microns).</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/#cluster-neighborhood-densities","title":"Cluster Neighborhood Densities\u00b6","text":"<p>The most likely interpretation of each cluster can be derived from its spatial characteristics, such as size, shape, density and the existence of germinal centers which would indicate a more organized structure typical of TLS. Germinal centers can be hard to identify in some clusters, and additional analysis may be required to confirm their presence, but we can easily compute the size and neighborhood densities for each cluster and set a threshold heuristic to determine whether a cluster is a Lagg or a TLS (typically TLS are larger and denser). First, we'll compute the neighborhood densities.</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/#compute-cluster-areas","title":"Compute Cluster Areas\u00b6","text":"<p>Next, we will compute the areas of the clusters. The <code>cluster_feats</code> function also computes the cluster area, dispersion and orientation for each cluster but we'll leave those features untouched for now and use only the cluster densities and areas for our analysis.</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/#tls-heuristic","title":"TLS Heuristic\u00b6","text":"<p>Now we'll create a crude heuristic for identifying potential TLS (Tertiary Lymphoid Structures) based on the cluster features we've computed. This heuristic will be based on a combination of cluster area and density. The bigger and denser the cluster is, the more likely it is to be a TLS.</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/#cluster-centroid-distances-to-different-tissues","title":"Cluster Centroid Distances to Different Tissues\u00b6","text":"<p>Let's now calculate the distances of the cluster centroids to different tissue types. This can help in understanding how the clusters are distributed in relation to the tissue types. We use the <code>cluster_dists_to_tissue</code> function for this purpose and measure the immune cluster centroid distances to other tissues. We will measure the centroid distances to tumor and omental fat tissues in this example.</p>"},{"location":"user_guide/workflows/tls_lymphoid_aggregate/#conclusions","title":"Conclusions\u00b6","text":"<p>In this workflow, we have demonstrated how to perform spatial clustering and cluster analysis of segmented inflammatory nuclei from H&amp;E WSI. We used density-based clustering to identify immune cell aggregates, computed cluster features, and visualized the clusters and their centroids. Additionally, we used a heuristic to classify the clusters based on their features, providing a way to identify potential Tertiary Lymphoid Structures (TLS) within the tissue and we explored the spatial relationships between these clusters and different tissue types.</p>"},{"location":"user_guide/workflows/tumor_cell_accessibility/","title":"Quantifying Neoplastic Cell Accessibility","text":"In\u00a0[1]: Copied! <pre>from histolytics.spatial_graph.graph import fit_graph\nfrom histolytics.utils.gdf import set_uid\nfrom histolytics.data import hgsc_cancer_nuclei\n\nnuc = hgsc_cancer_nuclei()\nnuc = set_uid(nuc)  # Ensure the GeoDataFrame has a unique identifier\n\n# fit spatial weights using Delaunay triangulation\nw, w_gdf = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100)\nax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1)\nw_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1)\nw_gdf.head(5)\nax.set_axis_off()\n</pre> from histolytics.spatial_graph.graph import fit_graph from histolytics.utils.gdf import set_uid from histolytics.data import hgsc_cancer_nuclei  nuc = hgsc_cancer_nuclei() nuc = set_uid(nuc)  # Ensure the GeoDataFrame has a unique identifier  # fit spatial weights using Delaunay triangulation w, w_gdf = fit_graph(nuc, \"delaunay\", id_col=\"uid\", threshold=100) ax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1) w_gdf.plot(ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1) w_gdf.head(5) ax.set_axis_off() In\u00a0[4]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom typing import List, Tuple\nfrom histolytics.spatial_agg.local_values import local_vals\n\n\n# Custom function to compute the ratio of inflammatory to connective neighbors\ndef compute_inflammatory_ratio(row):\n    if row[\"inflammatory_cnt\"] == 0 and row[\"connective_cnt\"] == 0:\n        return 0\n    return row[\"inflammatory_cnt\"] / (row[\"inflammatory_cnt\"] + row[\"connective_cnt\"])\n\n\ndef compute_neighbor_cnt(nhood_classes: List[str]) -&gt; Tuple[float, float]:\n    inflammatory_count = np.sum(np.array(nhood_classes) == \"inflammatory\")\n    connective_count = np.sum(np.array(nhood_classes) == \"connective\")\n    return inflammatory_count, connective_count\n\n\n# get the neighborhood classes for each nucleus\nnuc = local_vals(\n    nuc, w, val_col=\"class_name\", new_col_name=\"nhood_classes\", id_col=\"uid\"\n)\n\n# Filter for neoplastic nuclei\nneo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"].copy()\n\n# Compute neighborhood inflammatory and connective counts\nneo_nuc[[\"inflammatory_cnt\", \"connective_cnt\"]] = (\n    neo_nuc[\"nhood_classes\"].apply(compute_neighbor_cnt).apply(pd.Series)\n)\n\n# Compute the ratio of inflammatory to connective neighbors around neoplastic nuclei\nneo_nuc[\"inflammatory_ratio\"] = neo_nuc.apply(compute_inflammatory_ratio, axis=1)\n\n# keep only the neoplastic nuclei with inflammatory or connective neighbors\nfiltered_neo_nuc = neo_nuc[\n    (neo_nuc[\"inflammatory_cnt\"] &gt; 0) | (neo_nuc[\"connective_cnt\"] &gt; 0)\n]\n\n# Filter the links to only include inflammatory-neoplastic and connective-neoplastic\nimm_conn_neo_links = w_gdf[\n    w_gdf[\"class_name\"].isin([\"inflammatory-neoplastic\", \"connective-neoplastic\"])\n]\n\n# Let's visualize the neoplastic nuclei with their inflammatory ratio\nax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1)\nfiltered_neo_nuc.plot(ax=ax, column=\"inflammatory_ratio\", legend=True, aspect=1)\nimm_conn_neo_links.plot(\n    ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1, cmap=\"Set1\"\n)\nfiltered_neo_nuc.head(3)\nax.set_axis_off()\n</pre> import numpy as np import pandas as pd from typing import List, Tuple from histolytics.spatial_agg.local_values import local_vals   # Custom function to compute the ratio of inflammatory to connective neighbors def compute_inflammatory_ratio(row):     if row[\"inflammatory_cnt\"] == 0 and row[\"connective_cnt\"] == 0:         return 0     return row[\"inflammatory_cnt\"] / (row[\"inflammatory_cnt\"] + row[\"connective_cnt\"])   def compute_neighbor_cnt(nhood_classes: List[str]) -&gt; Tuple[float, float]:     inflammatory_count = np.sum(np.array(nhood_classes) == \"inflammatory\")     connective_count = np.sum(np.array(nhood_classes) == \"connective\")     return inflammatory_count, connective_count   # get the neighborhood classes for each nucleus nuc = local_vals(     nuc, w, val_col=\"class_name\", new_col_name=\"nhood_classes\", id_col=\"uid\" )  # Filter for neoplastic nuclei neo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"].copy()  # Compute neighborhood inflammatory and connective counts neo_nuc[[\"inflammatory_cnt\", \"connective_cnt\"]] = (     neo_nuc[\"nhood_classes\"].apply(compute_neighbor_cnt).apply(pd.Series) )  # Compute the ratio of inflammatory to connective neighbors around neoplastic nuclei neo_nuc[\"inflammatory_ratio\"] = neo_nuc.apply(compute_inflammatory_ratio, axis=1)  # keep only the neoplastic nuclei with inflammatory or connective neighbors filtered_neo_nuc = neo_nuc[     (neo_nuc[\"inflammatory_cnt\"] &gt; 0) | (neo_nuc[\"connective_cnt\"] &gt; 0) ]  # Filter the links to only include inflammatory-neoplastic and connective-neoplastic imm_conn_neo_links = w_gdf[     w_gdf[\"class_name\"].isin([\"inflammatory-neoplastic\", \"connective-neoplastic\"]) ]  # Let's visualize the neoplastic nuclei with their inflammatory ratio ax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1) filtered_neo_nuc.plot(ax=ax, column=\"inflammatory_ratio\", legend=True, aspect=1) imm_conn_neo_links.plot(     ax=ax, linewidth=1, column=\"class_name\", legend=True, aspect=1, cmap=\"Set1\" ) filtered_neo_nuc.head(3) ax.set_axis_off() <p>In this example, most of the neoplastic nuclei are surrounded by inflammatory nuclei, making them potentially more susceptible to immune infiltration. This is indicated by the high ratio of yellow to blue nuclei in the image above. The custom metric can be easily adapted to include other cell types or different heuristics based on the research question.</p> In\u00a0[5]: Copied! <pre>import pandas as pd\nfrom histolytics.spatial_agg.local_values import local_vals\n\n\n# Custom function to compute the ratio of inflammatory to connective neighbors\ndef compute_neoplastic_cardinality(row):\n    return len(row[\"nhood_classes\"])\n\n\n# Filter for neoplastic nuclei\nneo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"].copy()\nneo_nuc = set_uid(neo_nuc)  # Ensure the GeoDataFrame has a unique identifier\n\n# Let's use the distband to get the neighborhoods based on radius only\nw, w_gdf = fit_graph(neo_nuc, \"distband\", id_col=\"uid\", threshold=64)\n\n# get the neighborhood classes for each nucleus\nneo_nuc = local_vals(\n    neo_nuc, w, val_col=\"class_name\", new_col_name=\"nhood_classes\", id_col=\"uid\"\n)\n\nneo_nuc[\"packing_density\"] = neo_nuc.apply(compute_neoplastic_cardinality, axis=1)\n\n# filter the links to only include neoplastic-neoplastic for visualization\nneo_neo_links = w_gdf[w_gdf[\"class_name\"].isin([\"neoplastic-neoplastic\"])]\n\n# Let's visualize the neoplastic nuclei with their inflammatory ratio\nax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1)\nneo_nuc.plot(ax=ax, column=\"packing_density\", legend=True, aspect=1)\nneo_neo_links.plot(\n    ax=ax, linewidth=0.5, column=\"class_name\", legend=True, aspect=1, cmap=\"Set1\"\n)\nneo_nuc.head(3)\nax.set_axis_off()\n</pre> import pandas as pd from histolytics.spatial_agg.local_values import local_vals   # Custom function to compute the ratio of inflammatory to connective neighbors def compute_neoplastic_cardinality(row):     return len(row[\"nhood_classes\"])   # Filter for neoplastic nuclei neo_nuc = nuc[nuc[\"class_name\"] == \"neoplastic\"].copy() neo_nuc = set_uid(neo_nuc)  # Ensure the GeoDataFrame has a unique identifier  # Let's use the distband to get the neighborhoods based on radius only w, w_gdf = fit_graph(neo_nuc, \"distband\", id_col=\"uid\", threshold=64)  # get the neighborhood classes for each nucleus neo_nuc = local_vals(     neo_nuc, w, val_col=\"class_name\", new_col_name=\"nhood_classes\", id_col=\"uid\" )  neo_nuc[\"packing_density\"] = neo_nuc.apply(compute_neoplastic_cardinality, axis=1)  # filter the links to only include neoplastic-neoplastic for visualization neo_neo_links = w_gdf[w_gdf[\"class_name\"].isin([\"neoplastic-neoplastic\"])]  # Let's visualize the neoplastic nuclei with their inflammatory ratio ax = nuc.plot(figsize=(10, 10), column=\"class_name\", aspect=1) neo_nuc.plot(ax=ax, column=\"packing_density\", legend=True, aspect=1) neo_neo_links.plot(     ax=ax, linewidth=0.5, column=\"class_name\", legend=True, aspect=1, cmap=\"Set1\" ) neo_nuc.head(3) ax.set_axis_off() <p>Here we see that the neoplastic nuclei are densely packed in the tumor nests, which is typical for this type of solid growth pattern in HGSC. Again, these types of custom metrics can be easily adapted in Histolytics to explore different research questions in mind.</p>"},{"location":"user_guide/workflows/tumor_cell_accessibility/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial introduces a workflow to quantify tumor cell accessibility by analyzing the composition of a tumor cell\u2019s immediate neighborhood. The tumor microenvironment (TME) is a battleground where the ratio of anti-tumor immune cells to pro-tumor stromal cells can tell us about the potential for immune infiltration. Tumor cell neighborhoods rich in immune cells indicates greater accessibility for the immune system to target these cancer cells, while an abundance of stromal cells in the neighborhoods can create a dense, fibrotic barriers that physically impedes immune infiltration and promotes an immunosuppressive state.</p> <p>This guide will demonstrate how to use Histolytics to define and analyze the cellular neighborhoods surrounding individual tumor cells. By quantifying the ratio of immune cells to stromal cells in these local regions, you can objectively assess the level of tumor accessibility and gain a deeper understanding of the interplay between cancer cells and their surrounding microenvironment.</p>"},{"location":"user_guide/workflows/tumor_cell_accessibility/#neoplastic-immune-accessibility","title":"Neoplastic Immune-accessibility\u00b6","text":"<p>Next, we'll showcase how you can create custom metrics to analyze spatial neighborhood relationships between nuclei. We will define a custom metric to analyze the local inflammatory and connective nuclear microenvironment around the neoplastic nuclei. This metric will help us understand how neoplastic nuclei are surrounded by inflammatory and connective tissue cells and potentially physically blocked from immune infiltration. To do this, we calculate for each neoplastic nucleus, the ratio of inflammatory neighbors to the total number of inflammatory and connective neighbors. This highlights neoplastic nuclei that are surrounded predominantly by inflammatory nuclei versus connective nuclei, providing insights whether the neoplastic nuclei are physically blocked by the connective nuclei.</p>"},{"location":"user_guide/workflows/tumor_cell_accessibility/#neoplastic-cell-packing-density","title":"Neoplastic cell packing density\u00b6","text":"<p>Next we will quantify the neoplastic cell packing density in the tumor nest by simply counting the number of neoplastic nuclei within a defined radius around each neoplastic nucleus. For example, in HGSC, the cell packing density is heavily related to different tumor growth patterns e.g. solid, papillary etc.. To get the neighborhoods based on radius, we will use the distband graph with 64 pixel threshold (32 microns).</p>"}]}